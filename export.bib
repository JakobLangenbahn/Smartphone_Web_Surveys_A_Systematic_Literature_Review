@article{Wenz2019,
   abstract = {We asked members of the Understanding Society Innovation Panel about their willingness to participate in various data collection tasks on their mobile devices. We find that stated willingness varies considerably depending on the type of activity involved: respondents are less willing to participate in tasks that involve downloading and installing an app, or where data are collected passively. Stated willingness also varies between smartphones and tablets, and between types of respondents: respondents who report higher concerns about the security of data collected with mobile technologies and those who use their devices less intensively are less willing to participate in mobile data collection tasks.},
   author = {Alexander, Wenz and Annette Jackle and Mick P. Couper},
   doi = {10.18148/srm/2019.v13i1.7298},
   issn = {1864-3361},
   issue = {1},
   journal = {Survey Research Methods},
   keywords = {ACCEPTANCE,Bluetooth,GPS,accelerometer,app,smartphone,tablet},
   pages = {1-22},
   pmid = {WOS:000464212000001},
   title = {Willingness to use mobile technologies for data collection in a probability household panel},
   volume = {13},
   year = {2019},
}
@article{Hartman2019,
   abstract = {Background and Objective Recent evidence has shown that online surveys can reliably collect preference data, which markedly decrease the cost of health preference studies and expand their representativeness. As the use of mobile technology continues to grow, we wanted to examine its potential impact on health preferences. Methods Two recently completed discrete choice experiments using members of the US general population (n = 15,292) included information on respondent device (cell phone, tablet, Mac, PC) and internet connection (business, cellular, college, government, residential). In this analysis, we tested for differences in respondent characteristics, participation, response quality, and utility values for the 5-level EQ-5D (EQ-5D-5L) by device and connection. Results Compared to Mac and PC users, respondents using a cell phone or tablet had longer completion times and were significantly more likely to drop out during the surveys (p < 0.001). Tablet users also demonstrated more logical inconsistencies (p = 0.05). Likewise, respondents using a cellular internet connection exhibit significantly less consistency in their health preferences. However, matched samples for tablets and cell phones produced similar EQ-5D-5L utility values (mean differences < 0.06 on a quality-adjusted life-year [QALY] scale for all potential health states). Conclusion Allowing respondents to complete online surveys using a cell phone or tablet or over a cellular connection substantially increases the diversity of respondents and the likelihood of obtaining a representative sample, as many individuals have cell phones but not a computer. While the results showed systematic variability in participation and response quality by device and connection type, this study did not show any meaningful changes in utility values.},
   author = {John D. Hartman and Benjamin M. Craig},
   doi = {10.1007/s40271-019-00380-z},
   issn = {1178-1653},
   issue = {6},
   journal = {The Patient-Patient-Centered Outcomes Research},
   keywords = {DATA QUALITY,DISCRETE-CHOICE EXPERIMENTS,LOGICAL INCONSISTENCIES,MOBILE WEB,PROBABILITY-BASED PANEL,PROPENSITY SCORE,SENSITIVE TOPICS,SMARTPHONES,STATE VALUATIONS,WEB SURVEYS},
   pages = {639-650},
   pmid = {WOS:000499697500008},
   title = {Does Device or Connection Type Affect Health Preferences in Online Surveys?},
   volume = {12},
   year = {2019},
}
@article{Krebs2021,
   abstract = {The effects of scale direction on response behavior are well known in the survey literature, where a variety of theoretical approaches are discussed, and mixed empirical findings are reported. In addition, different types of survey completion devices seem to vary in their susceptibility to scale direction effects. In this study, we therefore investigate the effect of scale direction and device type on response behavior in PC and smartphone surveys. To do so, we conducted a web survey experiment in a German non-probability access panel (N=3,401) using a two-step split-ballot design with four groups that are defined by device type (PC and smartphone) and scale direction (decremental and incremental). The results reveal that both PCs and smartphones are robust against scale direction effects. The results also show that response behavior differs substantially between PCs and smartphones, indicating that the device type (PC or smartphone) matters. In particular, the findings show that the comparability of data obtained through multi-device surveys is limited.},
   author = {Dagmar Krebs and Jan K. Höhne},
   doi = {10.1093/jssam/smz058},
   issn = {2325-0984},
   issue = {3},
   journal = {Journal of Survey Statistics and Methodology},
   keywords = {AGREE/DISAGREE,ITEM-SPECIFIC QUESTIONS,Latent means,MEASUREMENT INVARIANCE,MOBILE WEB,Measurement invariance,Multi-device survey,NEED,ORDER,QUALITY,Rating scales,Response behavior,Scale direction,TABLETS,WEB SURVEYS},
   pages = {477-495},
   pmid = {WOS:000685208800006},
   title = {EXPLORING SCALE DIRECTION EFFECTS AND RESPONSE BEHAVIOR ACROSS PC AND SMARTPHONE SURVEYS},
   volume = {9},
   year = {2021},
}
@article{Lugtig2019,
   abstract = {A sizable minority of all web surveys are nowadays completed on smartphones. People who choose a smartphone for Internet-related tasks are different from people who mainly use a PC or tablet. Smartphone use is particularly high among the young and urban. We have to make web surveys attractive for smartphone completion in order not to lose these groups of smartphone users. In this paper we study how to encourage people to complete surveys on smartphones in order to attract hard-to-reach subgroups of the population. We experimentally test new features of a survey-friendly design: we test two versions of an invitation letter to a survey, a new questionnaire lay-out, and autoforwarding. The goal of the experiment is to evaluate whether the new survey design attracts more smartphone users, leads to a better survey experience on smartphones and results in more respondents signing up to become a member of a probability-based online panel. Our results show that the invitation letter that emphasizes the possibility for smartphone completion does not yield a higher response rate than the control condition, nor do we find differences in the socio-demographic background of respondents. We do find that slightly more respondents choose a smartphone for survey completion. The changes in the layout of the questionnaire do lead to a change in survey experience on the smartphone. Smartphone respondents need 20% less time to complete the survey when the questionnaire includes autoforwarding. However, we do not find that respondents evaluate the survey better, nor are they more likely to become a member of the panel when asked at the end of the survey. We conclude with a discussion of autoforwarding in web surveys and methods to attract smartphone users to web surveys.},
   author = {Peter Lugtig and Vera Toepoel and Marieke Haan and Robbert Zandvliet and Laurens Klein Kranenburg},
   issn = {2190-4936},
   issue = {2},
   journal = {methods, data, analyses},
   keywords = {Datengewinnung,Datenqualität,Mobiltelefon,Online-Befragung,Panel,Stichprobe,Umfrageforschung,cell phone,data capture,data quality,online survey,panel,sample,survey research},
   pages = {291-306},
   title = {Recruiting Young and Urban Groups into a Probability-Based Online Panel by Promoting Smartphone Use},
   volume = {13},
   url = {https://www.wiso-net.de/document/SSOA__63138},
   year = {2019},
}
@article{Steinbrecher2015,
   author = {Markus Steinbrecher and Joss Roßmann and Jan E. Blumenstiel},
   doi = {10.1093/IJPOR/EDU025},
   issn = {0954-2892},
   issue = {2},
   journal = {International Journal of Public Opinion Research},
   month = {6},
   note = {\{<br/>"topic": "Data Quality"<br/>"break_off_mobile_higher": True<br/><br/>\}},
   pages = {289-302},
   publisher = {Oxford Academic},
   title = {Why Do Respondents Break Off Web Surveys and Does It Matter? Results From Four Follow-up Surveys},
   volume = {27},
   url = {https://academic.oup.com/ijpor/article/27/2/289/745155},
   year = {2015},
}
@article{Toepoel2018,
   abstract = {With the rise of mobile surveys comes the need for shorter questionnaires. We investigate the modularization of an existing questionnaire in the Longitudinal Internet Study for the Social Sciences ...},
   author = {Vera Toepoel and Peter Lugtig},
   doi = {10.1177/0894439318784882},
   issn = {15528286},
   journal = {Social Science Computer Review},
   keywords = {data chunking,data modularization,data quality,mobile surveys,online surveys},
   month = {7},
   note = {Survey Design},
   publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
   title = {Modularization in an Era of Mobile Web: Investigating the Effects of Cutting a Survey Into Smaller Pieces on Data Quality},
   url = {https://journals.sagepub.com/doi/full/10.1177/0894439318784882},
   year = {2018},
}
@article{Mavletova2014,
   abstract = {There is some evidence that questionnaire design (scrolling or paging) and invitation mode (SMS or e-mail) have an impact on response rates in web surveys completed on personal computers (PCs). This paper examines whether these findings can be generalized to mobile web surveys. First, we explore the effect of scrolling versus paging design on the breakoff rate, item nonresponse, and completion time in mobile web surveys. Second, we investigate which type of invitation and reminder mode (SMS or e-mail) is more effective in terms of producing higher participation rates and maximizing the percentage of respondents who complete the survey via a mobile device rather than a PC. The paper summarizes the results of an experiment conducted among members of a volunteer online access panel in Russia, who were asked to complete the survey using a mobile device. We find that the scrolling design leads to significantly faster completion times, lower (though not significantly lower) breakoff rates, fewer technical problems, and higher subjective ratings of the questionnaire. We also find that SMS invitations are more effective than e-mail invitations in mobile web surveys.},
   author = {Aigul Mavletova and Mick P. Couper},
   doi = {10.1093/JSSAM/SMU015},
   issn = {2325-0984},
   issue = {4},
   journal = {Journal of Survey Statistics and Methodology},
   keywords = {Invitation mode,Mobile web surveys,Nonresponse,Participation rates,Survey design},
   month = {12},
   note = {Survey Design<br/>},
   pages = {498-518},
   publisher = {Oxford Academic},
   title = {Mobile Web Survey Design: Scrolling versus Paging, SMS versus E-mail Invitations},
   volume = {2},
   url = {https://academic.oup.com/jssam/article/2/4/498/2937094},
   year = {2014},
}
@article{Lambert2015,
   abstract = {With the growing reliance on tablets and smartphones for internet access, understanding the effects of completion device on online survey responses becomes increasing important. This study uses data from the Strategic National Arts Alumni Project, a multi-institution online alumni survey designed to obtain knowledge of arts education, to explore the effects of what type of device (PC, Mac, tablet, or smartphone) a respondent uses has on his/her responses. Differences by device type in the characteristics of survey respondents, survey completion, time spent responding, willingness to answer complex and open-ended questions, and lengths of open-ended responses are discussed.},
   author = {Amber D. Lambert and Angie L. Miller},
   doi = {10.1007/S11162-014-9354-7/TABLES/5},
   issn = {1573188X},
   issue = {2},
   journal = {Research in Higher Education},
   keywords = {Completion device,Smartphones,Survey response},
   month = {3},
   note = {Data Quality},
   pages = {166-177},
   publisher = {Kluwer Academic Publishers},
   title = {Living with Smartphones: Does Completion Device Affect Survey Responses?},
   volume = {56},
   url = {https://link.springer.com/article/10.1007/s11162-014-9354-7},
   year = {2015},
}
@article{Erens2019,
   abstract = {It is increasingly common for respondents to complete web surveys using mobile devices (smartphones and tablets) rather than personal computers/laptops (PCs). Evidence of the impact of the use of mobile devices on response and data quality shows mixed results and is only available for general population surveys. We looked at response quality for a work-related survey in the UK among general practitioners (GPs). GPs were sent email invitations to complete a web survey and half (55%) completed it on a mobile device. While GPs using a mobile device were less likely to complete the full questionnaire than those using a PC, we found no differences in data quality between mobile and PC users, except for PC users being more likely to respond to open-ended questions.},
   author = {Bob Erens and Tommaso Manacorda and Jennifer Gosling and Nicholas Mays and David Reid and William Taylor},
   journal = {Social Research Practice},
   note = {Data Quality},
   pages = {15-26},
   title = {Comparing data quality from personal computers and mobile devices in an online survey among professionals Funding acknowledgement},
   volume = {7},
   url = {www.qualtrics.com},
   year = {2019},
}
@article{Couper2013,
   abstract = {In this paper I review three key technology-related trends: 1) big data, 2) non-probability samples, and 3) mobile data collection.  I focus on the implications of these trends for survey research and the research profession.  With regard to big data, I review a number of concerns that need to be addressed, and argue for a balanced and careful evaluation of the role that big data can play in the future.  I argue that these developments are unlikely to replace transitional survey data collection, but will supplement surveys and expand the range of research methods.  I also argue for the need for the survey research profession to adapt to changing circumstances.},
   author = {Mick P. Couper},
   doi = {10.18148/SRM/2013.V7I3.5751},
   issn = {1864-3361},
   issue = {3},
   journal = {Survey Research Methods},
   keywords = {big data,mobile   surveys,non,organic data,probability surveys,social media},
   month = {12},
   note = {General Development of the field},
   pages = {145-156},
   title = {Is the Sky Falling?  New Technology, Changing Media, and the Future of Surveys},
   volume = {7},
   url = {https://ojs.ub.uni-konstanz.de/srm/article/view/5751},
   year = {2013},
}
@article{Callegaro2013,
   author = {Mario Callegaro},
   doi = {10.2501/IJMR-2013-026},
   issue = {2},
   journal = {International Journal of Market Research},
   note = {Who is using mobile?},
   pages = {317-320},
   title = {From mixed-mode to multiple devices Web surveys, smartphone surveys and apps: has the respondent gone ahead of us in answering surveys?},
   volume = {55},
   year = {2013},
}
@inproceedings{Wang2017,
   abstract = {With the growing use of smartphones, many surveys can now be administered using those phones. Such questionnaires are called mobile survey questionnaires. The designer of a mobile survey questionnaire is challenged with presenting text and controls on a small display, while allowing respondents to correctly understand and answer questions with ease. To address this challenge, we are developing an evidencebased framework of user interface design for mobile survey questionnaires. The framework includes two parts: standards for the basic elements of surveyrelevant mobile device operation and guidelines for the building blocks of mobile survey questionnaires. In this presentation, we will describe five behavioral experiments designed to collect evidence for developing the standards. These experiments cover visual perception and motor actions relevant to survey completion. Some preliminary results from ongoing data collection are presented.},
   author = {Lin Wang and Christopher Antoun and Russell Sanders and Elizabeth Nichols and Erica L. Hawala and Brian Falcone and Ivonne J. Figueroa and Jonathan Katz},
   city = {New York},
   doi = {10.1145/3027063.3053181},
   isbn = {9781450346566},
   journal = {Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
   keywords = {Framework,Mobile survey,Standards,Usability,Usability ACM Classification Keywords H52 User Interfaces: Human Factors},
   month = {5},
   note = {Survey Design},
   pages = {2998-3004},
   publisher = {Association for Computing Machinery},
   title = {Experimentation for developing evidence-based ui standards of mobile survey questionnaires},
   volume = {Part F127655},
   url = {http://dx.doi.org/10.1145/3027063.3053181},
   year = {2017},
}
@article{Bosch2019,
   abstract = {Millennials have been the focus of quite some research because of their differences with older cohorts. Besides, young respondents have been considered as a hard target population for surveys. Howe...},
   author = {Oriol J. Bosch and Melanie Revilla and Ezequiel Paura},
   doi = {10.1177/1470785318815567},
   issn = {14707853},
   issue = {4},
   journal = {International Journal of Market Research},
   keywords = {Millennials,break-off,smartphones,survey evaluation,survey participation},
   month = {12},
   note = {Who is using mobile?},
   pages = {359-365},
   publisher = {SAGE PublicationsSage UK: London, England},
   title = {Do Millennials differ in terms of survey participation?:},
   volume = {61},
   url = {https://journals.sagepub.com/doi/full/10.1177/1470785318815567},
   year = {2019},
}
@article{Zou2021,
   abstract = {Using mobile devices to complete web-based surveys is an inescapable trend. Given the growth of this medium, some researchers are concerned about whether mobile devices are a viable channel for administering self-report online surveys. Taking two online surveys respectively using the US and China samples, this study compared the responses quality between participants responding via mobile devices and via PCs. Results from both the US and China samples revealed that although mobile respondents took longer to complete surveys than PC respondents, response quality did not differ significantly between these groups. Several behaviour patterns among mobile respondents were also identified in both samples. These findings provide practical implications to optimize web-based surveys for mobile users in tourism and hospitality research.},
   author = {Suiwen Zou and Karen P. Tan and Hongbo Liu and Xiang Li and Ye Chen},
   doi = {10.1080/13683500.2020.1797645},
   issue = {10},
   journal = {Current Issues in Tourism},
   keywords = {COMPUTER,ISSUES,Mobile device,Online survey,RATES,Response quality,TECHNOLOGY,Theory ofsatisficing,WEB SURVEY DESIGN},
   note = {Data Quality},
   pages = {1345-1357},
   title = {Mobile vs. PC: the device mode effects on tourism online survey response quality},
   volume = {24},
   year = {2021},
}
@article{Zijlstra2018,
   abstract = {Transport studies constantly rely on surveys among travelers. Computer assisted web interview is the most popular survey mode. However, respondents complete online surveys nowadays using smartphones, tablets or traditional devices. We address three questions related to this development: how important are mobile respondents; how to deal with mobile respondents; and what is the effect of mobile response in a survey? In order to answer these questions, we used a series of information dense meta-analyses and state-of-the-art literature. Our results reveal that one out of every three respondents used a mobile device in 2016. The profile of mobile respondents adheres to the profiles of hard-to-reach candidates. Four design strategies for mixed-device surveys are identified and discussed. By taking an active approach to mixed-device surveys, multiple issues associated with mobile response can be overcome, i.e. differences in completion times and break-offs can be minimized. Mobile respondents appreciate redesigned surveys. Our results are in favor of facilitating mobile respondents with an adaptive or responsive web design in surveys.},
   author = {Toon Zijlstra and Krisje Wijgergangs and Sascha Hoogendoorn-Lanser},
   journal = {Transportation Research Procedia},
   keywords = {Datenqualität,Mischmaschine,Mischvorrichtung,Online-Überwachung,Smartphone,mobiles Gerät},
   note = {General Development},
   pages = {184-194},
   title = {Traditional and mobile devices in computer assisted web-interviews},
   volume = {32},
   url = {https://www.wiso-net.de/document/BEFO__20181041493-BEFO-ZDEE},
   year = {2018},
}
@article{Cameron2018,
   abstract = {Cameron and Gentleman investigate some of the responses to a series of four surveys administered on both the 23andMe website and 23andMe mobile app to understand differences between web-based and mobile-based data collection. The surveys cover a wide variety of topics, including socioeconomic status (SES), tobacco use, allergies, and caffeine intake, and were chosen because they were available to all customers on both platforms. The use of mobile applications to capture research data efficiently is a promising technique for researchers. However, with the growth of mobile-app-based data collection, the use of mobile apps as a research tool may continue to be an important complement to many of the more-traditional web-based data collection techniques.},
   author = {Briana Cameron and Robert Gentleman},
   issue = {4},
   journal = {Chance},
   keywords = {Applications programs,Caffeine,Collection,Data collection,Mobile computing,Platforms,Polls & surveys,Research methodology,Software,Statistics,Tobacco,Web sites,Websites},
   pages = {29},
   title = {Mobile Apps versus Web Browsers: A Comparison of Self-administered Survey Platforms},
   volume = {31},
   url = {https://www.proquest.com/scholarly-journals/mobile-apps-versus-web-browsers-comparison-self/docview/2170368207/se-2?accountid=14570},
   year = {2018},
}
@article{Bacon2017,
   abstract = {The Advertising Research Foundation's ongoing How Advertising Works program combines original experiments with outside research. Launched in 2015, the program intends to offer practical guidance for improving advertising effectiveness across media and across platforms. The latest investigations led by Advertising Research Foundation Executive Researcher Christopher Bacon focused on the quality of survey research on mobile devices, which consumers increasingly are using to respond to online surveys. Specifically, the authors explored the use of symbols (emojis) as an alternative to text in the design of mobile surveys to keep respondents from abandoning the survey and to improve user experience. In the pages that follow, the authors explain the historical precedent for using symbols as communication devices, the importance of mobile-survey design using symbols, and the implication for data quality and effective survey design going forward.},
   author = {Christopher Bacon and Frances M. Barlas and Zoe Dowling and Randall K. Thomas},
   doi = {10.2501/JAR-2017-053},
   issue = {4},
   journal = {Journal of Advertising Research},
   note = {Survey Design},
   pages = {462-470},
   title = {How Effective Are Emojis In Surveys Taken on Mobile Devices? Data-Quality Implications and the Potential To Improve Mobile-Survey Engagement and Experience},
   volume = {57},
   year = {2017},
}
@article{Huff2015,
   abstract = {This research investigated the completing of a web-based personality assessment using smart phones and computers. Data were collected from 47 undergraduate students using a within subjects design. Results indicated that the usability and the time to complete the assessment of a web-based non-optimized questionnaire is significantly different when completed with a smart phone versus a computer. However, there were no significant differences in personality scores. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
   author = {Kyle C. Huff},
   doi = {10.1016/j.chb.2015.03.008},
   issn = {0747-5632},
   journal = {Computers in Human Behavior},
   keywords = {ABILITY,Computer,Computers,DESIGN,EQUIVALENCE,INTERNET,Mobile Phones,Mobile assessment,PAPER-AND-PENCIL,PERSONALITY,Personality,Personality Measures,TESTS,Time,Usability},
   pages = {208-212},
   pmid = {2015-20975-025},
   title = {The comparison of mobile devices to computers for web-based assessments},
   volume = {49},
   url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2015-20975-025%26site%3dehost-live},
   year = {2015},
}
@article{Skeie2019,
   abstract = {Stated preference (SP) web surveys are increasingly completed on mobile devices such as smartphones and tablets instead of computers. Due to differences in technical attributes and response contexts of the devices, this trend may affect the quality of the survey data and elicited welfare measures. Little is known of such device effects in SP research. In the first such study of its kind, we compare willingness to pay (WTP) and response quality between devices in a large, national contingent valuation survey. Propensity score matching is used to distinguish device effects from observed sample composition effects due to self-selection. We find significantly higher WTP for smartphone respondents in the first out of four sequential WTP questions, and no differences for tablets. Concerning data (response) quality, results are mixed, but not consistently lower for smartphones and tablets compared to computers. Measured by indicators of response randomness, shares of don't know and protest zeros, smartphone responses even show signs of higher quality. Only in terms of the extent of internal scope sensitivity, do smartphones and tablets fare somewhat worse than computers. Overall, our results do not indicate substantial loss of response quality or differences in welfare measures for mobile devices.},
   author = {Magnus A. Skeie and Henrik Lindhjem and Sofie Skjeflo and Ståle Navrud},
   doi = {10.1016/j.ecolecon.2019.106390},
   issn = {0921-8009},
   journal = {Ecological Economics},
   keywords = {Contingent valuation,Ecosystem services,IMPACT,INTERNET,MAIL,MOBILE DEVICES,MODES,Mobile device,Mobiltelefon,Personal Computer,Propensity score matching,QUALITY,SCOPE,STATED-PREFERENCE SURVEYS,Survey quality,Umweltökonomik,Zahlungsbereitschaftsanalyse,Ökosystem},
   note = {Data Quality},
   pmid = {WOS:000487172500018},
   title = {Smartphone and tablet effects in contingent valuation web surveys - No reason to worry?},
   volume = {165},
   year = {2019},
}
@inproceedings{,
   abstract = {Growing numbers of people are using their mobile phones to respond to online surveys. As a result, survey designers face the challenge of displaying questions and their response options and navigation elements on small smartphone screens in a way that encourages survey completion. The purpose of the present study was to conduct a series of systematic assessments of how older adults using smartphones interact with different user-interface features in online surveys. This paper shares results of three different experiments. Experiment 1 compares different ways of displaying choose-one response options. Experiment 2 compares different ways of displaying numeric entry boxes, specifically ones used to collect currency information (e.g., prices, costs, salaries). Experiment 3 tests whether forward and backward navigational buttons on a smartphone survey should be labeled with words (previous, next) or simply indicated with arrow icons (). Results indicate that certain features such as picker-boxes that appear at the bottom of the screen (iOS devices), fixed formatting of numeric-entry boxes, and icon navigation buttons were problematic. They either had negative impacts on performance (response times and/or accuracy) or only a small percentage of participants preferred these design features when asked to compare them to the other features.},
   author = {Erica Olmsted-Hawala and Elizabeth Nichols and Brian Falcone and Ivonne J. Figueroa and Christopher Antoun and Lin Wang},
   city = {Cham},
   doi = {10.1007/978-3-319-92034-4_26},
   editor = {J. Zhou and G. Salvendy},
   journal = {Human Aspects of IT for the Aged Population. Acceptance, Communication and Participation. ITAP 2018. Lecture Notes in Computer Science, vol 10926.},
   keywords = {Erwachsener,Leitfaden,Online-Überwachung,Rückmeldezeit,Smartphone,Währung,älterer Mensch},
   month = {6},
   pages = {335},
   publisher = {Springer},
   title = {Optimal Data Entry Designs in Mobile Web Surveys for Older Adults},
   volume = {10926},
   url = {https://www.wiso-net.de/document/BEFO__20180718734-BEFO-DOMA-ITEC-ZDEE-TECU},
   year = {2018},
}
@article{Toepoel2015,
   author = {Vera Toepoel and Peter Lugtig},
   issn = {1864-6956},
   issue = {2},
   journal = {methods, data, analyses},
   keywords = {Antwortverhalten,response behavior},
   note = {Data Quality},
   pages = {155-162},
   title = {Online surveys are mixed-device surveys: issues associated with the use of different (mobile) devices in web surveys},
   volume = {9},
   url = {https://www.wiso-net.de/document/MDA__45668},
   year = {2015},
}
@article{Buskirk2015,
   abstract = {"The continued rise in smartphone penetration globally afford survey researchers with an unprecedented portal into personal survey data collection from respondents who could complete surveys from virtually any place at any time. While the basic research into optimizing the survey experience and data collection on mobile devices has continued to develop, there are still fundamental gaps in our knowledge of how to optimize certain types of questions in the mobile setting. In fact, survey researchers are still trying to understand which online design principles directly translate into presentation on mobile devices and which principles have to be modified to incorporate separate methods for these devices. One such area involves the use of input styles such as sliding scales that lend themselves to more touch centric input devices such as smartphones or tablets. Operationalizing these types of scales begs the question of an optimal starting position and whether these touch centric input styles are equally preferred by respondents using less touch capable devices. While an outside starting position seems optimal for slider questions completed via computer, this solution may not be optimal for completion via mobile devices as these devices are subjected to far more space and layout constraints compared to computers. This experiment moves the mixed device survey literature forward by directly comparing outcomes from respondents who completed a collection of survey scales using their smartphone, tablet or computer. Within each device, respondents were randomly assigned to complete one of 20 possible versions of scale items determined by a combination of three experimental factors including input style, length and number formatting. Results from this study suggest more weaknesses than strengths for using slider scales to collect survey data using mobile devices and also suggest that preference for these touch centric input styles varies across devices and may not be as high as the preference for the more traditional radio button style." (author's abstract)},
   author = {Trent D. Buskirk},
   issn = {1864-6956},
   issue = {2},
   journal = {methods, data, analyses},
   keywords = {Antwortverhalten,Computer,Datengewinnung,Datenqualität,Mobiltelefon,Online-Befragung,Umfrageforschung,cell phone,computer,data capture,data quality,online survey,response behavior,survey research},
   note = {Survey Design},
   pages = {229-260},
   title = {Are sliders too slick for surveys? An experiment comparing slider and radio button scales for smartphone, tablet and computer based surveys},
   volume = {9},
   url = {https://www.wiso-net.de/document/MDA__45666},
   year = {2015},
}
@article{Arn2015,
   abstract = {"In this paper, we look at the challenge of optimizing survey layout in online research to enable multi-device use. Several studies provide useful advice on target-oriented implementation of web design for CAWI surveys. This paper presents results of the implementation of a new adapted design at the panel of DemoSCOPE that allows the participants to take part in a survey on multiple (especially mobile) devices. To evaluate this adapted design, we compare interview data and question timing of panellists who participated in an insurance study before and after the design transition. Central key figures concerning the completion rate, item non-response, open questions, straightlining, timing of single questions and the length of the total interview are presented. In addition, we have presented examples of both old and new design to the community and invited them to assess these examples concerning orientation, color, design and usability. We evaluate the differences in these assessments before and after the design transition for smartphone and desktop users. We end with suggestions for best practice for online studies on different devices." (author's abstract)},
   author = {Birgit Arn and Stefan Klug and Janusz Kolodziejski},
   issn = {1864-6956},
   issue = {2},
   journal = {methods, data, analyses},
   keywords = {Umfrageforschung,survey research},
   note = {Survey Design and Data Quality},
   pages = {185-212},
   title = {Evaluation of an adapted design in a multi-device online panel: a DemoSCOPE case study},
   volume = {9},
   year = {2015},
}
@article{Struminskaya2015,
   abstract = {"The use of mobile devices such as smartphones and tablets for survey completion is growing rapidly, raising concerns regarding data quality in general, and nonresponse and measurement error in particular. We use the data from six online waves of the GESIS Panel, a probability-based mixed-mode panel representative of the German population to study whether the responses provided using tablets or smartphones differ on indicators of measurement and nonresponse errors from responses provided via personal computers or laptops. We follow an approach chosen by Lugtig and Toepoel (2015), using the following indicators of nonresponse error: item nonresponse, providing an answer to an open question; and the following indicators of measurement error: straightlining, number of characters in open questions, choice of left-aligned options in horizontal scales, and survey duration. Moreover, we extend the scope of past research by exploring whether data quality is a function of device-type or respondent-type characteristics using multilevel models. Overall, we find that responding with mobile devices is associated with a higher likelihood of measurement discrepancies compared to PC/laptop survey completion. For smartphone survey completion, the indicators of measurement and nonresponse error tend to be higher than for tablet completion. We find that most indicators of nonresponse and measurement error used in our analysis cannot be attributed to the respondent characteristics but are rather effects of mobile devices." (author's abstract)},
   author = {Bella Struminskaya and Kai Weyandt and Michael Bosnjak},
   issn = {1864-6956},
   issue = {2},
   journal = {methods, data, analyses},
   keywords = {Umfrageforschung,survey research},
   note = {Data Quality},
   pages = {261-292},
   title = {The effects of questionnaire completion using mobile devices on data quality: evidence from a probability-based general population panel},
   volume = {9},
   url = {https://www.wiso-net.de/document/MDA__45667},
   year = {2015},
}
@article{Fuchs2009,
   abstract = {In recent years, mobile devices are increasingly considered to access the World Wide Web. Several survey research organizations are about to use this technology as a means of conducting self-administered surveys. Among other advantages it allows survey researchers to overcome the lack of random selection procedures in online surveys since it provides the opportunity to use RDD-like probability sampling of cell phone numbers. However, low penetration rates of smart phones raise concerns that the coverage bias of a mobile Web survey might in fact harm survey estimates considerably. In this paper, we report results of a simulation study on the coverage bias of the mobile Web population across European countries. Based on a subset of the Eurobarometer data we estimate the relative coverage bias of the smart phone population in contrast to the general population. Even though we observed an incline of the mobile Web penetration rates over the course of the past years, coverage biases were still considerably large for socio-demographic variables. Nevertheless, in a few European countries mobile Web coverage biases are already smaller than the coverage biases of the population with traditional landline Internet access. Adapted from the source document.},
   author = {Marek Fuchs and Britta Busse},
   issn = {1662-5544, 1662-5544},
   issue = {1},
   journal = {International Journal of Internet Science},
   keywords = {0827: mass phenomena,9121: political behavior,Access,Bias,Courses,Europe,Internet,Mobile Web,Sampling,Simulation,Surveys,article,data collection,data quality,political behavior,public opinion,smart phone,survey},
   note = {Coverage Bias},
   pages = {21-33},
   pmid = {743047278; 201027538},
   title = {The Coverage Bias of Mobile Web Surveys Across European Countries},
   volume = {4},
   year = {2009},
}
@article{Mavletova2013,
   abstract = {A large number of findings in survey research suggest that responses to sensitive questions are situational and can vary in relation to context. The methodological literature demonstrates that social desirability biases are less prevalent in self-administered surveys, particularly in Web surveys, when there is no interviewer and less risk of presenting oneself in an unfavorable light. Since there is a growing number of users of mobile Web browsers, we focused our study on the effects of different devices (PC or cell phone) in Web surveys on the respondents' willingness to report sensitive information. To reduce selection bias, we carried out a two-wave cross-over experiment using a volunteer online access-panel in Russia. Participants were asked to complete the questionnaire in both survey modes: PC and mobile Web survey. We hypothesized that features of mobile Web usage may affect response accuracy and lead to more socially desirable responses compared to the PC Web survey mode. We found significant differences in the reporting of alcohol consumption by mode, consistent with our hypothesis. But other sensitive questions did not show similar effects. We also found that the presence of familiar bystanders had an impact on the responses, while the presence of strangers did not have any significant effect in either survey mode. Contrary to expectations, we did not find evidence of a positive impact of completing the questionnaire at home and trust in data confidentiality on the level of reporting. These results could help survey practitioners to design and improve data quality in Web surveys completed on different devices.},
   author = {Aigul Mavletova and Mick P. Couper},
   issn = {1864-3361},
   issue = {3},
   journal = {Survey Research Methods},
   keywords = {ALCOHOL-CONSUMPTION,BEHAVIOR,CONFIDENTIALITY CONCERNS,DATA-COLLECTION MODE,DRUG-USE,INTERNET,PAPER-AND-PENCIL,PRIVACY,SELF-ADMINISTERED QUESTIONNAIRES,SOCIAL DESIRABILITY BIAS,Web surveys,data quality,interview setting,mobile Web surveys,perceived privacy,presence of bystanders,sensitive questions},
   note = {Sensitive Topics},
   pages = {191-205},
   pmid = {WOS:000328484700005},
   title = {Sensitive Topics in PC Web and Mobile Web Surveys: Is There a Difference?},
   volume = {7},
   year = {2013},
}
@article{Wells2015,
   abstract = {Survey completions on mobile devices have been increasing rapidly. This important shift is something market researchers should definitely consider when designing and conducting self-administered online surveys. This article briefly summarises existing research and empirical results from mobile surveys. Based on the specific findings discussed, market researchers should be better aware of what to expect when fielding surveys completed by mobile respondents, whether this is intended or not. Bringing the findings together and discussing more broadly, for online surveys, market researchers should consider consciously and deliberately accommodating both mobile and PC respondents. Thus far, the research on mobile surveys indicates that consumers want the choice and ability to take surveys when they want, where they want and on the device of their choosing. It is to be hoped that market researchers are listening and become willing to accommodate survey respondents in terms of device and, by extension, time and location.},
   author = {Tom Wells},
   doi = {10.2501/IJMR-2015-045},
   issn = {1470-7853},
   issue = {4},
   journal = {International Journal of Market Research},
   keywords = {PC,QUALITY,WEB},
   note = {General development of the field},
   pages = {521-532},
   pmid = {WOS:000358814700003},
   title = {What market researchers should know about mobile surveys},
   volume = {57},
   year = {2015},
}
@article{Liebe2015,
   abstract = {Web surveys are becoming increasingly popular in survey research including stated preference surveys. Compared with face-to-face, telephone and mail surveys, web surveys may contain a different and new source of measurement error and bias: the type of device that respondents use to answer the survey questions. This is the first study that tests whether the use of mobile devices, tablets or smartphones, affects survey characteristics and stated preferences in a web-based choice experiment. The web survey on expanding renewable energy production in Germany was carried out with 3182 respondents, of which 12% used a mobile device. Propensity score matching is used to account for selection bias in the use of mobile devices for survey completion. We find that mobile device users spent more time than desktop/laptop users to answer the survey. Yet, desktop/laptop users and mobile device users do not differ in acquiescence tendency as an indicator of extreme response patterns. For mobile device users only, we find a negative correlation between screen size and interview length and a positive correlation between screen size and acquiescence tendency. In the choice experiment data, we do not find significant differences in the tendency to choose the status quo option and scale between both subsamples. However, some of the estimates of implicit prices differ, albeit not in a unidirectional fashion. Model results for mobile device users indicate a U-shaped relationship between error variance and screen size. Together, the results suggest that using mobile devices is not detrimental to survey quality. (C) 2015 Elsevier Ltd. All rights reserved.},
   author = {Ulf Liebe and Klaus Glenk and Malte Oehlmann and Jürgen Meyerhoff},
   doi = {10.1016/j.jocm.2015.02.002},
   issn = {1755-5345},
   journal = {Journal of choice modelling},
   keywords = {Acquiescence bias,COMPLEXITY,Choice experiment,FACE-TO-FACE,INTERNET,Mobile device,Propensity score matching,Renewable energy,SCALE,Sample selection bias,Smartphone,Survey format,Survey quality,VALUATION},
   note = {Data Quality},
   pages = {17-31},
   pmid = {WOS:000356186300002},
   title = {Does the use of mobile devices (tablets and smartphones) affect survey quality and choice behaviour in web surveys?},
   volume = {14},
   year = {2015},
}
@article{Toninelli2016,
   abstract = {More and more respondents use mobile devices to complete web surveys. These devices have different characteristics, if compared to PCs (e.g. smaller screen sizes and higher portability). These characteristics can affect the survey responses, mostly when a questionnaire includes sensitive questions. This topic was already studied by Mavletova and Couper (2013), through a two-wave experiment comparing PCs and mobile devices results for the same respondents in a Russian opt-in panel. We replicated this cross-over design, focusing on an opt-in panel for Spain, involving 1,800 panellists and comparing PCs and smartphones. Our results support most of Mavletova and Couper's (2013) findings (e.g. generally the used device does not significantly affect the reporting of sensitive information), confirming their robustness over the two studied countries. For other results (e.g. trust in data confidentiality), we found differences that can be justified by the diverse context/culture or by the quick changes that are still characterizing the mobile web survey participation.},
   author = {Daniele Toninelli and Melanie Revilla},
   doi = {10.18148/srm/2016.v1012.6274},
   issn = {1864-3361},
   issue = {2},
   journal = {Survey Research Methods},
   keywords = {BIAS,COMPUTERS,MODE,NONRESPONSE,SCREEN SIZE,Web surveys,measurement error,mobile participation,sensitive questions,smartphones,survey optimization},
   note = {Data Quality and Sensitive Topics},
   pages = {153-169},
   pmid = {WOS:000389771700006},
   title = {Smartphones vs PCs: Does the Device Affect the Web Survey Experience and the Measurement Error for Sensitive Topics? A Replication of the Mavletova & Couper's 2013 Experiment},
   volume = {10},
   year = {2016},
}
@article{Revilla2016,
   abstract = {Purpose - Despite the quick spread of the use of mobile devices in survey participation, there is still little knowledge about the potentialities and challenges that arise from this increase. The purpose of this paper is to study how respondents' preferences drive their choice of a certain device when participating in surveys. Furthermore, this paper evaluates the tolerance of participants when specifically asked to use mobile devices and carry out other specific tasks, such as taking photographs.
Design/methodology/approach - Data were collected by surveys in Spain, Portugal and Latin America by Netquest, an online fieldwork company.
Findings - Netquest panellists still mainly preferred to participate in surveys using personal computers. Nevertheless, the use of tablets and smartphones in surveys showed an increasing trend; more panellists would prefer mobile devices, if the questionnaires were adapted to them. Most respondents were not opposed to the idea of participating in tasks such as taking photographs or sharing GPS information.
Research limitations/implications - The research concerns an opt-in online panel that covers a specific area. For probability-based panels and other areas the findings may be different.
Practical implications - The findings show that online access panels need to adapt their surveys to mobile devices to satisfy the increasing demand from respondents. This will also allow new, and potentially very interesting data collection methods.
Originality/value - This study contributes to survey methodology with updated findings focusing on a currently underexplored area. Furthermore, it provides commercial online panels with useful information to determine their future strategies.},
   author = {Melanie Revilla and Daniele Toninelli and Carlos Ochoa and Germán Loewe},
   doi = {10.1108/IntR-02-2015-0032},
   issn = {1066-2243},
   issue = {5},
   journal = {Internet Research},
   keywords = {ADOPTION,COMPUTERS,Methodology,Mobile communications,Netquest,PC,Survey,Technological innovation,WEB SURVEYS,World Wide Web},
   note = {Who is using mobile? Images as anweser},
   pages = {1209-1227},
   pmid = {WOS:000386140600009},
   title = {Do online access panels need to adapt surveys for mobile devices?},
   volume = {26},
   year = {2016},
}
@article{Brosnan2017,
   abstract = {This study investigates whether it is the case that representativity is undermined if personal computer, tablet and smartphone respondents differ in sociodemographic characteristics and display different survey completion rates. Online market research is struggling with sample representativity. The analysis of more than ten million survey invitations, as well as stated device preference information, suggests that web survey respondents who are members of online panels still mostly use their personal computers, but do express increasing interest in using smartphones and tablets. Survey completion rates do vary across devices, and device use is significantly associated with socio-demographic characteristics and length of membership on a panel. Therefore, researchers must not limit respondents to use a specific device for completing a survey as this may compromise the quality of the survey completion experience, increase non response error and negatively affect representativity.},
   author = {Kylie Brosnan and Bettina Grün and Sara Dolnicar},
   doi = {10.2501/IJMR-2016-049},
   issn = {1470-7853},
   issue = {1},
   journal = {International Journal of Market Research},
   keywords = {COMPARING RESPONSE RATES,Consumer Attitudes,Consumer Research,Consumer Surveys,DESIGN,MAIL SURVEYS,METAANALYSIS,MOBILE,MODES,Mobile Devices,PANELS,Preferences,QUALITY,Tablet Computers,completion rates,device preference,device use,mobile devices,online research,web surveys},
   note = {Who is using mobile? and Data Quality},
   pages = {35-55},
   pmid = {WOS:000392908100005},
   title = {PC, phone or tablet? Use, preference and completion rates for web surveys},
   volume = {59},
   year = {2017},
}
@article{Bansal2017,
   abstract = {This paper explores strategies on how to best balance expanding survey length with the need for concise, relevant and engaging surveys, deployed in a device agnostic format. When designing a survey we, as an industry, are often seeking a balance between competing design challenges: clients have diverse and extensive objectives, survey participants have short attention spans and an ever increasing suite of connected devices to choose from. Survey participants are voting with their feet when surveys are not compatible with the device they want to use, whether that is the smart device in their pocket or laptop they are working on, and this is very real for online panels. We are seeing increased abandon rates, with the effects of extended fieldwork times, smaller pools of sample to draw from and the possibility of introducing bias into our data. Having spent much of 2015 working with clients to design more smart-device friendly surveys, Research Now has explored innovative ways to shorten survey length without compromising on the amount of material covered. Following on from work by Johnson et al. (2014), Research Now conducted a piece of primary research exploring survey modularisation as discussed in the current paper. The approach splits questionnaires into modules, with participants receiving only a specific module, a subset of the overall survey. It is expected that a long questionnaire can be split and when applied appropriately, designed properly and implemented effectively data can yield results comparable with a full non-modular survey. Building on previous industry work on this topic, and primary research conducted by Research Now, we discuss our methodology, the results and conclusions from this work, and explore opportunities to automate the approach. The overall goal of this study and resulting paper is to explore how adapting survey research in this way improves rather than complicates the lives of both researchers and research participants. If we are not able to shorten our surveys, then survey modularisation may prove to be our best hope for a complete, representative dataset and we need to ensure that this is achieved accurately, confidently and efficiently at scale.},
   author = {Harvir S. Bansal and James Eldridge and Avik Halder and Roddy Knowles and Michael Murray and Luke Sehmer and David Turner},
   doi = {10.2501/IJMR-2017-016},
   issn = {1470-7853},
   issue = {2},
   journal = {International Journal of Market Research},
   keywords = {DESIGN},
   note = {Survey Design},
   pages = {221-238},
   pmid = {WOS:000397839000006},
   title = {Shorter interviews, longer surveys Optimising the survey participant experience while accommodating ever expanding client demands},
   volume = {59},
   year = {2017},
}
@article{Tourangeau2017,
   abstract = {With respondents increasingly completing web surveys on tablet computers and smartphones, several studies have examined the potential effects of the switch from PCs to mobile devices. The studies have looked at a range of outcomes, including completion rates, breakoffs, and item nonresponse. We carried out a field experiment that compared responses obtained by smartphones, tablets, and laptop computers, focusing on the potential effects of the different devices on measurement errors. We examined whether the differences across devices in screen size (and the related need to scroll to see the entire question or the full set of response options) might moderate the effects of response order, affect the strategy respondents used to decide which of two options was preferable, change the effect of question context, or influence the use of definitions. Our experiments were based on the principle of visual prominence-the idea that respondents are more likely to notice and consider information that is easy to see. The experiments were deliberately designed to maximize the impact of screen size on the results, since the screen size would affect the visual prominence of key information. However, like many of the prior studies examining mobile devices, although response order, context, and evaluation strategy affected the answers respondents gave, few device effects emerged.},
   author = {Roger Tourangeau and Aaron Maitland and Gonzalo Rivero and Hanyu Sun and Douglas Williams and Ting Yan},
   doi = {10.1093/poq/nfx035},
   issn = {0033-362X},
   issue = {4},
   journal = {Public Opinion Quarterly},
   keywords = {COMPUTER,DESIGN,MOBILE,OPTIONS,PREFERENCE REVERSALS,SENSITIVE TOPICS},
   note = {Data Quality},
   pages = {896-929},
   pmid = {WOS:000419140100004},
   title = {WEB SURVEYS BY SMARTPHONE AND TABLETS EFFECTS ON SURVEY RESPONSES},
   volume = {81},
   year = {2017},
}
@article{Toepoel2018,
   abstract = {In an experiment dealing with the use of personal computer, tablet, or mobile, scale points (up to 5, 7, or 11) and response formats (bars or buttons) are varied to examine differences in mean scores and nonresponse. The total number of not applicable answers does not vary significantly. Personal computer has the lowest item nonresponse, followed by mobile and tablet, and a lower mean score than for mobile. Slider bars showed lower mean scores and more nonresponses than buttons, indicating that they are more prone to bias and difficult in use. Sider bars, which work with a drag-and-drop principle, perform worse than visual analogue scales working with a point-and-click principle and buttons. Five-point scales have more nonresponses than eleven-point scales. Respondents evaluate 11-point scales more positively than shorter scales.},
   author = {Vera Toepoel and Frederik Funke},
   doi = {10.1080/08898480.2018.1439245},
   issn = {0889-8480},
   issue = {2},
   journal = {Mathematical Population Studies},
   keywords = {DESIGN,Likert scale,OPTIMAL NUMBER,POINTS,RATING-SCALES,RELIABILITY,WEB SURVEYS,mobile surveys,questionnaire design,response formats,slider bars,visual analogue scales},
   note = {Survey Design},
   pages = {112-122},
   pmid = {WOS:000440746800005},
   title = {Sliders, visual analogue scales, or buttons: Influence of formats and scales in mobile and desktop surveys},
   volume = {25},
   year = {2018},
}
@article{DeLeeuw2018,
   abstract = {Mixed-mode surveys have been around since the late 1980s. In the past thirty years, major changes in technology and society influenced and changed data collection and survey methodology. However, in those years, mixed-mode strategies remained part of the daily survey practice, although the type of mix implemented followed the changes in technology and data collection methods. In this paper, I summarize the state of the art in traditional mixed-mode surveys and discuss implications for mixed device surveys.},
   author = {Edith D. DeLeeuw},
   doi = {10.18148/srm/2018.v12i2.7402},
   issn = {1864-3361},
   issue = {2},
   journal = {Survey Research Methods},
   keywords = {ASSOCIATION,DESIGNS,FACE-TO-FACE,IMPACT,METAANALYSIS,NONRESPONSE BIAS,PARTICIPATION RATES,SOCIAL DESIRABILITY BIAS,TELEPHONE SURVEYS,WEB SURVEYS,adjustment,equivalence,mobile surveys,mode measurement effect,mode selection effect,multiple devices,multiple modes,offline surveys,online surveys,prevention},
   note = {Mixed Mode},
   pages = {75-89},
   pmid = {WOS:000441444000001},
   title = {Mixed-Mode: Past, Present, and Future},
   volume = {12},
   year = {2018},
}
@article{,
   abstract = {This research note presents the results of an experiment that investigated how response rates and data quality could be improved for smartphone web surveys. First, we compare how invitations by text message versus by e-mail affect response rate. Text message invitations result in a similar total response rate as e-mail invitations when considering response via all types of online devices. When considering response via smartphones only, the survey completion rate is significantly higher. The text message contact mode also leads to a faster speed of initial response. Second, we examine the response effects of several questionnaire-design choices when using smartphones: paging versus scrolling, horizontal versus vertical question layout, number of answer options, and open-ended versus closed-ended questions. According to our findings, a scrolling layout leads to a shorter completion time than a paging layout. We further suggest that caution be used with horizontal and long-answer scales as well as open-ended text answer fields in smartphone surveys.},
   author = {Marika De Bruijne and Arnaud Wijnant},
   doi = {10.1093/poq/nfu046},
   issn = {0033362X},
   issue = {4},
   journal = {Public Opinion Quarterly},
   keywords = {0827: mass phenomena,7100:Market research,9121: political behavior,9130:Experiment/theoretical treatment,Choices,Comparative analysis,Data Quality,Electronic Mail,Experiments,Internet,ORDER,Political Science,Polls & surveys,Questionnaires,Research,Research Design,Research Responses,Response rates,Smartphones,Studies,Surveys,article,political behavior,public opinion},
   note = {Inivitation Mode},
   pages = {951-962},
   pmid = {1636484190},
   title = {Improving Response Rates and Questionnaire Design for Mobile Web Surveys},
   volume = {78},
   url = {https://www.proquest.com/scholarly-journals/improving-response-rates-questionnaire-design/docview/1636484190/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Awpsa&atitle=Improving+Response+Rates+and+Questionnaire+Design+for+Mobile+Web+Surveys&title=Public+Opinion+Quarterly&issn=0033362X&date=2014-12-01&volume=78&issue=4&spage=951&au=De+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Public+Opinion+Quarterly&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1093%2Fpoq%2Fnfu046 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Improving+Response+Rates+and+Questionnaire+Design+for+Mobile+Web+Surveys&title=Public+Opinion+Quarterly&issn=0033362X&date=2014-01-01&volume=78&issue=4&spage=951&au=De+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Public+Opinion+Quarterly&btitle=&rft_id=info:eric/201537256&rft_id=info:doi/10.1093%2Fpoq%2Fnfu046 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Awpsa&atitle=Improving+Response+Rates+and+Questionnaire+Design+for+Mobile+Web+Surveys&title=Public+Opinion+Quarterly&issn=0033362X&date=2014-01-01&volume=78&issue=4&spage=951&au=De+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Public+Opinion+Quarterly&btitle=&rft_id=info:eric/201524424&rft_id=info:doi/10.1093%2Fpoq%2Fnfu046},
   year = {2014},
}
@article{Ha2019,
   abstract = {Purpose The purpose of this paper is to examine the effect of smartphones and computers as web survey entry response devices on the quality of responses in different question formats and across different survey invitations delivery modes. The respondents' preference of device and the response immediacy were also compared.
Design/methodology/approach Two field experiments were conducted with a cluster sampling and a census of all students in a public university in the USA.
Findings Device effect on response quality was only found when using computer-aided self-interviews, but not in e-mail delivered web surveys. Even though the computer was the preferred device, but the smartphone's immediate response was significantly higher than the computer.
Research limitations/implications The sample was restricted to college students who are more proficient users of smartphones and have high access to computers. But the direct comparison in the two studies using the same population increases the internal validity of the study comparing different web survey delivery modes.
Practical implications Because of the minor differences in device on response quality, researchers can consider using more smartphones for field work such as computer-aided self-interviews to complement e-mail delivered surveys.
Originality/value This is the first study that compares the response device effects of computer-aided self-interviews and e-mailed delivered web surveys. Because web surveys are increasingly used and various devices are being used to collect data, how respondents behave in different devices and the strengths and weaknesses of different methods of delivery survey help researchers to improve data quality and develop effective web survey delivery and participant recruitment.},
   author = {Louisa, Ha and Chenjie Zhang},
   doi = {10.1108/OIR-11-2017-0322},
   issn = {1468-4527},
   issue = {3},
   journal = {Online Information Review},
   keywords = {Computer-aided self-interviews,DATA-COLLECTION,DESIGN,MOBILE,MODES,Mobile survey,PC WEB,QUALITY,Smartphones,Survey response quality,Web survey},
   note = {Data Quality},
   pages = {350-368},
   pmid = {WOS:000469416300002},
   title = {Are computers better than smartphones for web survey responses?},
   volume = {43},
   year = {2019},
}
@article{Haan2019,
   abstract = {In this study, we investigate whether mobile device use in surveys can be predicted. We aim to identify possible motives for device use and build a model by drawing on theory from technology acceptance research and survey research. We then test this model with a Structural Equation Modeling approach using data of seven waves of the GESIS panel. We test whether our theoretical model fits the data by focusing on measures of fit, and by studying the standardized effects of the model. Results reveal that intention to use a particular device can predict actual use quite well. Ease of smartphone use is the most meaningful variable: if people use a smartphone for specific tasks, their intention to use a smartphone for survey completion is also more likely. In conclusion, investing in ease of use of mobile survey completion could encourage respondents to use mobile devices. This can foremost be established by building well-designed surveys for mobile devices.},
   author = {Marieke Haan and Peter Lugtig and Vera Toepoel},
   doi = {10.1080/13645579.2019.1593340},
   issn = {1364-5579},
   issue = {5},
   journal = {International Journal of Social Research Methodology},
   keywords = {ACCEPTANCE,Mobile device use,ONLINE SURVEYS,TECHNOLOGY,WEB,mixed device,panel survey,technology acceptance},
   note = {Who is using mobile?},
   pages = {517-531},
   pmid = {WOS:000465798500001},
   title = {Can we predict device use? An investigation into mobile device use in surveys},
   volume = {22},
   year = {2019},
}
@article{Lee2019,
   abstract = {Smartphones have become very popular globally, and smartphone ownership has overtaken conventional cell phone ownership in many countries in recent years. With this rapid rise in smartphone penetration, researchers are looking at ways to conduct web surveys using smartphones. This is particularly true of student populations where smartphone penetration is very high and web surveys are already the norm. However, researchers are raising concerns about selection biases and measurement differences between PC and smartphone respondents. Questions also remain about comparisons to traditional interviewer-administered approaches. We designed an experimental comparison between a PC web survey, a smartphone web survey and a computer-assisted telephone interviewing (CATI) survey. This study was conducted using an annual survey of students at a large university in South Korea. The CATI (interviewer-administered) survey had a higher response rate, lower margins of error, and better representation of the student population than the two web (self-administered) modes, but at a higher cost. The CATI survey also had lower rates of item nonresponse. More significant differences were found between the modes for sensitive questions than for nonsensitive ones. This suggests that CATI surveys may still have a role to play in surveys of college students, even in a country with high rates of mobile technology adoption. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
   author = {Hana Lee and Sunwoong Kim and Mick P. Couper and Youngje Woo},
   doi = {10.1177/0894439318756867},
   issn = {0894-4393},
   issue = {2},
   journal = {Social Science Computer Review},
   keywords = {ALCOHOL,College Students,Computers,Error of Measurement,FACE-TO-FACE,MODE,Online Surveys,SENSITIVE TOPICS,Smartphones,Telephone Surveys,Test Administration,completion times,coverage,item nonresponse,measurement error,response rate,smartphone survey,survey costs,telephone survey,web survey},
   note = {Data Quality},
   pages = {234-247},
   pmid = {2019-13380-006},
   title = {Experimental comparison of PC web, smartphone web, and telephone surveys in the new technology era},
   volume = {37},
   url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2019-13380-006%26site%3dehost-live},
   year = {2019},
}
@article{Cernat2019,
   abstract = {Web surveys are struggling to attract and retain respondents due to high burden and competition for the users(') attention. One possible solution to this issue is the improvement of the visual design of surveys. In this article, we evaluate the impact of visual aids such as smiley faces, stars, hearts, and thumbs as alternatives to traditional radio buttons. We use an experimental design in a nonprobability online survey to investigate how the new designs compare with radio buttons and how the results might interact with device used for completion (PC vs mobile), the use of labels, the type of response scale (bipolar vs unipolar), and the number of response categories (5 vs 7 point). While we do not find big differences in response, quality and experience, there seem to be some indication that the use of smiley faces leads to worse data quality.},
   author = {Alexandru Cernat and Mingna Liu},
   doi = {10.1177/1470785318813520},
   issn = {1470-7853},
   issue = {3},
   journal = {International Journal of Market Research},
   keywords = {DATA QUALITY,SURVEY DESIGN,VISUAL ANALOG SCALES,experimental design,gamification,mobile survey,response scales,visual design,web surveys},
   note = {Survey Design},
   pages = {266-286},
   pmid = {WOS:000468290100005},
   title = {Radio buttons in web surveys: Searching for alternatives},
   volume = {61},
   year = {2019},
}
@article{Link2014,
   author = {Michael W. Link and Joe Murphy and Michael F. Schober and Trent D. Buskirk and Jennifer Hunter Childs and Casey Langer Tesfaye},
   doi = {10.1093/POQ/NFU054},
   issue = {4},
   journal = {Public Opinion Quarterly},
   pages = {779-787},
   title = {Mobile technologies for conducting, augmenting and potentially replacing surveys},
   volume = {78},
   year = {2014},
}
@article{Bucher2021,
   abstract = {The widespread usage of smartphones, as well as their technical features, offers many opportunities for survey research. As a result, the importance and popularity of smartphone surveys is steadily increasing. To explore the feasibility of a new text-to-web approach for surveying people directly via their smartphones, we conducted a case study in Germany in which we recruited respondents from a mobile random digit dialing sample via text messages that included a link to a web survey. We show that, although this survey approach is feasible, it is hampered by a number of issues, namely a high loss of numbers at the invitation stage, and a high rate of implicit refusals on the landing page of the survey.},
   author = {Hannah Bucher and Matthias Sand},
   doi = {10.1093/JSSAM/SMAB006},
   journal = {Journal of Survey Statistics and Methodology},
   note = {Invitation mode},
   title = {Exploring the Feasibility of Recruiting Respondents and Collecting Web Data Via Smartphone: A Case Study of Text-To-Web Recruitment for a General Population Survey in Germany},
   year = {2021},
}
@article{Revilla2016,
   abstract = {Most survey questions are closed questions, where respondents have to select an answer from a proposed set of alternatives. However, a lot of surveys also include, at least occasionally, some open questions. Open questions that call for elaborated and developed answers, called "open narrative questions", are used when the researchers want to go deeper into what the respondents think. This paper compares the answers to open narrative questions when the respondent is participating in a PC survey, in a smartphone-not-optimised survey or in a smartphone-optimised survey. The experiment was carried out in Spain using data collected by the Netquest online access panel. Respondents were assigned randomly to each type of device and survey format, in two successive waves. Because respondents have to type in their answer, we expect differences between devices, linked with the size and the kind of keyboards (i.e. physical versus digital, touch-screen or not). Differences are observed between answers that come from PCs and smartphones for the response time per written character, for the number of total characters and for the use of abbreviations, but not for the non-answer and non-substantive responses. No differences are observed between optimised and not optimised versions for smartphones, except for the response time per character written.},
   author = {Melanie Revilla and Carlos Ochoa},
   doi = {10.1007/s11135-015-0273-2},
   issn = {00335177},
   issue = {6},
   journal = {Quality & Quantity},
   keywords = {0104:methodology and research technology,33411:Computer and Peripheral Equipment Manufacturing,Analysis,COMPUTER,DESIGN,Data collection,Experiments,Hypotheses,Interactive computer systems,Internet,Israel,Keyboards,MOBILE WEB SURVEYS,Mobile optimised questionnaires,Narratives,OPEN-ENDED QUESTIONS,Open narrative questions,Personal computers,Polls & surveys,Questionnaires,RESPONSES,Research responses,Response time,SIZE,Smartphones,Spain,Statistics,Studies,Telephone communications,Web surveys,research methods/tools},
   note = {Data Quality},
   pages = {2495-2513},
   pmid = {1827074964},
   title = {Open narrative questions in PC and smartphones: is the device playing a role?},
   volume = {50},
   url = {https://www.proquest.com/scholarly-journals/open-narrative-questions-pc-smartphones-is-device/docview/1827074964/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Open+narrative+questions+in+PC+and+smartphones%3A+is+the+device+playing+a+role%3F&title=Quality+and+Quantity&issn=00335177&date=2016-11-01&volume=50&issue=6&spage=2495&au=Revilla%2C+Melanie%3BOchoa%2C+Carlos&isbn=&jtitle=Quality+and+Quantity&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1007%2Fs11135-015-0273-2},
   year = {2016},
}
@article{Antoun2019,
   abstract = {Although web surveys in which respondents are encouraged to use smartphones have started to emerge, it is still unclear whether they are a promising alternative to traditional web surveys in which most respondents use desktop computers. For sample members to participate in smartphone-based surveys, they need to have access to a smartphone and agree to use it to complete the survey; this raises concerns about coverage and nonresponse, as well as measurement if those who agree to participate have any difficulty using smartphones. In an analysis of data from a smartphone versus desktop (within-subjects) experiment conducted in a probability-based web panel, we compare estimates produced by the smartphone web survey (one condition) and PC web survey (other condition). We estimate mode effects and then examine the extent to which these effects are attributable to coverage, nonresponse, and measurement errors in the smartphone-based survey. While mode effects were generally small, we find that the smartphone web survey produced biased estimates relative to PC web for a subset of survey variables. This was largely due to noncoverage and, to a lesser extent, nonresponse. We find no evidence of measurement effects. Our findings point to the trade-off of the advanced data collection opportunities of smartphones and the potential selection errors that such devices may introduce.},
   author = {Christopher Antoun and Frederick G. Conrad and Mick P. Couper and Brady T. West},
   doi = {10.1093/JSSAM/SMY002},
   issue = {1},
   journal = {Journal of Survey Statistics and Methodology},
   keywords = {BIAS,COMPUTER,Coverage error,MOBILE WEB SURVEYS,Measurement error,Mobile web survey,Nonresponse error,PC WEB,QUALITY,SELECTION,SENSITIVE TOPICS,Total survey error},
   note = {Data Quality},
   pages = {93-117},
   title = {Simultaneous estimation of multiple sources of error in a smartphone-based survey},
   volume = {7},
   year = {2019},
}
@article{Antoun2017,
   abstract = {Survey participants are increasingly responding to Web surveys on their smartphones as opposed to their personal computers (PCs), and this change brings with it some potential data-quality issues. This study reports on a randomized crossover experiment to compare the effect of two different devices, smartphones and PCs, on response quality in a Web survey conducted in a probability-based panel. Participants (n = 1,390) were invited to complete an online questionnaire on both a smartphone (mobile Web) and PC (PC Web) in sequence. We hypothesized that smartphone use would result in lower-quality responses because people are more likely to use smartphones while multitasking or while around other people and because they could have difficulty recording their answers using a small touchscreen. While we found that respondents who participated in this study were more likely to multitask and more likely to be around other people when using smartphones, these factors had little impact on data quality. Respondents were at least as likely to provide conscientious and thoughtful answers and to disclose sensitive information on smartphones as on PCs. When using smartphones, however, respondents seemed to have trouble accurately moving a small-sized slider handle and a date-picker wheel to the intended values. Overall, we find that people using smartphones can provide high-quality responses, even when their context is more distracting, as long as they are presented with question formats that are easy to use on small touchscreens. [ABSTRACT FROM AUTHOR]},
   author = {Christopher Antoun and Mick P. Couper and Frederick G. Conrad},
   issn = {0033362X},
   issue = {S1},
   journal = {Public Opinion Quarterly},
   keywords = {COMPUTER,Data quality,INTERNET,Internet surveys,Personal computers,Probability theory,Respondents,Smartphones},
   note = {Data Quality},
   pages = {280-306},
   title = {EFFECTS OF MOBILE VERSUS PC WEB ON SURVEY RESPONSE QUALITY: A CROSSOVER EXPERIMENT IN A PROBABILITY WEB PANEL.},
   volume = {81},
   year = {2017},
}
@article{,
   abstract = {Header images are typically included in web surveys to make surveys more appealing for respondents. However, headers might also induce a systematic bias in response behavior. In order to examine both the potential effects (more specifically, effects on motivation and context effects) of header images with respondents using different devices, an experiment embedded in a web survey on students' time use and stress was conducted using a probability sample of 1,326 students at the University of Bonn. Respondents were presented either with a picture of an auditorium with students sitting in a class, a picture of leisure activities on campus, or no picture, respectively. To control for position effects, pictures were placed either in the upper right or upper left of the questionnaire. The results indicate that header images attract attention in the beginning of a survey, but do not significantly increase motivation over the course of the survey. When faced with a header picture, respondents in the picture conditions evaluate their time in class differently compared to respondents in the control group. While the device providing the visibility makes no difference, effects are only significant when the picture is placed on the left side of the screen. In sum, the interaction of header placement and the content-related proximity of header content and question may alter response behavior.},
   author = {Miriam Trübner},
   doi = {10.18148/srm/2020.v14i1.7367},
   issn = {1864-3361},
   issue = {1},
   journal = {Survey Research Methods},
   keywords = {Context effects,EXPOSURE,SURVEY PARTICIPATION,device effects,header images,web surveys},
   note = {Survey Design},
   pages = {43-53},
   pmid = {WOS:000526085800004},
   title = {Effects of Header Images on Different Devices in Web Surveys},
   volume = {14},
   year = {2020},
}
@article{,
   abstract = {The increased use of smartphones in web survey responding did not only raise new research questions but also fostered new ways to research survey completion behavior. Smartphones have many built-in sensors, such as accelerometers that measure acceleration (i.e., the rate of change of velocity of an object over time). Sensor data establish new research opportunities by providing information about physical completion conditions that, for instance, can affect response quality. In this study, we explore three research questions: (1) To what extent do respondents accept to comply with motion instructions? (2) What variables affect the acceleration of smartphones? (3) Do different motion levels affect response quality? We conducted a smartphone web survey experiment using the Netquest opt-in panel in Spain and asked respondents to stand at a fix point or walk around while answering five single questions. The results reveal high compliance with motion instructions, with compliance being higher in the standing than in the walking condition. We also discovered that several variables, such as the presence of third parties, increase the acceleration of smartphones. However, the quality of responses to the five single questions did not differ significantly between the motion conditions, a finding that is in line with previous research. Our findings provide new insights into how compliance changes with motion tasks and suggest that the collection of acceleration data is a feasible and fruitful way to explore survey completion behavior. The findings also indicate that refined research on the connection between motion levels and response quality is necessary.},
   author = {Jan K. Höhne and Melanie Revilla and Stephan Schlosser},
   doi = {10.1177/1470785319858587},
   issn = {1470-7853},
   issue = {1},
   journal = {International Journal of Market Research},
   keywords = {FORMATS,MOBILE DEVICES,PC,QUESTIONS,SurveyMotion,TIMES,WEB SURVEYS,accelerometer,compliance,response quality,smartphones,survey completion behavior,web survey},
   note = {Motion instruction},
   pages = {43-57},
   pmid = {WOS:000506432000006},
   title = {Motion instructions in surveys: Compliance, acceleration, and response quality},
   volume = {62},
   year = {2020},
}
@article{Mavletova2016,
   abstract = {This study should be considered as a preliminary exploration of the effect of differential incentives on participation rates, the proportion of mobile respondents, and sample composition in web surveys. The experiment has some limitations, which should be taken into consideration in subsequent studies. First, the experiment is based on a sample of frequent mobile web users, and the results could be different from a sample of those who use mobile internet less frequently. Second, the experiment is based on a volunteer online access panel and the results could be different in a representative online panel. Third, we tested the differential incentives starting with incentives 50% higher than typical incentives. We suggest that it is worth exploring the effect of other incentives (e.g. 20% or 30% higher). Finally, we suggest that it is worth exploring the difference in participation rates between the conditions in which higher-than-typical incentives would be offered for all participants and when offered only for using a particular device. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
   author = {Aigul Mavletova and Mick P. Couper},
   doi = {10.2501/IJMR-2016-034},
   issn = {1470-7853},
   issue = {4},
   journal = {International Journal of Market Research},
   keywords = {Incentives,Internet,Marketing,Mobile Devices,Surveys,market research,mobile devices,web surveys},
   note = {Invitation mode},
   pages = {523-544},
   pmid = {2017-05795-002},
   title = {Device use in web surveys: The effect of differential incentives},
   volume = {58},
   url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2017-05795-002%26site%3dehost-live},
   year = {2016},
}
@article{Revilla2018,
   abstract = {We studied the impact of different layouts for rank order questions on respondent effort, data quality, and substantive results among PC and smartphone respondents, in an experiment in an opt-in online panel in Spain, using an order-by-click design. We experimentally varied the device, the number of columns, and, for smartphone respondents, the position of the 'next' button in questions on trust in institutions.We found some evidence of lower data quality for smartphone users but no evidence that presenting ranking items in one column performs differently than two columns. We also find little evidence that these effects differ by the number of response options presented or the number to be ranked. The placement of the 'next' button had little effect on performance on ranking items. Overall, our findings suggest that the format and layout of order-by-click questions has little effect on data quality, regardless of device used.},
   author = {Melanie Revilla and Mick P. Couper},
   doi = {10.1080/13645579.2018.1471371},
   issn = {13645579},
   issue = {6},
   journal = {International Journal of Social Research Methodology},
   keywords = {Data quality,EXPERIENCE,Internet,Layout,PANELS,PARADATA,RATINGS,Ranking,Respondents,SENSITIVE TOPICS,Smartphones,Social Sciences: Comprehensive Works,VALUES,WEB SURVEYS,Web surveys,data quality,order-by-click questions,rank order questions,smartphone optimization},
   note = {Survey Design},
   pages = {695-712},
   pmid = {2113030466},
   title = {Testing different rank order question layouts for PC and smartphone respondents},
   volume = {21},
   url = {https://www.proquest.com/scholarly-journals/testing-different-rank-order-question-layouts-pc/docview/2113030466/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Testing+different+rank+order+question+layouts+for+PC+and+smartphone+respondents&title=International+Journal+of+Social+Research+Methodology&issn=13645579&date=2018-11-01&volume=21&issue=6&spage=695&au=Revilla%2C+Melanie%3BCouper%2C+Mick+P&isbn=&jtitle=International+Journal+of+Social+Research+Methodology&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1080%2F13645579.2018.1471371},
   year = {2018},
}
@article{Mason2019,
   abstract = {This article explores the comparability of assessment tools under different format conditions. Prior studies have not considered the interaction of format and device on time to complete an assessment and have instead treated each of them separately with conflicting results. This study assesses, by linear regressions using web-based data, the performance of multiple devices under varying formats while controlling for non-device factors such as demographic information. The results of this study add to the growing literature on the equivalence among devices and formats used to collect and interpret performance in a variety of organizational settings.},
   author = {Robert Mason and Kyle C. Huff},
   doi = {10.1080/13645579.2018.1542150},
   issn = {13645579},
   issue = {3},
   journal = {International Journal of Social Research Methodology},
   keywords = {ABILITY,COMPUTERS,Computer based,DESIGN,Demographic aspects,EQUIVALENCE,Evaluation,Internet,MOBILE,Mobile,Organizational effectiveness,PAPER,Questionnaires,Social Sciences: Comprehensive Works,TESTS,format,questionnaire,survey,usability,web},
   note = {Data Quality},
   pages = {271-280},
   pmid = {2196500543},
   title = {The effect of format and device on the performance and usability of web-based questionnaires},
   volume = {22},
   url = {https://www.proquest.com/scholarly-journals/effect-format-device-on-performance-usability-web/docview/2196500543/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=The+effect+of+format+and+device+on+the+performance+and+usability+of+web-based+questionnaires&title=International+Journal+of+Social+Research+Methodology&issn=13645579&date=2019-05-01&volume=22&issue=3&spage=271&au=Mason%2C+Robert%3BHuff%2C+Kyle&isbn=&jtitle=International+Journal+of+Social+Research+Methodology&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1080%2F13645579.2018.1542150},
   year = {2019},
}
@article{Ha2020,
   abstract = {Purpose Low response rates in web surveys and the use of different devices in entering web survey responses are the two main challenges to response quality of web surveys. The purpose of this study is to compare the effects of using interviewers to recruit participants in computer-assisted self-administered interviews (CASI) vs computer-assisted personal interviews (CAPI) and smartphones vs computers on participation rate and web survey response quality. Design/methodology/approach Two field experiments using two similar media use studies on US college students were conducted to compare response quality in different survey modes and response devices. Findings Response quality of computer entry was better than smartphone entry in both studies for open-ended and closed-ended question formats. Device effect was only significant on overall completion rate when interviewers were present. Practical implications Survey researchers are given guidance how to conduct online surveys using different devices and choice of question format to maximize survey response quality. The benefits and limitations of using an interviewer to recruit participants and smartphones as web survey response devices are discussed. Social implications It shows how computer-assisted self-interviews and smartphones can improve response quality and participation for underprivileged groups. Originality/value This is the first study to compare response quality in different question formats between CASI, e-mailed delivered online surveys and CAPI. It demonstrates the importance of human factor in creating sense of obligation to improve response quality.},
   author = {Louisa, Ha and Chenjie Zhang and Weiwei Jiang},
   doi = {10.1108/INTR-09-2018-0417},
   issn = {1066-2243},
   issue = {6},
   journal = {Internet Research},
   keywords = {Computer-assisted personal Interview,Computer-assisted self-interview,DESIGN,Data quality,FACE-TO-FACE,INDICATORS,Interviewer,MAIL,NONRESPONSE,ONLINE SURVEYS,Smartphone,Survey mode},
   note = {Data Quality},
   pages = {1763-1781},
   pmid = {WOS:000552986300001},
   title = {Data quality comparison between computers and smartphones in different web survey modes and question formats},
   volume = {30},
   year = {2020},
}
@article{,
   abstract = {Participation in web surveys via smartphones increased continuously in recent years. The reasons for this increase are a growing proportion of smartphone owners and an increase in mobile Internet access. However, research has shown that smartphone respondents are frequently distracted and/or multitasking, which might affect completion and response behavior in a negative way. We propose 'SurveyMotion (SMotion)', a JavaScript-based tool for mobile devices that can gather information about respondents' motions during web survey completion by using sensor data. Specifically, we collect data about the total acceleration (TA) of smartphones. We conducted a lab experiment and varied the form of survey completion (e.g. standing or walking). Furthermore, we employed questions with different response formats (e.g. radio buttons and sliders) and measured response times. The results reveal that SMotion detects higher TAs of smartphones for respondents with comparatively higher motion levels. In addition, respondents' motion level affects response times and the quality of responses given. The SMotion tool promotes the exploration of how respondents complete mobile web surveys and could be employed to understand how future mobile web surveys are completed.},
   author = {Jan K. Höhne and Stephan Schlosser},
   doi = {10.1080/13645579.2018.1550279},
   issn = {13645579},
   issue = {4},
   journal = {International Journal of Social Research Methodology},
   keywords = {Acceleration,Behavior,Completion,FORMATS,GRIDS,Internet access,JavaScript,MULTITASKING,Owners,PARADATA,PCS,Participation,Polls & surveys,QUALITY,Radio,Reaction time,Respondents,Smartphones,Social Sciences: Comprehensive Works,Telephone communications,Walking,mobile sensors,passive data collection,response quality,smartphones,web survey},
   note = {Sensor data},
   pages = {379-391},
   pmid = {2220295195},
   title = {SurveyMotion: what can we learn from sensor data about respondents' completion and response behavior in mobile web surveys?},
   volume = {22},
   url = {https://www.proquest.com/scholarly-journals/surveymotion-what-can-we-learn-sensor-data-about/docview/2220295195/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=SurveyMotion%3A+what+can+we+learn+from+sensor+data+about+respondents%27+completion+and+response+behavior+in+mobile+web+surveys%3F&title=International+Journal+of+Social+Research+Methodology&issn=13645579&date=2019-07-01&volume=22&issue=4&spage=379&au=H%C3%B6hne%2C+Jan+Karem%3BSchlosser%2C+Stephan&isbn=&jtitle=International+Journal+of+Social+Research+Methodology&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1080%2F13645579.2018.1550279},
   year = {2019},
}
@article{Weigold2021,
   abstract = {There is a limited body of experimental research examining the comparability of completing self-report surveys using different computerized devices. Additionally, available literature has not used complete or optimal procedures for determining device equivalence. The current study examined the comparability of surveys completed using paper-and-pencil and three popular devices: smartphone, tablet, and desktop computer. Participants consisted of 211 college students randomly assigned to conditions who completed measures of personality, social desirability, and computer self-efficacy. Results showed evidence of qualitative equivalence (internal consistency and subscale intercorrelations) across conditions. For quantitative and auxiliary equivalence, both equivalence testing and Bayesian analyses were conducted. Equivalence testing indicated quantitative (mean score) equivalence, as well as comparability for one aspect of auxiliary equivalence (missing data). Other aspects of auxiliary equivalence (completion time and comfort completing questionnaires) suggested potentially meaningful differences. Bayesian analyses typically replicated these results, with some notable exceptions regarding auxiliary equivalence.},
   author = {Arne Weigold and Ingrid K. Weigold and Stephanie A. Dykema and Naomi M. Drakeford and Caitlin A. Martin-Wagar},
   doi = {10.1080/10447318.2020.1848159},
   issn = {1044-7318},
   issue = {8},
   journal = {International Journal of Human-Computer Interaction},
   keywords = {INTERNET,MOBILE,QUALITY,SELF-EFFICACY,SHORT FORMS,SOCIAL DESIRABILITY,TECHNOLOGY,VALIDATION,WEB},
   note = {Data Quality},
   pages = {803-814},
   pmid = {WOS:000590280000001},
   title = {Computerized Device Equivalence: A Comparison of Surveys Completed Using A Smartphone, Tablet, Desktop Computer, and Paper-and-Pencil},
   volume = {37},
   year = {2021},
}
@article{Mavletova2016,
   abstract = {There is some evidence that a scrolling design may reduce breakoffs in mobile web surveys compared to a paging design, but there is little empirical evidence to guide the choice of the optimal number of items per page. We investigate the effect of the number of items presented on a page on data quality in two types of questionnaires: with or without user-controlled skips. Three versions of a 30-item instrument were compared, with 5, 15, or all 30 questions presented on a page, in two different surveys, one with skips and one without. We found that displaying 30 items on a page reduced the breakoff rate by almost one-third compared to presenting five items per page in the questionnaire without skips, but the difference was not statistically significant. In both surveys with and without skips, the completion times were significantly lower in the 30-item per page condition; however, item nonresponse rates were also higher. We give some practical recommendations to guide choices while designing questionnaires for mobile web surveys. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
   author = {Aigul Mavletova and Mick P. Couper},
   doi = {10.1177/1525822X15595151},
   issn = {1525-822X},
   issue = {2},
   journal = {Field methods},
   keywords = {0104:methodology and research technology,0514:culture and social structure,Anthropology,COMPUTER,Choices,DESIGN,DEVICES,Data quality,Internet,Item Analysis (Test),Online Experiments,PC,Polls & surveys,Questionnaires,SMARTPHONE,Surveys,Test Items,breakoff rates,mobile web surveys,research methods/tools,scrolling,skips,social anthropology},
   note = {Survey Design},
   pages = {170-193},
   pmid = {2016-17809-005},
   title = {Grouping of items in mobile web questionnaires},
   volume = {28},
   url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2016-17809-005%26site%3dehost-live https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Grouping+of+Items+in+Mobile+Web+Questionnaires&title=Field+Methods&issn=1525822X&date=2016-05-01&volume=28&issue=2&spage=170&au=Mavletova%2C+Aigul%3BCouper%2C+Mick+P&isbn=&jtitle=Field+Methods&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F1525822X15595151},
   year = {2016},
}
@article{Grady2019,
   abstract = {Across two studies, we aimed to determine the row and column size in matrix-style questions that best optimizes participant experience and data quality for computer and mobile users. In Study 1 (N = 2,492), respondents completed 20 questions (comprising four short scales) presented in a matrix grid (converted to item-by-item format on mobile phones). We varied the number of rows (5, 10, or 20) and columns (3, 5, or 7) of the matrix on each page. Outcomes included both data quality (straightlining, item skip rate, and internal reliability of scales) and survey experience measures (dropout rate, rating of survey experience, and completion time). Results for row size revealed dropout rate and reported survey difficulty increased as row size increased. For column size, seven columns increased the completion time of the survey, while three columns produced lower scale reliability. There was no interaction between row and column size. The best overall size tested was a 5 ? 5 matrix. In Study 2 (N = 2,570), we tested whether the effects of row size replicated when using a single 20-item scale that crossed page breaks and found that participant survey ratings were still best in the five-row condition. These results suggest that having around five rows or potentially fewer per page, and around five columns for answer options, gives the optimal survey experience, with equal or better data quality, when using matrix-style questions in an online survey. These recommendations will help researchers gain the benefits of using matrices in their surveys with the least downsides of the format.},
   author = {Rebecca H. Grady and Rachel L. Greenspan and Mingna Liu},
   doi = {10.1177/0894439318773733},
   issn = {0894-4393},
   issue = {3},
   journal = {Social Science Computer Review},
   keywords = {Balances (scales),Completion time,DESIGN,Data quality,Education--Computer Applications,Format,GRIDS,Internet,Matrices,Mobile computing,NUMBER,Polls & surveys,RESPONSES,Reliability,WEB,data quality,matrix format,matrix size,web survey},
   note = {Survey Design},
   pages = {435-445},
   title = {What Is the Best Size for Matrix-Style Questions in Online Surveys?},
   volume = {37},
   url = {https://doi.org/10.1177/0894439318773733 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=What+Is+the+Best+Size+for+Matrix-Style+Questions+in+Online+Surveys%3F&title=Social+Science+Computer+Review&issn=08944393&date=2019-06-01&volume=37&issue=3&spage=435&au=Grady%2C+Rebecca+Hofstein%3BGreenspan%2C+Rachel+Leigh%3BLiu+Mingnan&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439318773733},
   year = {2019},
}
@article{Antoun2020,
   abstract = {This article compares the factors affecting completion times (CTs) to web survey questions when they are answered using two different devices: personal computers (PCs) and smartphones. Several studies have reported longer CTs when respondents use smartphones than PCs. This is a concern to survey researchers because longer CTs may increase respondent burden and the risk of breakoff. However, few studies have analyzed the specific reasons for the time difference. In this analysis, we analyzed timing data from 836 respondents who completed the same web survey twice, once using a smartphone and once using PC, as part of a randomized crossover experiment in the Longitudinal Internet Studies for the Social Sciences panel. The survey contained a mix of questions (single choice, numeric entry, and text entry) that were displayed on separate pages. We included both page-level and respondent-level factors that may have contributed to the time difference between devices in cross-classified multilevel models. We found that respondents took about 1.4 times longer when using smartphones than PCs. This difference was larger when a page had more than one question or required text entry. The difference was also larger among respondents who had relatively low levels of familiarity and experience using smartphones. Respondent multitasking was associated with slower CTs, regardless of the device used. Practical implications and avenues for future research are discussed.},
   author = {Christopher Antoun and Alexandru Cernat},
   doi = {10.1177/0894439318823703},
   issn = {0894-4393},
   issue = {4},
   journal = {Social Science Computer Review},
   keywords = {Computers,Experimental Subjects,MOBILE,Online Surveys,Reaction Time,Smartphones,Testing Methods,mobile web surveys,response times,smartphone surveys},
   note = {Data Quality},
   pages = {477-489},
   title = {Factors Affecting Completion Times: A Comparative Analysis of Smartphone and PC Web Surveys},
   volume = {38},
   year = {2020},
}
@article{Revilla2017,
   abstract = {Some respondents already complete web surveys via mobile devices. These devices vary at several levels from PCs. In particular, we expect differences when grid questions are used due to the lower visibility on mobile devices and because in questionnaires optimized to be completed through smartphones, grids are split up into an item-by-item format. This paper reports the results of a two-wave experiment conducted in Spain in 2015, comparing three groups: PCs, smartphones not-optimized, or smartphones optimized. We found similar levels of interitem correlations, longer completion times for grid questions for smartphone respondents, and sometimes less non-differentiation for PCs. Thus, using the item-by-item format for smartphones and PCs seems the most appropriate way to improve comparability. [ABSTRACT FROM AUTHOR]},
   author = {Melanie Revilla and Daniele Toninelli and Carlos Ochoa},
   issn = {07365853},
   issue = {1},
   journal = {Telematics & Informatics},
   keywords = {Comparative studies,Completion time,Grid computing,Grids,Interitem correlation,Internet surveys,Mobile communication systems,Non-differentiation,Personal computers,Smartphones,Web surveys},
   note = {Survey Design},
   pages = {30-42},
   title = {An experiment comparing grids and item-by-item formats in web surveys completed through PCs and smartphones.},
   volume = {34},
   url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dufh%26AN%3d117440989%26site%3dehost-live},
   year = {2017},
}
@article{Bosch2021,
   abstract = {To involve Millennials in survey participation, and obtain high-quality answers from them, survey designers may require new tools that better catch Millennials' interest and attention. One key new tool that could improve the communication and make the survey participation more attractive to young respondents are the emojis. We used data from a survey conducted among Millennials by the online fieldwork company Netquest in Spain and Mexico (n = 1614) to determine how emojis can be used in mobile web surveys, in particular in open-ended questions, and how their use can affect data quality, completion time, and survey evaluation. Overall, results show a high willingness of Millennials to use emojis in surveys (both stated and actual use) and a positive impact of encouraging Millennials to use emojis in open-ended questions on the amount of information conveyed, the completion time and the survey enjoyment.},
   author = {Oriol J. Bosch and Melanie Revilla},
   doi = {10.1007/s11135-020-00994-8},
   issn = {00335177},
   journal = {Quality & Quantity},
   keywords = {Data quality,Emojis,Internet,Mexico,Millennials,Mobile web surveys,Participation,Polls & surveys,Spain,Statistics,Survey evaluation},
   note = {Survey Design},
   pages = {39-61},
   pmid = {2486883703},
   title = {Using emojis in mobile web surveys for Millennials? A study in Spain and Mexico},
   volume = {55},
   url = {https://www.proquest.com/scholarly-journals/using-emojis-mobile-web-surveys-millennials-study/docview/2486883703/se-2 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Using+emojis+in+mobile+web+surveys+for+Millennials%3F+A+study+in+Spain+and+Mexico&title=Quality+and+Quantity&issn=00335177&date=2021-02-01&volume=55&issue=1&spage=39&au=Bosch%2C+Oriol+J%3BRevilla%2C+Melanie&isbn=&jtitle=Quality+and+Quantity&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1007%2Fs11135-020-00994-8},
   year = {2021},
}
@article{Keusch2020,
   abstract = {Researchers are combining self-reports from mobile surveys with passive data collection using sensors and apps on smartphones increasingly more often. While smartphones are commonly used in some groups of individuals, smartphone penetration is significantly lower in other groups. In addition, different operating systems (OSs) limit how mobile data can be collected passively. These limitations cause concern about coverage error in studies targeting the general population. Based on data from the Panel Study Labour Market and Social Security (PASS), an annual probability-based mixed-mode survey on the labor market and poverty in Germany, we find that smartphone ownership and ownership of smartphones with specific OSs are correlated with a number of sociodemographic and substantive variables. The use of weighting techniques based on sociodemographic information available for both owners and nonowners reduces these differences but does not eliminate them.},
   author = {Florian Keusch and Sebastian Bähr and Georg-Christoph Haas and Frauke Kreuter and Mark Trappmann},
   doi = {10.1177/0049124120914924},
   issn = {0049-1241},
   journal = {Sociological Methods & Research},
   keywords = {coverage error,mobile web surveys,operating systems,passive mobile data collection,smartphones},
   note = {Who is using mobile? and Passive Data Collection},
   title = {Coverage Error in Data Collection Combining Mobile Surveys With Passive Measurement Using Apps: Data From a German National Survey},
   url = {https://doi.org/10.1177/0049124120914924},
   year = {2020},
}
@article{Tourangeau2018,
   abstract = {Does completing a web survey on a smartphone or tablet computer reduce the quality of the data obtained compared to completing the survey on a laptop computer? This is an important question, since a growing proportion of web surveys are done on smartphones and tablets. Several earlier studies have attempted to gauge the effects of the switch from personal computers to mobile devices on data quality. We carried out a field experiment in eight counties around the United States that compared responses obtained by smartphones, tablets, and laptop computers. We examined a range of data quality measures including completion times, rates of missing data, straightlining, and the reliability and validity of scale responses. A unique feature of our study design is that it minimized selection effects; we provided the randomly determined device on which respondents completed the survey after they agreed to take part. As a result, respondents may have been using a device (e.g., a smartphone) for the first time. However, like many of the prior studies examining mobile devices, we find few effects of the type of device on data quality.},
   author = {Roger Tourangeau and Hanyu Sun and Ting Yan and Aaron Maitland and Gonzalo Rivero and Douglas Williams},
   doi = {10.1177/0894439317719438},
   issn = {0894-4393},
   issue = {5},
   journal = {Social Science Computer Review},
   keywords = {COMPUTER,DESIGN,Data quality,Education--Computer Applications,Electronic devices,Internet,MOBILE DEVICES,Missing data,Mobile computing,PC,Personal computers,Polls & surveys,Rangefinding,Research responses,SENSITIVE TOPICS,Smartphones,Tablet computers,Telephone communications,data quality,measurement error,mobile devices,smartphones},
   note = {Data Quality},
   pages = {542-556},
   title = {Web Surveys by Smartphones and Tablets: Effects on Data Quality},
   volume = {36},
   url = {https://doi.org/10.1177/0894439317719438 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Web+Surveys+by+Smartphones+and+Tablets&title=Social+Science+Computer+Review&issn=08944393&date=2018-10-01&volume=36&issue=5&spage=542&au=Tourangeau%2C+Roger%3BSun+Hanyu%3BTing%2C+Yan%3BMaitland%2C+Aaron%3BRivero%2C+Gonzalo%3BWilliams%2C+Douglas&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439317719438},
   year = {2018},
}
@article{Revilla2017,
   abstract = {The development of web surveys has been accompanied by the emergence of new scales, taking advantages of the visual and interactive features provided by the Internet like drop-down menus, sliders, drag-and-drop, or order-by-click scales. This article focuses on the order-by-click scales, studying the comparability of the data obtained for this scale when answered through PCs versus smartphones. I used data from an experiment where panelists from the Netquest opt-in panel in Spain were randomly assigned to a PC, smartphone optimized, or smartphone not-optimized version of the same questionnaire in two waves. I found significant differences due to the device and optimization at least for some indicators and questions.},
   author = {Melanie Revilla},
   doi = {10.1177/1525822X16674701},
   issn = {1525-822X},
   issue = {3},
   journal = {Field methods},
   keywords = {0514:culture and social structure,Anthropology,COMPUTER,Internet,MOBILE,Microcomputers,Mobile Phones,PANELS,PC,Polls & surveys,Questionnaires,Research responses,Smartphones,Surveys,Websites,smartphones,social anthropology,web surveys},
   note = {Survey Design},
   pages = {266-280},
   title = {Are There Differences Depending on the Device Used to Complete a Web Survey (PC or Smartphone) for Order-by-click Questions?},
   volume = {29},
   url = {https://doi.org/10.1177/1525822X16674701 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Are+There+Differences+Depending+on+the+Device+Used+to+Complete+a+Web+Survey+%28PC+or+Smartphone%29+for+Order-by-click+Questions%3F&title=Field+Methods&issn=1525822X&date=2017-08-01&volume=29&issue=3&spage=266&au=Revilla%2C+Melanie&isbn=&jtitle=Field+Methods&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F1525822X16674701},
   year = {2017},
}
@article{Couper2017,
   abstract = {Surveys completed on mobile web devices (smartphones) have been found to take longer than surveys completed on a PC. This has been found both in surveys where respondents can choose which device they use and in surveys where respondents are randomly assigned to devices. A number of potential explanations have been offered for these findings, including (1) slower transmission over cellular or Wi-Fi networks, (2) the difficulty of reading questions and selecting responses on a small device, and (3) the increased mobility of mobile web users who have more distractions while answering web surveys. In a secondary analysis of student surveys, we find that only about one-fifth of the time difference can be accounted for by transmission time (between-page time) with the balance being within-page time differences. Using multilevel models, we explore possible page-level (question-level) and respondent-level factors that may contribute to the time difference. We find that much of the time difference can be accounted for by the additional scrolling required on mobile devices, especially for grid questions.},
   author = {Mick P. Couper and Gregg J. Peterson},
   doi = {10.1177/0894439316629932},
   issn = {0894-4393},
   issue = {3},
   journal = {Social Science Computer Review},
   keywords = {0188:methodology and research technology,83:Social and Behavioral Sciences (CI),AGE,COMPUTER,Cellular communication,Computer software,Data collection,Education--Computer Applications,Electronic devices,Internet,MOBILE,ONLINE SURVEYS,PARADATA,PC,Polls & surveys,QUALITY,RESPONSE-TIMES,Respondents,Response time,Scrolling,Smartphones,Time,Web sites,computer methods, media, & applications,online surveys,smartphone surveys,survey completion times,web surveys},
   note = {Data Quality},
   pages = {357-377},
   title = {Why Do Web Surveys Take Longer on Smartphones?},
   volume = {35},
   url = {https://doi.org/10.1177/0894439316629932 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Why+Do+Web+Surveys+Take+Longer+on+Smartphones%3F&title=Social+Science+Computer+Review&issn=08944393&date=2017-06-01&volume=35&issue=3&spage=357&au=Couper%2C+Mick+P%3BPeterson%2C+Gregg+J&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439316629932},
   year = {2017},
}
@article{Gummer2015,
   abstract = {Interview duration is an important variable in web surveys because it is a direct measure of the response burden. In this article, we analyze the effects of the survey design, respondent characteristics, and the interaction between these effects on interview duration. For that purpose, we applied multilevel analysis to a data set of 21 web surveys on political attitudes and behavior. Our results showed that factors on both levels, the individual and the survey level, had effects on interview duration. However, the larger share of the variation in interview duration is explained by the characteristics of the respondents. In this respect, we illustrate the impact of mobile devices and panel recruitment on interview duration. In addition, we found important relationships between the respondents? attitudes and how a web survey is designed: Highly motivated respondents spent significantly more time answering cognitively demanding questions than less motivated respondents. When planning a survey, not only the number and formats of questions need to be taken into account but also the expected sample composition and how the participants will respond to the design of the web survey.},
   author = {Tobias Gummer and Joss Roßmann},
   doi = {10.1177/0894439314533479},
   issn = {0894-4393},
   issue = {2},
   journal = {Social Science Computer Review},
   keywords = {AGE,Attitude surveys,BURDEN,DATA QUALITY,DESIGN,Data analysis,EXPERIENCE,Education--Computer Applications,INDICATORS,Internet,Interviews,LENGTH,Mobile Phones,ORDER,Political Attitudes,Political behavior,RESPONSE-TIMES,Research methodology,SURVEY QUESTIONS,Web services,interview duration,mobile devices,multilevel analysis,online panels,web survey design},
   note = {Data Quality},
   pages = {217-234},
   title = {Explaining Interview Duration in Web Surveys: A Multilevel Approach},
   volume = {33},
   url = {https://doi.org/10.1177/0894439314533479 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Explaining+Interview+Duration+in+Web+Surveys%3A+A+Multilevel+Approach&title=Social+Science+Computer+Review&issn=08944393&date=2015-04-01&volume=33&issue=2&spage=217&au=Gummer%2C+Tobias%3BRossmann%2C+Joss&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439314533479},
   year = {2015},
}
@article{Mendelson2017,
   abstract = {Videos are often used in web surveys to assess attitudes. While including videos may allow researchers to test immediate reactions, there may be issues associated with displaying videos that are overlooked. In this article, we examine the effects of using video stimuli on responses in a probability-based web survey. Specifically, we evaluate the association between demographics, mobile device usage, and the ability to view videos; differences in ad recall based on whether respondents saw a video or still images of the video; whether respondents? complete viewing of videos is related to presentation order; and the data quality of follow-up questions to the videos as a function of presentation order and complete viewing. Overall, we found that respondents using mobile browsers were less likely to be able to view videos in the survey. Those who could view videos were more likely to indicate recall compared to those who viewed images, and videos that were shown later in the survey were viewed in their entirety less frequently than those shown earlier. These results directly pertain to the legitimacy of using videos in web surveys to gather data about attitudes.},
   author = {Jonathan Mendelson and Jennifer L. Gibson and Jennifer Romano-Bergstrom},
   doi = {10.1177/0894439316662439},
   issn = {0894-4393},
   issue = {5},
   journal = {Social Science Computer Review},
   keywords = {Attitude Measures,Digital Video,Mobile Devices,Surveys,Test Construction,Video Display Units,advertising,data quality,measurement,videos,web surveys},
   note = {Survey Design},
   pages = {654-665},
   title = {Displaying Videos in Web Surveys: Implications for Complete Viewing and Survey Responses},
   volume = {35},
   url = {https://doi.org/10.1177/0894439316662439},
   year = {2017},
}
@article{Bosch2019,
   abstract = {Most mobile devices nowadays have a camera. Besides, posting and sharing images have been found as one of the most frequent and engaging Internet activities. However, to our knowledge, no research has explored the feasibility of asking respondents of online surveys to upload images to answer survey questions. The main goal of this article is to investigate the viability of asking respondents of an online opt-in panel to upload during a mobile web survey: First, a photo taken in the moment, and second, an image already saved on their smartphone. In addition, we want to test to what extent the Google Vision application programming interface (API), which can label images into categories, produces similar tags than a human coder. Overall, results from a survey conducted among millennials in Spain and Mexico (N = 1,614) show that more than half of the respondents uploaded an image. Of those, 77.3% and 83.4%, respectively, complied with what the question asked. Moreover, respectively, 52.4% and 65.0% of the images were similarly codified by the Google Vision API and the human coder. In addition, the API codified 1,818 images in less than 5 min, whereas the human coder spent nearly 35 hours to complete the same task.},
   author = {Oriol J. Bosch and Melanie Revilla and Ezequiel Paura},
   doi = {10.1177/0894439318791515},
   issn = {0894-4393},
   issue = {5},
   journal = {Social Science Computer Review},
   keywords = {API,Application programming interface,COMMUNICATION,Cameras,Codification,Computer vision,Computers,Education--Computer Applications,Electronic devices,Images,Internet,MILLENNIALS,Millennials,Mobile Devices,Online Surveys,PHOTO,Photography,Polls & surveys,Questions,Smartphones,Viability,computer vision,image recognition,mobile web survey,new data types,smartphone},
   note = {Image input},
   pages = {669-683},
   title = {Answering Mobile Surveys With Images: An Exploration Using a Computer Vision API},
   volume = {37},
   url = {https://doi.org/10.1177/0894439318791515 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Answering+Mobile+Surveys+With+Images%3A+An+Exploration+Using+a+Computer+Vision+API&title=Social+Science+Computer+Review&issn=08944393&date=2019-10-01&volume=37&issue=5&spage=669&au=Bosch%2C+Oriol+J%3BRevilla%2C+Melanie%3BPaura+Ezequiel&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439318791515},
   year = {2019},
}
@article{Bosch2019,
   abstract = {Little is known about the reliability and validity in web surveys, although this is crucial information to evaluate how accurate the results might be and/or to correct for measurement errors. In particular, there are few studies based on probability-based samples for web surveys, looking at web-specific response scales and considering the impact of having smartphone respondents. In this article, we start filling these gaps by estimating the measurement quality of sliders compared to radio button scales controlling for the device respondents used. We conducted therefore two multitrait?multimethod (MTMM) experiments in the Norwegian Citizen Panel (NCP), a probability-based online panel. Overall, we find that if smartphone respondents represent a nonnegligible part of the whole sample, offering the response options in form of a slider or a radio button scale leads to a quite similar measurement quality. This means that sliders could be used more often without harming the data quality. Besides, if there are no smartphone respondents, we find that sliders can also be used, but that the marker should be placed initially in the middle rather than on the left side. However, in practice, there is no need to shift from radio buttons to sliders since the quality is not highly improved by providing sliders.},
   author = {Oriol J. Bosch and Melanie Revilla and Anna DeCastellarnau and Wiebke Weber},
   doi = {10.1177/0894439317750089},
   issn = {0894-4393},
   issue = {1},
   journal = {Social Science Computer Review},
   keywords = {Buttons,COMPUTER,Data quality,Education--Computer Applications,Internet,MODELS,Measurement,Measurement errors,Methodology,Online Surveys,Polls & surveys,Quality,Radio,Reliability,Research Quality,Scales,Smartphones,Test Forms,Test Reliability,Test Validity,VISUAL ANALOG SCALES,WEB SURVEYS,measurement quality,multitrait-multimethod experiment,multitrait–multimethod experiment,probability-based online panel,radio buttons,reliability and validity,slider scales,web surveys},
   note = {Survey Design},
   pages = {119-132},
   title = {Measurement Reliability, Validity, and Quality of Slider Versus Radio Button Scales in an Online Probability-Based Panel in Norway},
   volume = {37},
   url = {https://doi.org/10.1177/0894439317750089 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Measurement+Reliability%2C+Validity%2C+and+Quality+of+Slider+Versus+Radio+Button+Scales+in+an+Online+Probability-Based+Panel+in+Norway&title=Social+Science+Computer+Review&issn=08944393&date=2019-02-01&volume=37&issue=1&spage=119&au=Bosch%2C+Oriol+J%3BRevilla%2C+Melanie%3BDeCastellarnau+Anna%3BWeber%2C+Wiebke&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439317750089},
   year = {2019},
}
@article{Schlosser2018,
   abstract = {In this article, we present a study on the data quality and the response process of mobile online surveys using an experimental design as compared to a standard computer. We used the following indicators to measure data quality and response properties: reaction time to survey invitation, break-off rate, item nonresponse, length of responses to open-ended questions and survey transmission, processing, and completion time. With regard to completion time, we also explored the significance of the place as well as the situation in which the survey was completed, the kind of Internet connection the respondents had as well as the hardware properties of the devices used to answer the online survey. Our results suggest comparable data quality and response properties in most aspects: There were no noticeable differences between computer and mobile users as regards break-off rate, item nonresponse, and length of responses to open-ended questions, nor the place where the survey was completed. However, it took respondents in the mobile group longer to complete the survey as compared to respondents answering the online survey on their computer. In terms of the completion time, there was a significant decrease in the differences between mobile devices and PCs when respondents used technically advanced mobile devices and had access to a fast Internet connection.},
   author = {Stephan Schlosser and Anja Mays},
   doi = {10.1177/0894439317698437},
   issn = {0894-4393},
   issue = {2},
   journal = {Social Science Computer Review},
   keywords = {Access,COMPUTER,Completion time,Computers,Data Sets,Data quality,Education--Computer Applications,Electronic devices,Internet,Mobile Devices,Mobile computing,Polls & surveys,Properties (attributes),Reaction time,Research design,Research responses,SMARTPHONE,Studies,Surveys,TIMES,Technology,WEB SURVEY,break-off rate,data quality,item nonresponse,length of responses to open-ended questions,mobile device,mode effect,online survey,paradata,reaction time to survey invitation,response time},
   note = {Data Quality},
   pages = {212-230},
   title = {Mobile and Dirty: Does Using Mobile Devices Affect the Data Quality and the Response Process of Online Surveys?},
   volume = {36},
   url = {https://doi.org/10.1177/0894439317698437 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Mobile+and+Dirty&title=Social+Science+Computer+Review&issn=08944393&date=2018-04-01&volume=36&issue=2&spage=212&au=Schlosser%2C+Stephan%3BMays%2C+Anja&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439317698437},
   year = {2018},
}
@article{Revilla2020,
   abstract = {We implemented an experiment within a smartphone web survey to explore the feasibility of using voice input (VI) options. Based on device used, participants were randomly assigned to a treatment or control group. Respondents in the iPhone operating system (iOS) treatment group were asked to use the dictation button, in which the voice was translated automatically into text by the device. Respondents with Android devices were asked to use a VI button which recorded the voice and transmitted the audio file. Both control groups were asked to answer open-ended questions using standard text entry. We found that the use of VI still presents a number of challenges for respondents. Voice recording (Android) led to substantially higher nonresponse, whereas dictation (iOS) led to slightly higher nonresponse, relative to text input. However, completion time was significantly reduced using VI. Among those who provided an answer, when dictation was used, we found fewer valid answers and less information provided, whereas for voice recording, longer and more elaborated answers were obtained. Voice recording (Android) led to significantly lower survey evaluations, but not dictation (iOS).},
   author = {Melanie Revilla and Mick P. Couper and Oriol J. Bosch and Marc Asensio},
   doi = {10.1177/0894439318810715},
   issn = {0894-4393},
   issue = {2},
   journal = {Social Science Computer Review},
   keywords = {Audio data,COMPUTER,Completion time,Education--Computer Applications,Internet,MOBILE DEVICES,PC,PROBABILITY-BASED PANEL,Polls & surveys,QUESTIONS,RESPONSE QUALITY,Recording,Research responses,Smartphones,Voice,data quality,dictation,mobile web surveys,speech-to-text,voice recording},
   note = {Voice Input},
   pages = {207-224},
   title = {Testing the Use of Voice Input in a Smartphone Web Survey},
   volume = {38},
   url = {https://doi.org/10.1177/0894439318810715 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Testing+the+Use+of+Voice+Input+in+a+Smartphone+Web+Survey&title=Social+Science+Computer+Review&issn=08944393&date=2020-04-01&volume=38&issue=2&spage=207&au=Revilla%2C+Melanie%3BCouper%2C+Mick+P%3BBosch%2C+Oriol+J%3BAsensio%2C+Marc&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439318810715},
   year = {2020},
}
@article{Mavletova2013,
   abstract = {The considerable growth in the number of smart mobile devices with a fast Internet connection provides new challenges for survey researchers. In this article, I compare the data quality between two survey modes: self-administered web surveys conducted via personal computer and those conducted via mobile phones. Data quality is compared based on five indicators: (a) completion rates, (b) response order effects, (c) social desirability, (d) non-substantive responses, and (e) length of open answers. I hypothesized that mobile web surveys would result in lower completion rates, stronger response order effects, and less elaborate answers to open-ended questions. No difference was expected in the level of reporting in sensitive items and in the rate of non-substantive responses. To test the assumptions, an experiment with two survey modes was conducted using a volunteer online access panel in Russia. As expected, mobile web was associated with a lower completion rate, shorter length of open answers, and similar level of socially undesirable and non-substantive responses. However, no stronger primacy effects in mobile web survey mode were found.},
   author = {Aigul Mavletova},
   doi = {10.1177/0894439313485201},
   issn = {0894-4393},
   issue = {6},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,AUDIO,BIAS,Comparative studies,Data Collection,Data Quality,Data analysis,Education--Computer Applications,Internet,Internet access,METAANALYSIS,MODE,Methodology (Data Collection),Microcomputers,Mobile Devices,NONRESPONSE,Polls & surveys,Quality,RESPONSE RATES,Russia,SENSITIVE QUESTIONS,Smartphones,Social Desirability,Surveys,Volunteers,article,completion rates,computer methods, media, & applications,data quality,length of open-ended questions,mobile devices,mobile web surveys,non-substantive responses,personal computers,primacy effects,response order effects,social desirability,web surveys,web surveys mobile web surveys data quality completion rates response order effects primacy effects social desirability non-substantive responses length of open-ended questions},
   note = {Data Quality},
   pages = {725-743},
   title = {Data Quality in PC and Mobile Web Surveys},
   volume = {31},
   url = {https://doi.org/10.1177/0894439313485201 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Data+Quality+in+PC+and+Mobile+Web+Surveys&title=Social+Science+Computer+Review&issn=08944393&date=2013-12-01&volume=31&issue=6&spage=725&au=Mavletova%2C+Aigul&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201405063&rft_id=info:doi/10.1177%2F0894439313485201 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Data+Quality+in+PC+and+Mobile+Web+Surveys&title=Social+Science+Computer+Review&issn=08944393&date=2013-12-01&volume=31&issue=6&spage=725&au=Mavletova%2C+Aigul&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439313485201},
   year = {2013},
}
@article{Mavletova2018,
   abstract = {While grids or matrix questions are a widely used format in PC web surveys, there is no agreement on the format in mobile web surveys. We conducted a two-wave experiment in an opt in panel in Russia, varying the question format (grid format and item-by-item format) and device respondents used for survey completion (smartphone and PC). The 1,678 respondents completed the survey in the assigned conditions in the first wave and 1,079 in the second wave. Overall, we found somewhat higher measurement error in the grid format in both mobile and PC web conditions. We found almost no significant effect of the question format on test?retest correlations between the latent scores in two waves and no differences in breakoff rates between the question formats. The multigroup comparison showed some measurement equivalence between the question formats. However, the difference varied depending on the length of a scale with a longer scale producing some differences in the measurement equivalence between the conditions. The levels of straightlining were higher in the grid than in the item-by-item format. In addition, concurrent validity was lower in the grid format in both PC and mobile web conditions. Finally, subjective indicators of respondent burden showed that the grid format increased reported technical difficulties and decreased subjective evaluation of the survey.},
   author = {Aigul Mavletova and Mick P. Couper and Daniil Lebedev},
   doi = {10.1177/0894439317735307},
   issn = {0894-4393},
   issue = {6},
   journal = {Social Science Computer Review},
   keywords = {DESIGN,Education--Computer Applications,Equivalence,Error analysis,Format,Internet,MEASUREMENT INVARIANCE,Measurement errors,Mobile Devices,Online Surveys,Polls & surveys,Psychometrics,Scales,Smartphones,Test Validity,concurrent validity,grid questions,item-by-item format,matrix questions,measurement equivalence,measurement error,mobile web surveys,reliability,web surveys},
   note = {Survey Design},
   pages = {647-668},
   title = {Grid and Item-by-Item Formats in PC and Mobile Web Surveys},
   volume = {36},
   url = {https://doi.org/10.1177/0894439317735307 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Grid+and+Item-by-Item+Formats+in+PC+and+Mobile+Web+Surveys&title=Social+Science+Computer+Review&issn=08944393&date=2018-12-01&volume=36&issue=6&spage=647&au=Mavletova+Aigul%3BCouper%2C+Mick+P%3BLebedev+Daniil&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439317735307},
   year = {2018},
}
@article{Lugtig2016,
   abstract = {Respondents in an Internet panel survey can often choose which device they use to complete questionnaires: a traditional PC, laptop, tablet computer, or a smartphone. Because all these devices have different screen sizes and modes of data entry, measurement errors may differ between devices. Using data from the Dutch Longitudinal Internet Study for the Social sciences panel, we evaluate which devices respondents use over time. We study the measurement error associated with each device and show that measurement errors are larger on tablets and smartphone than on PCs. To gain insight into the causes of these differences, we study changes in measurement error over time, associated with a switch of devices over two consecutive waves of the panel. We show that within individuals, measurement errors do not change with a switch in device. Therefore, we conclude that the higher measurement error in tablets and smartphones is associated with self-selection of the sample into using a particular device.},
   author = {Peter Lugtig and Vera Toepoel},
   doi = {10.1177/0894439315574248},
   issn = {0894-4393},
   issue = {1},
   journal = {Social Science Computer Review},
   keywords = {MOBILE WEB SURVEYS,measurement error,mixed device,mobile phones,panel survey,tablets},
   note = {Data Quality},
   pages = {78-94},
   title = {The Use of PCs, Smartphones, and Tablets in a Probability-Based Panel Survey: Effects on Survey Measurement Error},
   volume = {34},
   url = {https://doi.org/10.1177/0894439315574248},
   year = {2016},
}
@article{Toepoel2021,
   abstract = {This article compares the effectiveness of a research messenger layout to a traditional online layout with regards to probing. Responses to different types of probes (explanation, elaboration and category selection probes) were examined in terms of length and quality, measured by number of characters, number of themes, and an indicator for response quality. The research messenger layout, regardless of device being used, had a negative effect on both response length, number of themes and response quality. Further, we found that in both the traditional and research messenger layout, using a mobile device negatively affects the number of characters and themes used in probed responses. We conclude that probing is most effective when a traditional survey is completed on a computer. The research messenger layout was not able to generate responses of similar quality compared to the traditional layout, regardless of device being used.},
   author = {Vera Toepoel and Karlijn Mathon and Puck Tussenbroek and Peter Lugtig},
   doi = {10.1177/07591063211019953},
   issn = {0759-1063},
   issue = {1},
   journal = {Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique},
   note = {Alternative formats (Messenger)},
   pages = {74-95},
   title = {Probing in online mixed-device surveys: Is a research messenger layout more effective than a traditional online layout, especially on mobile devices?},
   volume = {151},
   url = {https://doi.org/10.1177/07591063211019953},
   year = {2021},
}
@article{Wells2014,
   abstract = {The dramatic rise of smartphones has profound implications for survey research. Namely, can smartphones become a viable and comparable device for self-administered surveys? The current study is based on approximately 1,500 online U.S. panelists who were smartphone users and who were randomly assigned to the mobile app or online computer mode of a survey. Within the survey, we embedded several experiments that had been previously tested in other modes (mail, PC web, mobile web). First, we test whether responses in the mobile app survey are sensitive to particular experimental manipulations as they are in other modes. Second, we test whether responses collected in the mobile app survey are similar to those collected in the online computer survey. Our mobile survey experiments show that mobile survey responses are sensitive to the presentation of frequency scales and the size of open-ended text boxes, as are responses in other survey modes. Examining responses across modes, we find very limited evidence for mode effects between mobile app and PC web survey administrations. This may open the possibility for multimode (mobile and online computer) surveys, assuming that certain survey design recommendations for mobile surveys are used consistently in both modes.},
   author = {Tom Wells and Justin T. Bailey and Michael W. Link},
   doi = {10.1177/0894439313505829},
   issn = {0894-4393},
   issue = {2},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,Computers,Education--Computer Applications,Human Computer Interaction,Human Machine Systems Design,Internet,MAIL,MOBILE WEB SURVEY,Mobile Devices,Mobile Phones,OPEN-ENDED QUESTIONS,ORDER,Polls & surveys,RESPONSES,Smartphones,Software,Surveys,Test Forms,Text Structure,United States--US,Users,Websites,article,computer methods, media, & applications,computers,experiments,mobile surveys,mobile surveys online surveys smartphones experiments,online surveys,smartphones,test administration format,text boxes,websites},
   note = {Data Quality},
   pages = {238-255},
   title = {Comparison of Smartphone and Online Computer Survey Administration},
   volume = {32},
   url = {https://doi.org/10.1177/0894439313505829 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Comparison+of+Smartphone+and+Online+Computer+Survey+Administration&title=Social+Science+Computer+Review&issn=08944393&date=2014-04-01&volume=32&issue=2&spage=238&au=Wells%2C+Tom%3BBailey%2C+Justin+T%3BLink%2C+Michael+W&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201422705&rft_id=info:doi/10.1177%2F0894439313505829 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Comparison+of+Smartphone+and+Online+Computer+Survey+Administration&title=Social+Science+Computer+Review&issn=08944393&date=2014-04-01&volume=32&issue=2&spage=238&au=Wells%2C+Tom%3BBailey%2C+Justin+T%3BLink%2C+Michael+W&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439313505829},
   year = {2014},
}
@article{Antoun2018,
   abstract = {Design principles for survey questionnaires viewed on desktop and laptop computers are increasingly being seen as inadequate for the design of questionnaires viewed on smartphones. Insights gained from empirical research can help those conducting mobile surveys to improve their questionnaires. This article reports on a systematic literature review of research presented or published between 2007 and 2016 that evaluated the effect of smartphone questionnaire design features on indicators of response quality. The evidence suggests that survey designers should make efforts to ?optimize? their questionnaires to make them easier to complete on smartphones, fit question content to the width of smartphone screens to prevent horizontal scrolling, and choose simpler types of questions (single-choice questions, multiple-choice questions, text-entry boxes) over more complicated types of questions (large grids, drop boxes, slider questions). Based on these results, we identify design heuristics, or general principles, for creating effective smartphone questionnaires. We distinguish between five of them: readability, ease of selection, visibility across the page, simplicity of design elements, and predictability across devices. They provide an initial framework by which to evaluate smartphone questionnaires, though empirical testing and further refinement of the heuristics is necessary.},
   author = {Christopher Antoun and Jonathan Katz and Josef Argueta and Lin Wang},
   doi = {10.1177/0894439317727072},
   issn = {0894-4393},
   issue = {5},
   journal = {Social Science Computer Review},
   keywords = {Boxes,COMPUTER,DEVICE AFFECT,Design,Education--Computer Applications,Heuristic,Literature reviews,MOBILE WEB SURVEY,PANEL,PC,Polls & surveys,QUALITY,Questionnaires,Research design,SCALES,SLIDER,Screens,Scrolling,Smartphones,Visibility,mobile web surveys,questionnaire design,response quality,smartphone surveys},
   note = {Survey Design},
   pages = {557-574},
   title = {Design Heuristics for Effective Smartphone Questionnaires},
   volume = {36},
   year = {2018},
}
@article{Peytchev2010,
   abstract = {Self-administered surveys can be conducted on mobile web-capable devices, yet these devices have unique features that can affect response processes. Ninety-two adults were randomly selected and provided with mobile devices to complete weekly web surveys. Experiments were designed to address three main objectives. First, the authors test fundamental findings which have been found robust across other modes, but whose impact may be diminished in mobile web surveys (due largely to the device), by manipulating question order and scale frequencies. Second, the authors test findings from experiments in computer-administered web surveys, altering the presentation of images and the number of questions per page. Third, the authors experiment with the unique display, navigation, and input methods, through the need to scroll, the vertical versus horizontal orientation of scales, and the willingness to provide open-ended responses. Although most findings from other modes are upheld, the small screen and keyboard introduce undesirable differences in responses.},
   author = {Andy Peytchev and Craig A. Hill},
   doi = {10.1177/0894439309353037},
   issn = {0894-4393},
   issue = {3},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,CONTEXT,DATA-COLLECTION,Data collection,Education--Computer Applications,Internet,Personal digital assistants,Polls & surveys,Product design,QUESTIONS,SCREEN SIZE,Smartphones,Surveys,article,cell phones,computer methods, media, & applications,mobile devices,mobile devices cell phones smartphones survey design mobile web surveys,mobile web surveys,smartphones,survey design},
   note = {Data Quality and Design},
   pages = {319-335},
   title = {Experiments in Mobile Web Survey Design: Similarities to Other Modes and Unique Considerations},
   volume = {28},
   url = {https://doi.org/10.1177/0894439309353037 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Experiments+in+Mobile+Web+Survey+Design%3A+Similarities+to+Other+Modes+and+Unique+Considerations&title=Social+Science+Computer+Review&issn=08944393&date=2010-10-01&volume=28&issue=3&spage=319&au=Peytchev%2C+Andy%3BHill%2C+Craig+A&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439309353037 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Experiments+in+Mobile+Web+Survey+Design&title=Social+Science+Computer+Review&issn=08944393&date=2010-08-01&volume=28&issue=3&spage=319&au=Peytchev%2C+Andy%3BHill%2C+Craig+A&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201052759&rft_id=info:doi/10.1177%2F0894439309353037},
   year = {2010},
}
@article{Revilla2018,
   abstract = {Much research has been done comparing grids and item-by-item formats. However, the results are mixed, and more research is needed especially when a significant proportion of respondents answer using smartphones. In this study, we implemented an experiment with seven groups (n = 1,476), varying the device used (PC or smartphone), the presentation of the questions (grids, item-by-item vertical, item-by-item horizontal), and, in the case of smartphones only, the visibility of the ?next? button (always visible or only visible at the end of the page, after scrolling down). The survey was conducted by the Netquest online fieldwork company in Spain in 2016. We examined several outcomes for three sets of questions, which are related to respondent behavior (completion time, lost focus, answer changes, and screen orientation) and data quality (item missing data, nonsubstantive responses, instructional manipulation check failure, and nondifferentiation). The most striking difference found is for the placement of the next button in the smartphone item-by-item conditions: When the button is always visible, item missing data are substantially higher.},
   author = {Melanie Revilla and Mick P. Couper},
   doi = {10.1177/0894439317715626},
   issn = {0894-4393},
   issue = {3},
   journal = {Social Science Computer Review},
   keywords = {Completion time,DESIGN,Data quality,Education--Computer Applications,Human Factors Engineering,Internet,LAYOUT,Microcomputers,Missing data,Mobile Phones,PARADATA,Product Design,Scrolling,Smartphones,Technology,Telephone communications,Visibility,WEB SURVEYS,data quality,grids,item-by-item,respondent behavior,scale orientation,smartphones,web surveys},
   note = {Survey Design},
   pages = {349-368},
   title = {Comparing Grids With Vertical and Horizontal Item-by-Item Formats for PCs and Smartphones},
   volume = {36},
   url = {https://doi.org/10.1177/0894439317715626 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Comparing+Grids+With+Vertical+and+Horizontal+Item-by-Item+Formats+for+PCs+and+Smartphones&title=Social+Science+Computer+Review&issn=08944393&date=2018-06-01&volume=36&issue=3&spage=349&au=Revilla%2C+Melanie%3BCouper%2C+Mick+P&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439317715626},
   year = {2018},
}
@article{Gummer2019,
   abstract = {Mobile coverage recently has reached an all-time high, and in most countries, high-speed Internet connections are widely available. Due to technological development, smartphones and tablets have become increasingly popular. Accordingly, we have observed an increasing use of mobile devices to complete web surveys and, hence, survey methodologists have shifted their attention to the challenges that stem from this development. The present study investigated whether the growing use of smartphones has decreased how systematically this choice of device varies between groups of respondents (i.e., how selective smartphone usage for completing web surveys is). We collected a data set of 18,520 respondents from 18 web surveys that were fielded in Germany between 2012 and 2016. Based on these data, we show that while the use of smartphones to complete web surveys has considerably increased over time, selectivity with respect to using this device has remained stable.},
   author = {Tobias Gummer and Franziska Quoß and Joss Roßmann},
   doi = {10.1177/0894439318766836},
   issn = {0894-4393},
   issue = {3},
   journal = {Social Science Computer Review},
   keywords = {BIAS,Choice Behavior,DATA QUALITY,Education--Computer Applications,Electronic devices,Focus groups,IMPROVING RESPONSE,Internet,Microcomputers,Mobile Devices,Mobile communication systems,PC,PROBABILITY-BASED PANEL,Polls & surveys,Selectivity,Smartphones,Surveys,TABLETS,Tablet computers,Technological change,device choice,mobile devices,selectivity,smartphone,smartphones,survey error,web surveys},
   note = {Who is using mobile?},
   pages = {371-384},
   title = {Does Increasing Mobile Device Coverage Reduce Heterogeneity in Completing Web Surveys on Smartphones?},
   volume = {37},
   url = {https://doi.org/10.1177/0894439318766836 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Does+Increasing+Mobile+Device+Coverage+Reduce+Heterogeneity+in+Completing+Web+Surveys+on+Smartphones%3F&title=Social+Science+Computer+Review&issn=08944393&date=2019-06-01&volume=37&issue=3&spage=371&au=Gummer%2C+Tobias%3BQuo%C3%9F+Franziska%3BJoss%2C+Ro%C3%9Fmann&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439318766836},
   year = {2019},
}
@article{Daikeler2020,
   abstract = {Filter questions are used to administer follow-up questions to eligible respondents while allowing respondents who are not eligible to skip those questions. Filter questions can be asked in either the interleafed or the grouped formats. In the interleafed format, the follow-ups are asked immediately after the filter question; in the grouped format, follow-ups are asked after the filter question block. Underreporting can occur in the interleafed format due to respondents? desire to reduce the burden of the survey. This phenomenon is called motivated misreporting. Because smartphone surveys are more burdensome than web surveys completed on a computer or laptop, due to the smaller screen size, longer page loading times, and more distraction, we expect that motivated misreporting is more pronounced on smartphones. Furthermore, we expect that misreporting occurs not only in the filter questions themselves but also extends to data quality in the follow-up questions. We randomly assigned 3,517 respondents of a German online access panel to either the PC or the smartphone. Our results show that while both PC and smartphone respondents trigger fewer filter questions in the interleafed format than the grouped format, we did not find differences between PC and smartphone respondents regarding the number of triggered filter questions. However, smartphone respondents provide lower data quality in the follow-up questions, especially in the grouped format. We conclude with recommendations for web survey designers who intend to incorporate smartphone respondents in their surveys.},
   author = {Jessica Daikeler and Ruben L. Bach and Henning Silber and Stephanie Eckman},
   doi = {10.1177/0894439319900936},
   issn = {0894-4393},
   journal = {Social Science Computer Review},
   keywords = {DATA QUALITY,DEVICES,FILTER QUESTIONS,MOBILE WEB,PC,TABLETS,filter questions,follow-up questions,measurement error,misreporting,mobile data quality,motivated underreporting},
   note = {Data Quality},
   pages = {0894439319900936},
   title = {Motivated Misreporting in Smartphone Surveys},
   url = {https://doi.org/10.1177/0894439319900936},
   year = {2020},
}
@article{Maineri2021,
   abstract = {This study explores some features of slider bars in the context of a multi-device web survey. Using data collected among the students of the University of Trento in 2015 and 2016 by means of two web surveys (N = 6,343 and 4,124) including two experiments, we investigated the effect of the initial position of the handle and the presence of numeric labels on answers provided using slider bars. It emerged that the initial position of the handle affected answers and that the number of rounded scores increased with numeric feedback. Smartphone respondents appeared more sensitive to the initial position of the handle but also less affected by the presence of numeric labels resulting in a lower tendency to rounding. Yet, outcomes on anchoring were inconclusive. Overall, no relevant differences have been detected between tablet and PC respondents. Understanding to what extent interactive and engaging tools such as slider bars can be successfully employed in multi-device surveys without affecting data quality is a key challenge for those who want to exploit the potential of web-based and multi-device data collection without undermining the quality of measurement.},
   author = {Angelica M. Maineri and Ivano Bison and Ruud Luijkx},
   doi = {10.1177/0894439319879132},
   issn = {0894-4393},
   issue = {4},
   journal = {Social Science Computer Review},
   keywords = {Bars,COMPUTER,College students,DATA QUALITY,DESIGN,Data collection,Data quality,Education--Computer Applications,FORMATS,Internet,Labels,MOBILE,Polls & surveys,RADIO BUTTON SCALES,RELIABILITY,Rounding,SMARTPHONE,VISUAL ANALOG SCALES,mobile surveys,multi-device surveys,slider question,sliders,survey experiment,web surveys},
   note = {Survey Design},
   pages = {573-591},
   title = {Slider Bars in Multi-Device Web Surveys},
   volume = {39},
   url = {https://doi.org/10.1177/0894439319879132 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Slider+Bars+in+Multi-Device+Web+Surveys&title=Social+Science+Computer+Review&issn=08944393&date=2021-08-01&volume=39&issue=4&spage=573&au=Maineri%2C+Angelica+M%3BBison+Ivano%3BLuijkx+Ruud&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439319879132},
   year = {2021},
}
@article{Keusch2017,
   abstract = {Due to a rising mobile device penetration, Web surveys are increasingly accessed and completed on smartphones or tablets instead of desktop computers or laptops. Mobile Web surveys are also gaining popularity as an alternative self-administered data collection mode among survey researchers. We conducted a methodological experiment among iPhone owners and compared the participation and response behavior of three groups of respondents: iPhone owners who started and completed our survey on a desktop or laptop PC, iPhone owners who self-selected to complete the survey on an iPhone, and iPhone owners who started on a PC but were requested to switch to iPhone. We found that respondents who completed the survey on a PC were more likely to be male, to have a lower educational level, and to have more experience with Web surveys than mobile Web respondents, regardless of whether they used the iPhone voluntarily or were asked to switch from a PC to an iPhone. Overall, iPhone respondents had more missing data and took longer to complete the survey than respondents who answered the questions on a PC, but they also showed less straightlining behavior. There are only minimal device differences on survey answers obtained from PCs and iPhones.},
   author = {Florian Keusch and Ting Yan},
   doi = {10.1177/0894439316675566},
   issn = {0894-4393},
   issue = {6},
   journal = {Social Science Computer Review},
   keywords = {83:Social and Behavioral Sciences (CI),Alternative approaches,COMPUTER,Data acquisition,Data collection,Education--Computer Applications,Educational attainment,Internet,MECHANICAL TURK,Microcomputers,Missing data,Mobile communication systems,Mobile computing,ONLINE SURVEYS,PC,PROBABILITY-BASED PANEL,Participation,Personal computers,Polls & surveys,QUALITY,SMARTPHONE,Smartphones,Tablet computers,Web surveys,break-offs,data quality,iPhone,missing data,mobile Web surveys,response times,smartphones,straightlining,unintentional mobile},
   note = {Who is using mobile? and Data Quality},
   pages = {751-769},
   title = {Web Versus Mobile Web: An Experimental Study of Device Effects and Self-Selection Effects},
   volume = {35},
   url = {https://doi.org/10.1177/0894439316675566 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Web+Versus+Mobile+Web%3A+An+Experimental+Study+of+Device+Effects+and+Self-Selection+Effects&title=Social+Science+Computer+Review&issn=08944393&date=2017-12-01&volume=35&issue=6&spage=751&au=Keusch%2C+Florian%3BYan%2C+Ting&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439316675566},
   year = {2017},
}
@article{Keusch2021,
   abstract = {Researchers attempting to survey refugees over time face methodological issues because of the transient nature of the target population. In this article, we examine whether applying smartphone technology could alleviate these issues. We interviewed 529 refugees and afterward invited them to four follow-up mobile web surveys and to install a research app for passive mobile data collection. Our main findings are as follows: First, participation in mobile web surveys declines rapidly and is rather selective with significant coverage and nonresponse biases. Second, we do not find any factor predicting types of smartphone ownership, and only low reading proficiency is significantly correlated with app nonparticipation. However, obtaining sufficiently large samples is challenging?only 5 percent of the eligible refugees installed our app. Third, offering a 30 Euro incentive leads to a statistically insignificant increase in participation in passive mobile data collection.},
   author = {Florian Keusch and Mariel M. Leonard and Christoph Sajons and Susan Steiner},
   doi = {10.1177/0049124119852377},
   issn = {0049-1241},
   issue = {4},
   journal = {Sociological Methods & Research},
   keywords = {Competence,DISCLOSURE,Data collection,Germany,HARM,Internet,MOBILE WEB,Methodological problems,Nonresponse,Ownership,Participation,Polls & surveys,RISK,Refugees,Smartphones,Sociology,Technology,coverage,mobile web surveys,participation,passive mobile data collection,refugees,smartphones},
   note = {Who is using mobile? Invitation mode},
   pages = {1863-1894},
   title = {Using Smartphone Technology for Research on Refugees: Evidence from Germany},
   volume = {50},
   url = {https://doi.org/10.1177/0049124119852377 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Using+Smartphone+Technology+for+Research+on+Refugees%3A+Evidence+from+Germany&title=Sociological+Methods+and+Research&issn=00491241&date=2021-11-01&volume=50&issue=4&spage=1863&au=Keusch+Florian%3BLeonard%2C+Mariel+M%3BSajons+Christoph%3BSteiner%2C+Susan&isbn=&jtitle=Sociological+Methods+and+Research&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0049124119852377},
   year = {2021},
}
@article{Buskirk2014,
   abstract = {With nearly 50% of U.S. mobile phone subscribers using smartphones, survey researchers are beginning to explore their use as a data collection tool. The Got Healthy Apps Study (GHAS) conducted a randomized experiment to compare mode effects for a survey completed via iPhone mobile browser and online via desktop/laptop computer web browser. Mode effects were assessed for three types of outcomes: randomization/recruitment, survey process/completion, and survey items. In short, the distribution of survey completion times and the distribution of the number of apps owned were significantly different across survey mode after accounting for block group. Other key mode effects outcomes (including open-ended items, slider bar questions, and missing item rates) showed no significant differences across survey mode. Some interesting qualitative findings suggest that iPhone respondents enter more characters and omit fewer items than originally thought.},
   author = {Trent D. Buskirk and Charles H. Andrus},
   doi = {10.1177/1525822X14526146},
   issn = {1525-822X},
   issue = {4},
   journal = {Field methods},
   keywords = {0514: culture and social structure,Computer Applications,Computers,Internet,Methodology,Methodology (Data Collection),Mobile Phones,Online Surveys,Recruitment,Smartphones,Surveys,Websites,apps,article,mode effects,smartphones,smartphones survey research mode effects apps,social anthropology,survey research},
   note = {Data Quality},
   pages = {322-342},
   title = {Making Mobile Browser Surveys Smarter: Results from a Randomized Experiment Comparing Online Surveys Completed via Computer or Smartphone},
   volume = {26},
   url = {https://doi.org/10.1177/1525822X14526146 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Making+Mobile+Browser+Surveys+Smarter%3A+Results+from+a+Randomized+Experiment+Comparing+Online+Surveys+Completed+via+Computer+or+Smartphone&title=Field+Methods&issn=1525822X&date=2014-11-01&volume=26&issue=4&spage=322&au=Buskirk%2C+Trent+D%3BAndrus%2C+Charles+H&isbn=&jtitle=Field+Methods&btitle=&rft_id=info:eric/201525526&rft_id=info:doi/10.1177%2F1525822X14526146},
   year = {2014},
}
@article{Revilla2021,
   abstract = {More and more respondents are answering web surveys using mobile devices. Mobile respondents tend to provide shorter responses to open questions than PC respondents. Using voice recording to answer open-ended questions could increase data quality and help engage groups usually underrepresented in web surveys. Revilla, Couper, Bosch, and Asensio showed that in particular the use of voice recording still presents many challenges, even if it could be a promising tool. This article reports results from a follow-up experiment in which the main goals were to (1) test whether different instructions on how to use the voice recording tool reduce technical and understanding problems, and thereby reduce item nonresponse while preserving data quality and the evaluation of the tool; (2) test whether nonresponse due to context can be reduced by using a filter question, and how this affects data quality and the tool evaluation; and (3) understand which factors affect nonresponse to open-ended questions using voice recording, and if these factors also affect data quality and the evaluation of the tool. The experiment was implemented within a smartphone web survey in Spain focused on Android devices. The results suggest that different instructions did not affect nonresponse to the open questions and had little effect on data quality for those who did answer. Introducing a filter to ensure that people were in a setting that permits voice recording seems useful. Despite efforts to reduce problems, a substantial proportion of respondents are still unwilling or unable to answer open questions using voice recording.},
   author = {Melanie Revilla and Mick P. Couper},
   doi = {10.1177/0894439319888708},
   issn = {08944393},
   issue = {6},
   journal = {Social Science Computer Review},
   keywords = {Birthdays,COMPUTER,DATA QUALITY,Data quality,Education--Computer Applications,Electronic devices,Emotions,Evaluation,Internet,MOBILE DEVICES,PC,PROBABILITY-BASED PANEL,Polls & surveys,Questions,Recording,Research responses,Smartphones,Voice,WEB,Webs,android,data quality,mobile web survey,nonresponse,smartphones,tool evaluation,voice recording},
   note = {Voice input},
   pages = {1159-1178},
   pmid = {2607554021},
   title = {Improving the Use of Voice Recording in a Smartphone Survey},
   volume = {39},
   url = {https://www.proquest.com/scholarly-journals/improving-use-voice-recording-smartphone-survey/docview/2607554021/se-2?accountid=14570 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Improving+the+Use+of+Voice+Recording+in+a+Smartphone+Survey&title=Social+Science+Computer+Review&issn=08944393&date=2021-12-01&volume=39&issue=6&spage=1159&au=Revilla%2C+Melanie%3BCouper%2C+Mick+P&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439319888708},
   year = {2021},
}
@article{,
   abstract = {This article investigates unintended mobile access to surveys in online, probability-based panels. We find that spontaneous tablet usage is drastically increasing in web surveys, while smartphone usage remains low. Further, we analyze the bias of respondent profiles using smartphones and tablets compared to those using computers, on the basis of several sociodemographic characteristics. Our results indicate not only that mobile web respondents differ from PC users but also that tablet users differ from smartphone users. While tablets are used for survey completion by working (young) adults, smartphones are used merely by the young. In addition, our results indicate that mobile web respondents are more progressive and describe themselves more often as pioneers or forerunners in adopting new technology, compared to PC respondents. We further discover that respondents? preferences for devices to complete surveys are clearly in line with unintended mobile response. Finally, we present a similar analysis on intended mobile response in an experiment where smartphone users were requested to complete a mobile survey. Based on these findings, testing on tablets is strongly recommended in online surveys. If the goal is to reach young respondents, enabling surveys via smartphones should be considered.},
   author = {Marika De Bruijne and Arnaud Wijnant},
   doi = {10.1177/0894439314525918},
   issn = {0894-4393},
   issue = {6},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,Bias,Computers,Demographic Characteristics,Demographics,Education--Computer Applications,Internet,Mobile Devices,Preferences,Smartphones,Sociodemographic Characteristics,Surveys,Technology,Websites,Young Adults,article,computer methods, media, & applications,mobile web survey,mobile web survey unintended mobile response survey error web panel respondent preference,respondent preference,survey error,unintended mobile response,web panel},
   note = {Who is using mobile? And Data Quality},
   pages = {728-742},
   title = {Mobile Response in Web Panels},
   volume = {32},
   url = {https://doi.org/10.1177/0894439314525918 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Mobile+Response+in+Web+Panels&title=Social+Science+Computer+Review&issn=08944393&date=2014-12-01&volume=32&issue=6&spage=728&au=de+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201520920&rft_id=info:doi/10.1177%2F0894439314525918 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Mobile+Response+in+Web+Panels&title=Social+Science+Computer+Review&issn=08944393&date=2014-12-01&volume=32&issue=6&spage=728&au=de+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439314525918},
   year = {2014},
}
@article{Toepoel2014,
   abstract = {This article reports from a pilot study that was conducted in a probability-based online panel in the Netherlands. Two parallel surveys were conducted: one in the traditional questionnaire layout of the panel and the other optimized for mobile completion with new software that uses a responsive design (optimizes the layout for the device chosen). The latter questionnaire was optimized for mobile completion, and respondents could choose whether they wanted to complete the survey on their mobile phone or on a regular desktop. Results show that a substantive number of respondents (57%) used their mobile phone for survey completion. No differences were found between mobile and desktop users with regard to break offs, item nonresponse, time to complete the survey, or response effects such as length of answers to an open-ended question and the number of responses in a check-all-that-apply question. A considerable number of respondents gave permission to record their GPS coordinates, which are helpful in defining where the survey was taken. Income, household size, and household composition were found to predict mobile completion. In addition, younger respondents, who typically form a hard-to-reach group, show higher mobile completion rates.},
   author = {Vera Toepoel and Peter Lugtig},
   doi = {10.1177/0894439313510482},
   issn = {0894-4393},
   issue = {4},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,Computer Software,DESIGN,Education--Computer Applications,Global positioning systems--GPS,Households,Income,Internet,MODES,Netherlands,Probability,Software,article,computer methods, media, & applications,measurement effects,mobile phone survey,mobile phone survey panel survey measurement effects nonresponse effects,nonresponse effects,panel survey},
   note = {Data Quality},
   pages = {544-560},
   title = {What Happens if You Offer a Mobile Option to Your Web Panel? Evidence From a Probability-Based Panel of Internet Users},
   volume = {32},
   url = {https://doi.org/10.1177/0894439313510482 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=What+Happens+if+You+Offer+a+Mobile+Option+to+Your+Web+Panel%3F+Evidence+From+a+Probability-Based+Panel+of+Internet+Users&title=Social+Science+Computer+Review&issn=08944393&date=2014-08-01&volume=32&issue=4&spage=544&au=Toepoel%2C+Vera%3BLugtig%2C+Peter&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201520952&rft_id=info:doi/10.1177%2F0894439313510482 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=What+Happens+if+You+Offer+a+Mobile+Option+to+Your+Web+Panel%3F+Evidence+From+a+Probability-Based+Panel+of+Internet+Users&title=Social+Science+Computer+Review&issn=08944393&date=2014-08-01&volume=32&issue=4&spage=544&au=Toepoel%2C+Vera%3BLugtig%2C+Peter&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439313510482},
   year = {2014},
}
@article{,
   abstract = {With the growing popularity of smartphones and tablet PCs (tablets) equipped with mobile browsers, the possibilities to administer surveys via mobile devices have expanded. To investigate the possible mode effect on answer behavior, results are compared between a mobile device?assisted web survey and a computer-assisted web survey. First, a premeasurement in the CentERpanel is conducted to analyze the user group of mobile devices. Second, the users are randomly allocated one of the three conditions: (1) conventional computer-assisted web survey, (2) hybrid version: a computer-assisted web survey with a layout similar to mobile web survey, and (3) mobile web survey. Special attention is given to the design of the mobile web questionnaire, taking small screen size, and typical functionalities for touchscreens into account. The findings suggest that survey completion on mobile devices need not lead to different results than on computers, but one should be prepared for a lower response rate and longer survey completion time. Further, the study offers considerations for researchers on survey satisfaction, location during survey completion, and preferred device to access Internet. With adaptations, surveys can be conducted on the newest mobile devices, although new challenges are emerging and further research is called for.},
   author = {Marika De Bruijne and Arnaud Wijnant},
   doi = {10.1177/0894439313483976},
   issn = {0894-4393},
   issue = {4},
   journal = {Social Science Computer Review},
   keywords = {0188: methodology and research technology,Access,Attention,Comparative studies,Computers,Education--Computer Applications,Heterogeneity,INTERPRETIVE HEURISTICS,Internet,Mobile Phones,OPTIMAL NUMBER,Polls & surveys,Popularity,Portable computers,RATING-SCALES,Response rates,SEARCH,SURVEY DESIGN,Satisfaction,Smartphones,Surveys,article,computer methods, media, & applications,computer web survey,mobile device,mobile devices,mobile web survey,mobile web survey smartphones tablet PCs mobile devices survey design touch user interfaces,research surveys,response rate,smartphones,special attention,survey design,tablet PCs,touch user interfaces},
   note = {Data Quality},
   pages = {482-504},
   title = {Comparing Survey Results Obtained via Mobile Devices and Computers: An Experiment With a Mobile Web Survey on a Heterogeneous Group of Mobile Devices Versus a Computer-Assisted Web Survey},
   volume = {31},
   url = {https://doi.org/10.1177/0894439313483976 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Comparing+Survey+Results+Obtained+via+Mobile+Devices+and+Computers%3A+An+Experiment+With+a+Mobile+Web+Survey+on+a+Heterogeneous+Group+of+Mobile+Devices+Versus+a+Computer-Assisted+Web+Survey&title=Social+Science+Computer+Review&issn=08944393&date=2013-08-01&volume=31&issue=4&spage=482&au=de+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/201339794&rft_id=info:doi/10.1177%2F0894439313483976 https://primo-49man.hosted.exlibrisgroup.com/openurl/MAN/MAN_UB_service_page?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&genre=article&sid=ProQ:ProQ%3Asocabs&atitle=Comparing+Survey+Results+Obtained+via+Mobile+Devices+and+Computers%3A+An+Experiment+With+a+Mobile+Web+Survey+on+a+Heterogeneous+Group+of+Mobile+Devices+Versus+a+Computer-Assisted+Web+Survey&title=Social+Science+Computer+Review&issn=08944393&date=2013-08-01&volume=31&issue=4&spage=482&au=de+Bruijne%2C+Marika%3BWijnant%2C+Arnaud&isbn=&jtitle=Social+Science+Computer+Review&btitle=&rft_id=info:eric/&rft_id=info:doi/10.1177%2F0894439313483976},
   year = {2013},
}
