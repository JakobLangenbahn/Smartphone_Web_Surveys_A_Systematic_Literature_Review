
@article{page_prisma_2021,
	title = {The {PRISMA} 2020 statement: an updated guideline for reporting systematic reviews},
	volume = {372},
	url = {https://www.bmj.com/content/372/bmj.n71},
	doi = {10.1136/BMJ.N71},
	abstract = {The Preferred Reporting Items for Systematic reviews and Meta-Analyses ({PRISMA}) statement, published in 2009, was designed to help systematic reviewers transparently report why the review was done, what the authors did, and what they found. Over the past decade, advances in systematic review methodology and terminology have necessitated an update to the guideline. The {PRISMA} 2020 statement replaces the 2009 statement and includes new reporting guidance that reflects advances in methods to identify, select, appraise, and synthesise studies. The structure and presentation of the items have been modified to facilitate implementation. In this article, we present the {PRISMA} 2020 27-item checklist, an expanded checklist that details reporting recommendations for each item, the {PRISMA} 2020 abstract checklist, and the revised flow diagrams for original and updated reviews.
Systematic reviews serve many critical roles. They can provide syntheses of the state of knowledge in a field, from which future research priorities can be identified; they can address questions that otherwise could not be answered by individual studies; they can identify problems in primary research that should be rectified in future studies; and they can generate or evaluate theories about how or why phenomena occur. Systematic reviews therefore generate various types of knowledge for different users of reviews (such as patients, healthcare providers, researchers, and policy makers).12 To ensure a systematic review is valuable to users, authors should prepare a transparent, complete, and accurate account of why the review was done, what they did (such as how studies were identified and selected) and what they found (such as characteristics of contributing studies and results of meta-analyses). Up-to-date reporting guidance facilitates authors achieving this.3
The Preferred Reporting Items for Systematic reviews and Meta-Analyses ({PRISMA}) statement published in 2009 (hereafter referred to as {PRISMA} 2009)45678910 …},
	journaltitle = {{BMJ}},
	author = {Page, Matthew J. and {McKenzie}, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hróbjartsson, Asbjørn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and {McDonald}, Steve and {McGuinness}, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
	date = {2021-03},
	note = {Publisher: British Medical Journal Publishing Group},
}

@article{langenbahn_smartphone_2021,
	title = {Smartphone Web Surveys: A Systematic Literature Review},
	url = {https://github.com/JakobLangenbahn/Smartphone_Web_Surveys_A_Systematic_Literature_Review},
	author = {Langenbahn, Jakob},
	date = {2021-01},
}

@article{durach_new_2017,
	title = {A New Paradigm for Systematic Literature Reviews in Supply Chain Management},
	volume = {53},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/jscm.12145},
	doi = {10.1111/JSCM.12145},
	abstract = {While systematic literature reviews ({SLRs}) have contributed substantially to developing knowledge in fields such as medicine, they have made limited contributions to developing knowledge in the supply chain management domain. This is due to the ontological and epistemological idiosyncrasies of research in supply chain management, which need to be accounted for when retrieving, selecting, and synthesizing studies in an {SLR}. Therefore, we propose a new paradigm for {SLRs} in the supply chain domain that is based on both best practice and the unique attributes of doing supply chain management research. This approach involves exploring existing studies with attention to theoretical boundaries, units of analysis, sources of data, study contexts, and definitions and the operationalization of constructs, as well as research methods, with the goal of refining or revising existing theory. This new paradigm will push supply chain management research to the frontier of current methodological standards and build a foundation for improving the contribution of future {SLRs} in the supply chain and adjacent management disciplines.},
	pages = {67--85},
	number = {4},
	journaltitle = {Journal of Supply Chain Management},
	author = {Durach, Christian F. and Kembro, Joakim and Wieland, Andreas},
	date = {2017-10},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {literature review, bias, guidelines, methodology, paradigm},
}

@article{moher_preferred_2009,
	title = {Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The {PRISMA} Statement},
	volume = {6},
	issn = {2006062298},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1000097},
	doi = {10.1371/JOURNAL.PMED.1000097},
	pages = {e1000097--e1000097},
	number = {7},
	journaltitle = {{PLOS} Medicine},
	author = {Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G. and Altman, Doug and Antes, Gerd and Atkins, David and Barbour, Virginia and Barrowman, Nick and Berlin, Jesse A. and Clark, Jocalyn and Clarke, Mike and Cook, Deborah and D'Amico, Roberto and Deeks, Jonathan J. and Devereaux, P. J. and Dickersin, Kay and Egger, Matthias and Ernst, Edzard and Gøtzsche, Peter C. and Grimshaw, Jeremy and Guyatt, Gordon and Higgins, Julian and Ioannidis, John P.A. and Kleijnen, Jos and Lang, Tom and Magrini, Nicola and {McNamee}, David and Moja, Lorenzo and Mulrow, Cynthia and Napoli, Maryann and Oxman, Andy and Pham, Bá and Rennie, Drummond and Sampson, Margaret and Schulz, Kenneth F. and Shekelle, Paul G. and Tovey, David and Tugwell, Peter},
	date = {2009-07},
	note = {Publisher: Public Library of Science},
	keywords = {Clinical research design, Database searching, Medical journals, Medical risk factors, Metaanalysis, Publication ethics, Research reporting guidelines, Systematic reviews},
}

@article{weidt_systematic_2016,
	title = {Systematic Literature Review in  Computer Science - A Practical Guide},
	volume = {1},
	url = {https://nrc.ice.ufjf.br/seer/index.php/relate/article/view/154},
	abstract = {This work aims to provide a practical guide to assist students of Computer Science courses and related fields to conduct a systematic literature review. The steps proposed in this paper to conduct a systematic review were extracted from a technical report published by the researcher Bárbara Kitchenham and arranged in a more objective format, in order to make information more accessible and practical, especially for those who are having their first contact with this technique. The target audience for this work are undergraduate, master's and doctoral students that are in the initial phase of their bibliographic research.      Link:  {ResearchGate}},
	number = {0},
	journaltitle = {Relatórios Técnicos do {DCC}/{UFJF}},
	author = {Weidt, Frâncila and Silva, Rodrigo},
	date = {2016},
}

@article{xiao_guidance_2017,
	title = {Guidance on Conducting a Systematic Literature Review:},
	volume = {39},
	url = {https://journals.sagepub.com/doi/full/10.1177/0739456X17723971},
	doi = {10.1177/0739456X17723971},
	abstract = {Literature reviews establish the foundation of academic inquires. However, in the planning field, we lack rigorous systematic reviews. In this article, through a systematic search on the methodolog...},
	pages = {93--112},
	number = {1},
	journaltitle = {https://doi.org/10.1177/0739456X17723971},
	author = {Xiao, Yu and Watson, Maria},
	date = {2017},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	keywords = {literature review, methodology, typology, synthesis},
}

@article{papaioannou_literature_2010,
	title = {Literature searching for social science systematic reviews: consideration of a range of search techniques},
	volume = {27},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1471-1842.2009.00863.x},
	doi = {10.1111/J.1471-1842.2009.00863.X},
	abstract = {Background: Literature for a systematic review on the student experience of e-learning is located across a range of subject areas including health, education, social science, library and information science. Objectives: To assess the merits and shortcomings of using different search techniques in retrieval of evidence in the social science literature. Methods: A conventional subject search was undertaken as the principal method of identifying the literature for the review. Four supplementary search methods were used including citation searching, reference list checking, contact with experts and pearl growing. Results: The conventional subject search identified 30 of 41 included references; retrieved from 10 different databases. References were missed by this method and a further 11 references were identified via citation searching, reference list checking and contact with experts. Pearl growing was suspended as the nominated pearls were dispersed across numerous databases, with no single database indexing more than four pearls. Conclusions: Searching within the social sciences literature requires careful consideration. Conventional subject searching identified the majority of references, but additional search techniques were essential and located further high quality references. © 2009 Health Libraries Group.},
	pages = {114--122},
	number = {2},
	journaltitle = {Health Information \& Libraries Journal},
	author = {Papaioannou, Diana and Sutton, Anthea and Carroll, Christopher and Booth, Andrew and Wong, Ruth},
	date = {2010},
	note = {Publisher: John Wiley \& Sons, Ltd},
}

@article{petticrew_systematic_2008,
	title = {Systematic Reviews in the Social Sciences: A Practical Guide},
	doi = {10.1002/9780470754887},
	abstract = {Such diverse thinkers as Lao-Tze, Confucius, and U.S. Defense Secretary Donald Rumsfeld have all pointed out that we need to be able to tell the difference between real and assumed knowledge. The systematic review is a scientific tool that can help with this difficult task. It can help, for example, with appraising, summarising, and communicating the results and implications of otherwise unmanageable quantities of data. This book, written by two highly-respected social scientists, provides an overview of systematic literature review methods: Outlining the rationale and methods of systematic reviews; Giving worked examples from social science and other fields; Applying the practice to all social science disciplines; It requires no previous knowledge, but takes the reader through the process stage by stage; Drawing on examples from such diverse fields as psychology, criminology, education, transport, social welfare, public health, and housing and urban policy, among others. Including detailed sections on assessing the quality of both quantitative, and qualitative research; searching for evidence in the social sciences; meta-analytic and other methods of evidence synthesis; publication bias; heterogeneity; and approaches to dissemination. © 2006 Mark Petticrew and Helen Roberts.},
	pages = {1--336},
	journaltitle = {Systematic Reviews in the Social Sciences: A Practical Guide},
	author = {Petticrew, Mark and Roberts, Helen},
	date = {2008},
	note = {Publisher: Blackwell Publishing Ltd},
}

@article{gusenbauer_which_2020,
	title = {Which academic search systems are suitable for systematic reviews or meta-analyses? Evaluating retrieval qualities of Google Scholar, {PubMed}, and 26 other resources},
	volume = {11},
	doi = {10.1002/JRSM.1378},
	abstract = {Rigorous evidence identification is essential for systematic reviews and meta-analyses (evidence syntheses) because the sample selection of relevant studies determines a review's outcome, validity, and explanatory power. Yet, the search systems allowing access to this evidence provide varying levels of precision, recall, and reproducibility and also demand different levels of effort. To date, it remains unclear which search systems are most appropriate for evidence synthesis and why. Advice on which search engines and bibliographic databases to choose for systematic searches is limited and lacking systematic, empirical performance assessments. This study investigates and compares the systematic search qualities of 28 widely used academic search systems, including Google Scholar, {PubMed}, and Web of Science. A novel, query-based method tests how well users are able to interact and retrieve records with each system. The study is the first to show the extent to which search systems can effectively and efficiently perform (Boolean) searches with regards to precision, recall, and reproducibility. We found substantial differences in the performance of search systems, meaning that their usability in systematic searches varies. Indeed, only half of the search systems analyzed and only a few Open Access databases can be recommended for evidence syntheses without adding substantial caveats. Particularly, our findings demonstrate why Google Scholar is inappropriate as principal search system. We call for database owners to recognize the requirements of evidence synthesis and for academic journals to reassess quality requirements for systematic reviews. Our findings aim to support researchers in conducting better searches for better evidence synthesis.},
	pages = {181--217},
	number = {2},
	journaltitle = {Research Synthesis Methods},
	author = {Gusenbauer, Michael and Haddaway, Neal R},
	date = {2020},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {evaluation, information retrieval, academic search systems, discovery, systematic review, systematic search},
}

@article{armitage_undertaking_2008,
	title = {Undertaking a Structured Literature Review or Structuring a Literature Review: Tales from the Field},
	volume = {6},
	url = {https://academic-publishing.org/index.php/ejbrm/article/view/1231},
	abstract = {The diversity of sources of literature within the management disciplines has resulted in a growing need for a systematic methodology to map the territory of its associated theories and models. As such, when scoping out a doctoral or policy based study the Structured Literature Review ({SLR}) as espoused by Tranfield et al (2003) can be considered as a means by which critical literature central to and underpinning the research can be rigorously and systematically mapped out. However, there is little guidance, or evidence, of this being the case when undertaking small scale projects for example undergraduate or masters degree dissertations. This paper reports four case studies using semi‑structured interviews of master's degree students following management programmes who undertook a Structured Literature Review ({SLR}) based dissertation and the issues and problems they had to encounter during their journey. The findings from the case studies suggest that Tranfield et al's (2003) approach to {SLR}'s, whilst suited to doctoral level and policy based research is not appropriate when dealing with undergraduate and masters dissertations and projects. The case study findings identified that these students conducting a {SLR} had to deal with a new set of conceptual, methodological and data collection problems relating to this 'unorthodox' approach to conducting a postgraduate research dissertation. The findings show that students had to confront new paradigms of enquiry that are not normally taught or found in 'traditional' research texts and research methods courses that are taught on degree programmes. However, the findings do reveal that students gained a greater depth and insight into the subject they were researching through a more rigorous and structured approach. The paper then presents alternative remedies by way of the Rapid Structured Literature Review ({RSLR}) research strategy which is argued as an appropriate approach in conducting small scale literature based research projects when used with undergraduate and master's degree students rather than the {SLR} espoused by Tranfield et al (2003) which is better suited for other types of research such as doctoral and policy based activities.},
	pages = {pp141‑152--pp141‑152},
	number = {2},
	journaltitle = {Electronic Journal of Business Research Methods},
	author = {Armitage, Andrew and Keeble-Allen, Diane},
	date = {2008},
	keywords = {synthesis, based research, rapid structured literature reviews, systematic literature reviews},
}

@article{page_systematic_2008,
	title = {Systematic Literature Searching and the Bibliographic Database Haystack},
	volume = {6},
	url = {https://academic-publishing.org/index.php/ejbrm/article/view/1236},
	abstract = {Researchers performing literature searches are increasingly using bibliographic databases as their initial and dominant resource. While the increasing number, volume and ease of access to academic and other databases potentially speeds searching, researchers require a rapidly evolving set of skills to do this efficiently. Current literature on this topic and research organisations developing techniques in this area are discussed. Aspects to be considered when designing search filters to extract relevant literature are also detailed. Further method development by the author performed during a systematic literature search on the topic of Barriers and constraints for women leaders is additionally examined.},
	pages = {pp199‑208--pp199‑208},
	number = {2},
	journaltitle = {Electronic Journal of Business Research Methods},
	author = {Page, Douglas},
	date = {2008},
	keywords = {literature review, women, analysis, database, Boolean algebra, leadership, meta, social research},
}

@article{snyder_literature_2019,
	title = {Literature review as a research methodology: An overview and guidelines},
	volume = {104},
	doi = {10.1016/J.JBUSRES.2019.07.039},
	abstract = {Knowledge production within the field of business research is accelerating at a tremendous speed while at the same time remaining fragmented and interdisciplinary. This makes it hard to keep up with state-of-the-art and to be at the forefront of research, as well as to assess the collective evidence in a particular area of business research. This is why the literature review as a research method is more relevant than ever. Traditional literature reviews often lack thoroughness and rigor and are conducted ad hoc, rather than following a specific methodology. Therefore, questions can be raised about the quality and trustworthiness of these types of reviews. This paper discusses literature review as a methodology for conducting research and offers an overview of different types of reviews, as well as some guidelines to how to both conduct and evaluate a literature review paper. It also discusses common pitfalls and how to get literature reviews published.},
	pages = {333--339},
	journaltitle = {Journal of Business Research},
	author = {Snyder, Hannah},
	date = {2019},
	note = {Publisher: Elsevier},
	keywords = {Literature review, Systematic review, Integrative review, Research methodology, Synthesis},
}

@article{booth_systematic_2012,
	title = {Systematic Approaches to a Successful Literature Review},
	volume = {26},
	pages = {352--354},
	number = {7},
	journaltitle = {Language Learning},
	author = {Booth, A and Sutton, A and Papaioannou, D},
	date = {2012},
	note = {Publisher: Sage Publications Limited},
	keywords = {Acquisition order, Processability Theory, Processing},
}

@article{paul_art_2020,
	title = {The art of writing literature review: What do we know and what do we need to know?},
	volume = {29},
	doi = {10.1016/J.IBUSREV.2020.101717},
	abstract = {A literature review article provides a comprehensive overview of literature related to a theme/theory/method and synthesizes prior studies to strengthen the foundation of knowledge. In the growing International Business ({IB}) research field, systematic literature reviews have great value, yet there are not many reviews published describing how researchers can design and develop classic review articles. In explaining the purpose, methodology, and structure of a systematic review, we provide guidelines for developing most insightful and useful review articles. By outlining steps and thumb rules to keep in mind, we present an overview of different types of review articles and explain how future researchers could potentially find them useful. In addition, we introduce nine articles finally selected for this special issue of systematic literature review-Looking back to look forward International Business research in the days to come.},
	number = {4},
	journaltitle = {undefined},
	author = {Paul, Justin and Criado, Alex Rialp},
	date = {2020},
	note = {Publisher: Elsevier Ltd},
}

@article{steward_writing_2016,
	title = {Writing a Literature Review:},
	volume = {67},
	url = {https://journals.sagepub.com/doi/abs/10.1177/030802260406701105},
	doi = {10.1177/030802260406701105},
	abstract = {The processes of searching for literature and appraising evidence critically are well documented. Yet effective ways to report literature reviews, either as pieces of research in their own right or...},
	pages = {495--500},
	number = {11},
	journaltitle = {http://dx.doi.org/10.1177/030802260406701105},
	author = {Steward, Barbara},
	date = {2016},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
}

@article{wee_how_2015,
	title = {How to Write a Literature Review Paper?},
	volume = {36},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01441647.2015.1065456},
	doi = {10.1080/01441647.2015.1065456},
	abstract = {This paper discusses the question about how to write a literature review paper ({LRP}). It stresses the primary importance of adding value, rather than only providing an overview, and it then discuss...},
	pages = {278--288},
	number = {2},
	journaltitle = {https://doi.org/10.1080/01441647.2015.1065456},
	author = {Wee, Bert Van and Banister, David},
	date = {2015},
	note = {Publisher: Routledge},
	keywords = {literature review, guidelines, added value, method, paper structure},
}

@incollection{denyer_producing_2009,
	location = {Thousand Oaks,  {CA}},
	title = {Producing a systematic review.},
	isbn = {978-1-4129-3118-2 (Hardcover)},
	abstract = {The aim of this chapter is to provide guidance to scholars, practitioners, and policy makers who are engaged in producing, commissioning, or using reviews of research evidence in the field of management and organization studies. In contrast with many other natural and social science fields, inexperienced researchers, particularly doctoral students in management and organization studies, often receive relatively little training in conducting research reviews. Systematic review is a specific methodology that locates existing studies, selects and evaluates contributions, analyses and synthesizes data, and reports the evidence in such a way that allows reasonably clear conclusions to be reached about what is and is not known. A systematic review should not be regarded as a literature review in the traditional sense, but as a self-contained research project in itself that explores a clearly specified question, usually derived from a policy or practice problem, using existing studies. Additionally, systematic review also differs from other review methods because of its distinct and exacting principles. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {671--689},
	booktitle = {The Sage handbook of organizational research methods.},
	publisher = {Sage Publications Ltd},
	author = {Denyer, David and Tranfield, David},
	date = {2009},
	keywords = {Management, *Experimentation, *Methodology, *Organizational Behavior, *Organizations, Social Sciences},
}

@article{pascoe_systematic_2021,
	title = {Systematic Literature Searching in Social Work: A Practical Guide With Database Appraisal:},
	volume = {31},
	url = {https://journals.sagepub.com/doi/10.1177/1049731520986857?icid=int.sj-full-text.citing-articles.3},
	doi = {10.1177/1049731520986857},
	abstract = {Context:In response to the growth of evidence-based practice in social work, systematic literature reviews offer significant value to social work but are often met with concerns of time scarcity.Pu...},
	pages = {541--551},
	number = {5},
	journaltitle = {https://doi.org/10.1177/1049731520986857},
	author = {Pascoe, Katheryn Margaret and Waterhouse-Bradley, Bethany and {McGinn}, Tony},
	date = {2021},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	keywords = {evidence-based practice, social work, bibliographic, databases, systematic literature searching},
}

@article{wenz_willingness_2019,
	title = {Willingness to use mobile technologies for data collection in a probability household panel},
	volume = {13},
	doi = {10.18148/srm/2019.v13i1.7298},
	abstract = {We asked members of the Understanding Society Innovation Panel about their willingness to participate in various data collection tasks on their mobile devices. We find that stated willingness varies considerably depending on the type of activity involved: respondents are less willing to participate in tasks that involve downloading and installing an app, or where data are collected passively. Stated willingness also varies between smartphones and tablets, and between types of respondents: respondents who report higher concerns about the security of data collected with mobile technologies and those who use their devices less intensively are less willing to participate in mobile data collection tasks.},
	pages = {1--22},
	number = {1},
	journaltitle = {Survey Research Methods},
	author = {Wenz, , Alexander and Jackle, Annette and Couper, Mick P.},
	date = {2019},
	keywords = {{ACCEPTANCE}, Bluetooth, smartphone, tablet, accelerometer, {GPS}, app},
}

@article{hartman_does_2019,
	title = {Does Device or Connection Type Affect Health Preferences in Online Surveys?},
	volume = {12},
	doi = {10.1007/s40271-019-00380-z},
	abstract = {Background and Objective Recent evidence has shown that online surveys can reliably collect preference data, which markedly decrease the cost of health preference studies and expand their representativeness. As the use of mobile technology continues to grow, we wanted to examine its potential impact on health preferences. Methods Two recently completed discrete choice experiments using members of the {US} general population (n = 15,292) included information on respondent device (cell phone, tablet, Mac, {PC}) and internet connection (business, cellular, college, government, residential). In this analysis, we tested for differences in respondent characteristics, participation, response quality, and utility values for the 5-level {EQ}-5D ({EQ}-5D-5L) by device and connection. Results Compared to Mac and {PC} users, respondents using a cell phone or tablet had longer completion times and were significantly more likely to drop out during the surveys (p {\textless} 0.001). Tablet users also demonstrated more logical inconsistencies (p = 0.05). Likewise, respondents using a cellular internet connection exhibit significantly less consistency in their health preferences. However, matched samples for tablets and cell phones produced similar {EQ}-5D-5L utility values (mean differences {\textless} 0.06 on a quality-adjusted life-year [{QALY}] scale for all potential health states). Conclusion Allowing respondents to complete online surveys using a cell phone or tablet or over a cellular connection substantially increases the diversity of respondents and the likelihood of obtaining a representative sample, as many individuals have cell phones but not a computer. While the results showed systematic variability in participation and response quality by device and connection type, this study did not show any meaningful changes in utility values.},
	pages = {639--650},
	number = {6},
	journaltitle = {The Patient-Patient-Centered Outcomes Research},
	author = {Hartman, John D. and Craig, Benjamin M.},
	date = {2019},
	keywords = {{PROBABILITY}-{BASED} {PANEL}, {SMARTPHONES}, {DATA} {QUALITY}, {MOBILE} {WEB}, {SENSITIVE} {TOPICS}, {WEB} {SURVEYS}, {DISCRETE}-{CHOICE} {EXPERIMENTS}, {LOGICAL} {INCONSISTENCIES}, {PROPENSITY} {SCORE}, {STATE} {VALUATIONS}},
}

@article{krebs_exploring_2021,
	title = {{EXPLORING} {SCALE} {DIRECTION} {EFFECTS} {AND} {RESPONSE} {BEHAVIOR} {ACROSS} {PC} {AND} {SMARTPHONE} {SURVEYS}},
	volume = {9},
	doi = {10.1093/jssam/smz058},
	abstract = {The effects of scale direction on response behavior are well known in the survey literature, where a variety of theoretical approaches are discussed, and mixed empirical findings are reported. In addition, different types of survey completion devices seem to vary in their susceptibility to scale direction effects. In this study, we therefore investigate the effect of scale direction and device type on response behavior in {PC} and smartphone surveys. To do so, we conducted a web survey experiment in a German non-probability access panel (N=3,401) using a two-step split-ballot design with four groups that are defined by device type ({PC} and smartphone) and scale direction (decremental and incremental). The results reveal that both {PCs} and smartphones are robust against scale direction effects. The results also show that response behavior differs substantially between {PCs} and smartphones, indicating that the device type ({PC} or smartphone) matters. In particular, the findings show that the comparability of data obtained through multi-device surveys is limited.},
	pages = {477--495},
	number = {3},
	journaltitle = {Journal of Survey Statistics and Methodology},
	author = {Krebs, Dagmar and Höhne, Jan K.},
	date = {2021},
	keywords = {{QUALITY}, {NEED}, {AGREE}/{DISAGREE}, {MOBILE} {WEB}, {TABLETS}, {WEB} {SURVEYS}, {MEASUREMENT} {INVARIANCE}, {ORDER}, {ITEM}-{SPECIFIC} {QUESTIONS}, Latent means, Measurement invariance, Multi-device survey, Rating scales, Response behavior, Scale direction},
}

@article{lugtig_recruiting_2019,
	title = {Recruiting Young and Urban Groups into a Probability-Based Online Panel by Promoting Smartphone Use},
	volume = {13},
	url = {https://www.wiso-net.de/document/SSOA__63138},
	abstract = {A sizable minority of all web surveys are nowadays completed on smartphones. People who choose a smartphone for Internet-related tasks are different from people who mainly use a {PC} or tablet. Smartphone use is particularly high among the young and urban. We have to make web surveys attractive for smartphone completion in order not to lose these groups of smartphone users. In this paper we study how to encourage people to complete surveys on smartphones in order to attract hard-to-reach subgroups of the population. We experimentally test new features of a survey-friendly design: we test two versions of an invitation letter to a survey, a new questionnaire lay-out, and autoforwarding. The goal of the experiment is to evaluate whether the new survey design attracts more smartphone users, leads to a better survey experience on smartphones and results in more respondents signing up to become a member of a probability-based online panel. Our results show that the invitation letter that emphasizes the possibility for smartphone completion does not yield a higher response rate than the control condition, nor do we find differences in the socio-demographic background of respondents. We do find that slightly more respondents choose a smartphone for survey completion. The changes in the layout of the questionnaire do lead to a change in survey experience on the smartphone. Smartphone respondents need 20\% less time to complete the survey when the questionnaire includes autoforwarding. However, we do not find that respondents evaluate the survey better, nor are they more likely to become a member of the panel when asked at the end of the survey. We conclude with a discussion of autoforwarding in web surveys and methods to attract smartphone users to web surveys.},
	pages = {291--306},
	number = {2},
	journaltitle = {methods, data, analyses},
	author = {Lugtig, Peter and Toepoel, Vera and Haan, Marieke and Zandvliet, Robbert and Klein Kranenburg, Laurens},
	date = {2019},
	keywords = {Mobiltelefon, Datengewinnung, data capture, online survey, Online-Befragung, panel, Panel, sample, Stichprobe, survey research, Umfrageforschung, Datenqualität, data quality, cell phone},
}

@article{steinbrecher_why_2015,
	title = {Why Do Respondents Break Off Web Surveys and Does It Matter? Results From Four Follow-up Surveys},
	volume = {27},
	url = {https://academic.oup.com/ijpor/article/27/2/289/745155},
	doi = {10.1093/IJPOR/EDU025},
	pages = {289--302},
	number = {2},
	journaltitle = {International Journal of Public Opinion Research},
	author = {Steinbrecher, Markus and Roßmann, Joss and Blumenstiel, Jan E.},
	date = {2015-06},
	note = {Publisher: Oxford Academic},
}

@article{toepoel_modularization_2018,
	title = {Modularization in an Era of Mobile Web: Investigating the Effects of Cutting a Survey Into Smaller Pieces on Data Quality},
	url = {https://journals.sagepub.com/doi/full/10.1177/0894439318784882},
	doi = {10.1177/0894439318784882},
	abstract = {With the rise of mobile surveys comes the need for shorter questionnaires. We investigate the modularization of an existing questionnaire in the Longitudinal Internet Study for the Social Sciences ...},
	journaltitle = {Social Science Computer Review},
	author = {Toepoel, Vera and Lugtig, Peter},
	date = {2018-07},
	note = {Publisher: {SAGE} {PublicationsSage} {CA}: Los Angeles, {CA}},
	keywords = {data quality, mobile surveys, online surveys, data chunking, data modularization},
}

@article{mavletova_mobile_2014,
	title = {Mobile Web Survey Design: Scrolling versus Paging, {SMS} versus E-mail Invitations},
	volume = {2},
	url = {https://academic.oup.com/jssam/article/2/4/498/2937094},
	doi = {10.1093/JSSAM/SMU015},
	abstract = {There is some evidence that questionnaire design (scrolling or paging) and invitation mode ({SMS} or e-mail) have an impact on response rates in web surveys completed on personal computers ({PCs}). This paper examines whether these findings can be generalized to mobile web surveys. First, we explore the effect of scrolling versus paging design on the breakoff rate, item nonresponse, and completion time in mobile web surveys. Second, we investigate which type of invitation and reminder mode ({SMS} or e-mail) is more effective in terms of producing higher participation rates and maximizing the percentage of respondents who complete the survey via a mobile device rather than a {PC}. The paper summarizes the results of an experiment conducted among members of a volunteer online access panel in Russia, who were asked to complete the survey using a mobile device. We find that the scrolling design leads to significantly faster completion times, lower (though not significantly lower) breakoff rates, fewer technical problems, and higher subjective ratings of the questionnaire. We also find that {SMS} invitations are more effective than e-mail invitations in mobile web surveys.},
	pages = {498--518},
	number = {4},
	journaltitle = {Journal of Survey Statistics and Methodology},
	author = {Mavletova, Aigul and Couper, Mick P.},
	date = {2014-12},
	note = {Publisher: Oxford Academic},
	keywords = {Nonresponse, Mobile web surveys, Invitation mode, Participation rates, Survey design},
}

@article{lambert_living_2015,
	title = {Living with Smartphones: Does Completion Device Affect Survey Responses?},
	volume = {56},
	url = {https://link.springer.com/article/10.1007/s11162-014-9354-7},
	doi = {10.1007/S11162-014-9354-7/TABLES/5},
	abstract = {With the growing reliance on tablets and smartphones for internet access, understanding the effects of completion device on online survey responses becomes increasing important. This study uses data from the Strategic National Arts Alumni Project, a multi-institution online alumni survey designed to obtain knowledge of arts education, to explore the effects of what type of device ({PC}, Mac, tablet, or smartphone) a respondent uses has on his/her responses. Differences by device type in the characteristics of survey respondents, survey completion, time spent responding, willingness to answer complex and open-ended questions, and lengths of open-ended responses are discussed.},
	pages = {166--177},
	number = {2},
	journaltitle = {Research in Higher Education},
	author = {Lambert, Amber D. and Miller, Angie L.},
	date = {2015-03},
	note = {Publisher: Kluwer Academic Publishers},
	keywords = {Smartphones, Completion device, Survey response},
}

@article{erens_comparing_2019,
	title = {Comparing data quality from personal computers and mobile devices in an online survey among professionals Funding acknowledgement},
	volume = {7},
	url = {www.qualtrics.com},
	abstract = {It is increasingly common for respondents to complete web surveys using mobile devices (smartphones and tablets) rather than personal computers/laptops ({PCs}). Evidence of the impact of the use of mobile devices on response and data quality shows mixed results and is only available for general population surveys. We looked at response quality for a work-related survey in the {UK} among general practitioners ({GPs}). {GPs} were sent email invitations to complete a web survey and half (55\%) completed it on a mobile device. While {GPs} using a mobile device were less likely to complete the full questionnaire than those using a {PC}, we found no differences in data quality between mobile and {PC} users, except for {PC} users being more likely to respond to open-ended questions.},
	pages = {15--26},
	journaltitle = {Social Research Practice},
	author = {Erens, Bob and Manacorda, Tommaso and Gosling, Jennifer and Mays, Nicholas and Reid, David and Taylor, William},
	date = {2019},
}

@article{couper_is_2013,
	title = {Is the Sky Falling?  New Technology, Changing Media, and the Future of Surveys},
	volume = {7},
	url = {https://ojs.ub.uni-konstanz.de/srm/article/view/5751},
	doi = {10.18148/SRM/2013.V7I3.5751},
	abstract = {In this paper I review three key technology-related trends: 1) big data, 2) non-probability samples, and 3) mobile data collection.  I focus on the implications of these trends for survey research and the research profession.  With regard to big data, I review a number of concerns that need to be addressed, and argue for a balanced and careful evaluation of the role that big data can play in the future.  I argue that these developments are unlikely to replace transitional survey data collection, but will supplement surveys and expand the range of research methods.  I also argue for the need for the survey research profession to adapt to changing circumstances.},
	pages = {145--156},
	number = {3},
	journaltitle = {Survey Research Methods},
	author = {Couper, Mick P.},
	date = {2013-12},
	keywords = {big data, social media, mobile   surveys, non, organic data, probability surveys},
}

@article{callegaro_mixed-mode_2013,
	title = {From mixed-mode to multiple devices Web surveys, smartphone surveys and apps: has the respondent gone ahead of us in answering surveys?},
	volume = {55},
	doi = {10.2501/IJMR-2013-026},
	pages = {317--320},
	number = {2},
	journaltitle = {International Journal of Market Research},
	author = {Callegaro, Mario},
	date = {2013},
}

@inproceedings{wang_experimentation_2017,
	location = {New York},
	title = {Experimentation for developing evidence-based ui standards of mobile survey questionnaires},
	volume = {Part F127655},
	isbn = {978-1-4503-4656-6},
	url = {http://dx.doi.org/10.1145/3027063.3053181},
	doi = {10.1145/3027063.3053181},
	abstract = {With the growing use of smartphones, many surveys can now be administered using those phones. Such questionnaires are called mobile survey questionnaires. The designer of a mobile survey questionnaire is challenged with presenting text and controls on a small display, while allowing respondents to correctly understand and answer questions with ease. To address this challenge, we are developing an evidencebased framework of user interface design for mobile survey questionnaires. The framework includes two parts: standards for the basic elements of surveyrelevant mobile device operation and guidelines for the building blocks of mobile survey questionnaires. In this presentation, we will describe five behavioral experiments designed to collect evidence for developing the standards. These experiments cover visual perception and motor actions relevant to survey completion. Some preliminary results from ongoing data collection are presented.},
	eventtitle = {Proceedings of the 2017 {CHI} Conference Extended Abstracts on Human Factors in Computing Systems},
	pages = {2998--3004},
	publisher = {Association for Computing Machinery},
	author = {Wang, Lin and Antoun, Christopher and Sanders, Russell and Nichols, Elizabeth and Hawala, Erica L. and Falcone, Brian and Figueroa, Ivonne J. and Katz, Jonathan},
	date = {2017-05},
	keywords = {Framework, Standards, Usability, Mobile survey, Usability {ACM} Classification Keywords H52 User Interfaces: Human Factors},
}

@article{bosch_millennials_2019,
	title = {Do Millennials differ in terms of survey participation?:},
	volume = {61},
	url = {https://journals.sagepub.com/doi/full/10.1177/1470785318815567},
	doi = {10.1177/1470785318815567},
	abstract = {Millennials have been the focus of quite some research because of their differences with older cohorts. Besides, young respondents have been considered as a hard target population for surveys. Howe...},
	pages = {359--365},
	number = {4},
	journaltitle = {International Journal of Market Research},
	author = {Bosch, Oriol J. and Revilla, Melanie and Paura, Ezequiel},
	date = {2019-12},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
	keywords = {smartphones, Millennials, survey participation, survey evaluation, break-off},
}

@article{zou_mobile_2021,
	title = {Mobile vs. {PC}: the device mode effects on tourism online survey response quality},
	volume = {24},
	doi = {10.1080/13683500.2020.1797645},
	abstract = {Using mobile devices to complete web-based surveys is an inescapable trend. Given the growth of this medium, some researchers are concerned about whether mobile devices are a viable channel for administering self-report online surveys. Taking two online surveys respectively using the {US} and China samples, this study compared the responses quality between participants responding via mobile devices and via {PCs}. Results from both the {US} and China samples revealed that although mobile respondents took longer to complete surveys than {PC} respondents, response quality did not differ significantly between these groups. Several behaviour patterns among mobile respondents were also identified in both samples. These findings provide practical implications to optimize web-based surveys for mobile users in tourism and hospitality research.},
	pages = {1345--1357},
	number = {10},
	journaltitle = {Current Issues in Tourism},
	author = {Zou, Suiwen and Tan, Karen P. and Liu, Hongbo and Li, Xiang and Chen, Ye},
	date = {2021},
	keywords = {{TECHNOLOGY}, {ISSUES}, Mobile device, {RATES}, {COMPUTER}, Online survey, Response quality, Theory ofsatisficing, {WEB} {SURVEY} {DESIGN}},
}

@article{zijlstra_traditional_2018,
	title = {Traditional and mobile devices in computer assisted web-interviews},
	volume = {32},
	url = {https://www.wiso-net.de/document/BEFO__20181041493-BEFO-ZDEE},
	abstract = {Transport studies constantly rely on surveys among travelers. Computer assisted web interview is the most popular survey mode. However, respondents complete online surveys nowadays using smartphones, tablets or traditional devices. We address three questions related to this development: how important are mobile respondents; how to deal with mobile respondents; and what is the effect of mobile response in a survey? In order to answer these questions, we used a series of information dense meta-analyses and state-of-the-art literature. Our results reveal that one out of every three respondents used a mobile device in 2016. The profile of mobile respondents adheres to the profiles of hard-to-reach candidates. Four design strategies for mixed-device surveys are identified and discussed. By taking an active approach to mixed-device surveys, multiple issues associated with mobile response can be overcome, i.e. differences in completion times and break-offs can be minimized. Mobile respondents appreciate redesigned surveys. Our results are in favor of facilitating mobile respondents with an adaptive or responsive web design in surveys.},
	pages = {184--194},
	journaltitle = {Transportation Research Procedia},
	author = {Zijlstra, Toon and Wijgergangs, Krisje and Hoogendoorn-Lanser, Sascha},
	date = {2018},
	keywords = {Online-Überwachung, Smartphone, mobiles Gerät, Datenqualität, Mischmaschine, Mischvorrichtung},
}

@article{cameron_mobile_2018,
	title = {Mobile Apps versus Web Browsers: A Comparison of Self-administered Survey Platforms},
	volume = {31},
	url = {https://www.proquest.com/scholarly-journals/mobile-apps-versus-web-browsers-comparison-self/docview/2170368207/se-2?accountid=14570},
	abstract = {Cameron and Gentleman investigate some of the responses to a series of four surveys administered on both the 23andMe website and 23andMe mobile app to understand differences between web-based and mobile-based data collection. The surveys cover a wide variety of topics, including socioeconomic status ({SES}), tobacco use, allergies, and caffeine intake, and were chosen because they were available to all customers on both platforms. The use of mobile applications to capture research data efficiently is a promising technique for researchers. However, with the growth of mobile-app-based data collection, the use of mobile apps as a research tool may continue to be an important complement to many of the more-traditional web-based data collection techniques.},
	pages = {29--29},
	number = {4},
	journaltitle = {Chance},
	author = {Cameron, Briana and Gentleman, Robert},
	date = {2018},
	keywords = {Platforms, Data collection, Statistics, Research methodology, Polls \& surveys, Web sites, Applications programs, Mobile computing, Software, Websites, Collection, Caffeine, Tobacco},
}

@article{bacon_how_2017,
	title = {How Effective Are Emojis In Surveys Taken on Mobile Devices? Data-Quality Implications and the Potential To Improve Mobile-Survey Engagement and Experience},
	volume = {57},
	doi = {10.2501/JAR-2017-053},
	abstract = {The Advertising Research Foundation's ongoing How Advertising Works program combines original experiments with outside research. Launched in 2015, the program intends to offer practical guidance for improving advertising effectiveness across media and across platforms. The latest investigations led by Advertising Research Foundation Executive Researcher Christopher Bacon focused on the quality of survey research on mobile devices, which consumers increasingly are using to respond to online surveys. Specifically, the authors explored the use of symbols (emojis) as an alternative to text in the design of mobile surveys to keep respondents from abandoning the survey and to improve user experience. In the pages that follow, the authors explain the historical precedent for using symbols as communication devices, the importance of mobile-survey design using symbols, and the implication for data quality and effective survey design going forward.},
	pages = {462--470},
	number = {4},
	journaltitle = {Journal of Advertising Research},
	author = {Bacon, Christopher and Barlas, Frances M. and Dowling, Zoe and Thomas, Randall K.},
	date = {2017},
}

@article{huff_comparison_2015,
	title = {The comparison of mobile devices to computers for web-based assessments},
	volume = {49},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2015-20975-025%26site%3dehost-live},
	doi = {10.1016/j.chb.2015.03.008},
	abstract = {This research investigated the completing of a web-based personality assessment using smart phones and computers. Data were collected from 47 undergraduate students using a within subjects design. Results indicated that the usability and the time to complete the assessment of a web-based non-optimized questionnaire is significantly different when completed with a smart phone versus a computer. However, there were no significant differences in personality scores. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {208--212},
	journaltitle = {Computers in Human Behavior},
	author = {Huff, Kyle C.},
	date = {2015},
	keywords = {Mobile Phones, Computer, Computers, {INTERNET}, {PERSONALITY}, {DESIGN}, Personality, Usability, {TESTS}, Mobile assessment, Personality Measures, Time, {ABILITY}, {EQUIVALENCE}, {PAPER}-{AND}-{PENCIL}},
}

@article{skeie_smartphone_2019,
	title = {Smartphone and tablet effects in contingent valuation web surveys - No reason to worry?},
	volume = {165},
	doi = {10.1016/j.ecolecon.2019.106390},
	abstract = {Stated preference ({SP}) web surveys are increasingly completed on mobile devices such as smartphones and tablets instead of computers. Due to differences in technical attributes and response contexts of the devices, this trend may affect the quality of the survey data and elicited welfare measures. Little is known of such device effects in {SP} research. In the first such study of its kind, we compare willingness to pay ({WTP}) and response quality between devices in a large, national contingent valuation survey. Propensity score matching is used to distinguish device effects from observed sample composition effects due to self-selection. We find significantly higher {WTP} for smartphone respondents in the first out of four sequential {WTP} questions, and no differences for tablets. Concerning data (response) quality, results are mixed, but not consistently lower for smartphones and tablets compared to computers. Measured by indicators of response randomness, shares of don't know and protest zeros, smartphone responses even show signs of higher quality. Only in terms of the extent of internal scope sensitivity, do smartphones and tablets fare somewhat worse than computers. Overall, our results do not indicate substantial loss of response quality or differences in welfare measures for mobile devices.},
	journaltitle = {Ecological Economics},
	author = {Skeie, Magnus A. and Lindhjem, Henrik and Skjeflo, Sofie and Navrud, Ståle},
	date = {2019},
	keywords = {Mobiltelefon, Ökosystem, {IMPACT}, {QUALITY}, Personal Computer, {INTERNET}, Umweltökonomik, Zahlungsbereitschaftsanalyse, Mobile device, {MOBILE} {DEVICES}, Contingent valuation, Ecosystem services, {MAIL}, {MODES}, Propensity score matching, {SCOPE}, {STATED}-{PREFERENCE} {SURVEYS}, Survey quality},
}

@inproceedings{olmsted-hawala_optimal_2018,
	location = {Cham},
	title = {Optimal Data Entry Designs in Mobile Web Surveys for Older Adults},
	volume = {10926},
	doi = {10.1007/978-3-319-92034-4_26},
	abstract = {Growing numbers of people are using their mobile phones to respond to online surveys. As a result, survey designers face the challenge of displaying questions and their response options and navigation elements on small smartphone screens in a way that encourages survey completion. The purpose of the present study was to conduct a series of systematic assessments of how older adults using smartphones interact with different user-interface features in online surveys. This paper shares results of three different experiments. Experiment 1 compares different ways of displaying choose-one response options. Experiment 2 compares different ways of displaying numeric entry boxes, specifically ones used to collect currency information (e.g., prices, costs, salaries). Experiment 3 tests whether forward and backward navigational buttons on a smartphone survey should be labeled with words (previous, next) or simply indicated with arrow icons (). Results indicate that certain features such as picker-boxes that appear at the bottom of the screen ({iOS} devices), fixed formatting of numeric-entry boxes, and icon navigation buttons were problematic. They either had negative impacts on performance (response times and/or accuracy) or only a small percentage of participants preferred these design features when asked to compare them to the other features.},
	eventtitle = {Human Aspects of {IT} for the Aged Population. Acceptance, Communication and Participation. {ITAP} 2018. Lecture Notes in Computer Science, vol 10926.},
	pages = {335--335},
	publisher = {Springer},
	author = {Olmsted-Hawala, Erica and Nichols, Elizabeth and Falcone, Brian and Figueroa, Ivonne J. and Antoun, Christopher and Wang, Lin},
	editor = {Zhou, J. and Salvendy, G.},
	date = {2018-06},
	keywords = {Rückmeldezeit, Online-Überwachung, Smartphone, Erwachsener, Leitfaden, älterer Mensch, Währung},
}

@article{toepoel_online_2015,
	title = {Online surveys are mixed-device surveys: issues associated with the use of different (mobile) devices in web surveys},
	volume = {9},
	url = {https://www.wiso-net.de/document/MDA__45668},
	pages = {155--162},
	number = {2},
	journaltitle = {methods, data, analyses},
	author = {Toepoel, Vera and Lugtig, Peter},
	date = {2015},
	keywords = {Antwortverhalten, response behavior},
}

@article{buskirk_are_2015,
	title = {Are sliders too slick for surveys? An experiment comparing slider and radio button scales for smartphone, tablet and computer based surveys},
	volume = {9},
	url = {https://www.wiso-net.de/document/MDA__45666},
	abstract = {"The continued rise in smartphone penetration globally afford survey researchers with an unprecedented portal into personal survey data collection from respondents who could complete surveys from virtually any place at any time. While the basic research into optimizing the survey experience and data collection on mobile devices has continued to develop, there are still fundamental gaps in our knowledge of how to optimize certain types of questions in the mobile setting. In fact, survey researchers are still trying to understand which online design principles directly translate into presentation on mobile devices and which principles have to be modified to incorporate separate methods for these devices. One such area involves the use of input styles such as sliding scales that lend themselves to more touch centric input devices such as smartphones or tablets. Operationalizing these types of scales begs the question of an optimal starting position and whether these touch centric input styles are equally preferred by respondents using less touch capable devices. While an outside starting position seems optimal for slider questions completed via computer, this solution may not be optimal for completion via mobile devices as these devices are subjected to far more space and layout constraints compared to computers. This experiment moves the mixed device survey literature forward by directly comparing outcomes from respondents who completed a collection of survey scales using their smartphone, tablet or computer. Within each device, respondents were randomly assigned to complete one of 20 possible versions of scale items determined by a combination of three experimental factors including input style, length and number formatting. Results from this study suggest more weaknesses than strengths for using slider scales to collect survey data using mobile devices and also suggest that preference for these touch centric input styles varies across devices and may not be as high as the preference for the more traditional radio button style." (author's abstract)},
	pages = {229--260},
	number = {2},
	journaltitle = {methods, data, analyses},
	author = {Buskirk, Trent D.},
	date = {2015},
	keywords = {Mobiltelefon, Datengewinnung, Antwortverhalten, data capture, online survey, Online-Befragung, response behavior, survey research, Umfrageforschung, Datenqualität, data quality, cell phone, computer, Computer},
}

@article{arn_evaluation_2015,
	title = {Evaluation of an adapted design in a multi-device online panel: a {DemoSCOPE} case study},
	volume = {9},
	abstract = {"In this paper, we look at the challenge of optimizing survey layout in online research to enable multi-device use. Several studies provide useful advice on target-oriented implementation of web design for {CAWI} surveys. This paper presents results of the implementation of a new adapted design at the panel of {DemoSCOPE} that allows the participants to take part in a survey on multiple (especially mobile) devices. To evaluate this adapted design, we compare interview data and question timing of panellists who participated in an insurance study before and after the design transition. Central key figures concerning the completion rate, item non-response, open questions, straightlining, timing of single questions and the length of the total interview are presented. In addition, we have presented examples of both old and new design to the community and invited them to assess these examples concerning orientation, color, design and usability. We evaluate the differences in these assessments before and after the design transition for smartphone and desktop users. We end with suggestions for best practice for online studies on different devices." (author's abstract)},
	pages = {185--212},
	number = {2},
	journaltitle = {methods, data, analyses},
	author = {Arn, Birgit and Klug, Stefan and Kolodziejski, Janusz},
	date = {2015},
	keywords = {survey research, Umfrageforschung},
}

@article{struminskaya_effects_2015,
	title = {The effects of questionnaire completion using mobile devices on data quality: evidence from a probability-based general population panel},
	volume = {9},
	url = {https://www.wiso-net.de/document/MDA__45667},
	abstract = {"The use of mobile devices such as smartphones and tablets for survey completion is growing rapidly, raising concerns regarding data quality in general, and nonresponse and measurement error in particular. We use the data from six online waves of the {GESIS} Panel, a probability-based mixed-mode panel representative of the German population to study whether the responses provided using tablets or smartphones differ on indicators of measurement and nonresponse errors from responses provided via personal computers or laptops. We follow an approach chosen by Lugtig and Toepoel (2015), using the following indicators of nonresponse error: item nonresponse, providing an answer to an open question; and the following indicators of measurement error: straightlining, number of characters in open questions, choice of left-aligned options in horizontal scales, and survey duration. Moreover, we extend the scope of past research by exploring whether data quality is a function of device-type or respondent-type characteristics using multilevel models. Overall, we find that responding with mobile devices is associated with a higher likelihood of measurement discrepancies compared to {PC}/laptop survey completion. For smartphone survey completion, the indicators of measurement and nonresponse error tend to be higher than for tablet completion. We find that most indicators of nonresponse and measurement error used in our analysis cannot be attributed to the respondent characteristics but are rather effects of mobile devices." (author's abstract)},
	pages = {261--292},
	number = {2},
	journaltitle = {methods, data, analyses},
	author = {Struminskaya, Bella and Weyandt, Kai and Bosnjak, Michael},
	date = {2015},
	keywords = {survey research, Umfrageforschung},
}

@article{fuchs_coverage_2009,
	title = {The Coverage Bias of Mobile Web Surveys Across European Countries},
	volume = {4},
	abstract = {In recent years, mobile devices are increasingly considered to access the World Wide Web. Several survey research organizations are about to use this technology as a means of conducting self-administered surveys. Among other advantages it allows survey researchers to overcome the lack of random selection procedures in online surveys since it provides the opportunity to use {RDD}-like probability sampling of cell phone numbers. However, low penetration rates of smart phones raise concerns that the coverage bias of a mobile Web survey might in fact harm survey estimates considerably. In this paper, we report results of a simulation study on the coverage bias of the mobile Web population across European countries. Based on a subset of the Eurobarometer data we estimate the relative coverage bias of the smart phone population in contrast to the general population. Even though we observed an incline of the mobile Web penetration rates over the course of the past years, coverage biases were still considerably large for socio-demographic variables. Nevertheless, in a few European countries mobile Web coverage biases are already smaller than the coverage biases of the population with traditional landline Internet access. Adapted from the source document.},
	pages = {21--33},
	number = {1},
	journaltitle = {International Journal of Internet Science},
	author = {Fuchs, Marek and Busse, Britta},
	date = {2009},
	keywords = {data collection, Simulation, Sampling, Internet, data quality, survey, Surveys, smart phone, Bias, Access, Europe, political behavior, article, 0827: mass phenomena, public opinion, 9121: political behavior, Courses, Mobile Web},
}

@article{mavletova_sensitive_2013,
	title = {Sensitive Topics in {PC} Web and Mobile Web Surveys: Is There a Difference?},
	volume = {7},
	abstract = {A large number of findings in survey research suggest that responses to sensitive questions are situational and can vary in relation to context. The methodological literature demonstrates that social desirability biases are less prevalent in self-administered surveys, particularly in Web surveys, when there is no interviewer and less risk of presenting oneself in an unfavorable light. Since there is a growing number of users of mobile Web browsers, we focused our study on the effects of different devices ({PC} or cell phone) in Web surveys on the respondents' willingness to report sensitive information. To reduce selection bias, we carried out a two-wave cross-over experiment using a volunteer online access-panel in Russia. Participants were asked to complete the questionnaire in both survey modes: {PC} and mobile Web survey. We hypothesized that features of mobile Web usage may affect response accuracy and lead to more socially desirable responses compared to the {PC} Web survey mode. We found significant differences in the reporting of alcohol consumption by mode, consistent with our hypothesis. But other sensitive questions did not show similar effects. We also found that the presence of familiar bystanders had an impact on the responses, while the presence of strangers did not have any significant effect in either survey mode. Contrary to expectations, we did not find evidence of a positive impact of completing the questionnaire at home and trust in data confidentiality on the level of reporting. These results could help survey practitioners to design and improve data quality in Web surveys completed on different devices.},
	pages = {191--205},
	number = {3},
	journaltitle = {Survey Research Methods},
	author = {Mavletova, Aigul and Couper, Mick P.},
	date = {2013},
	keywords = {data quality, {BEHAVIOR}, {INTERNET}, {DATA}-{COLLECTION} {MODE}, {SOCIAL} {DESIRABILITY} {BIAS}, {PRIVACY}, mobile Web surveys, Web surveys, {PAPER}-{AND}-{PENCIL}, sensitive questions, {ALCOHOL}-{CONSUMPTION}, {CONFIDENTIALITY} {CONCERNS}, {DRUG}-{USE}, interview setting, perceived privacy, presence of bystanders, {SELF}-{ADMINISTERED} {QUESTIONNAIRES}},
}

@article{wells_what_2015,
	title = {What market researchers should know about mobile surveys},
	volume = {57},
	doi = {10.2501/IJMR-2015-045},
	abstract = {Survey completions on mobile devices have been increasing rapidly. This important shift is something market researchers should definitely consider when designing and conducting self-administered online surveys. This article briefly summarises existing research and empirical results from mobile surveys. Based on the specific findings discussed, market researchers should be better aware of what to expect when fielding surveys completed by mobile respondents, whether this is intended or not. Bringing the findings together and discussing more broadly, for online surveys, market researchers should consider consciously and deliberately accommodating both mobile and {PC} respondents. Thus far, the research on mobile surveys indicates that consumers want the choice and ability to take surveys when they want, where they want and on the device of their choosing. It is to be hoped that market researchers are listening and become willing to accommodate survey respondents in terms of device and, by extension, time and location.},
	pages = {521--532},
	number = {4},
	journaltitle = {International Journal of Market Research},
	author = {Wells, Tom},
	date = {2015},
	keywords = {{QUALITY}, {PC}, {WEB}},
}

@article{liebe_does_2015,
	title = {Does the use of mobile devices (tablets and smartphones) affect survey quality and choice behaviour in web surveys?},
	volume = {14},
	doi = {10.1016/j.jocm.2015.02.002},
	abstract = {Web surveys are becoming increasingly popular in survey research including stated preference surveys. Compared with face-to-face, telephone and mail surveys, web surveys may contain a different and new source of measurement error and bias: the type of device that respondents use to answer the survey questions. This is the first study that tests whether the use of mobile devices, tablets or smartphones, affects survey characteristics and stated preferences in a web-based choice experiment. The web survey on expanding renewable energy production in Germany was carried out with 3182 respondents, of which 12\% used a mobile device. Propensity score matching is used to account for selection bias in the use of mobile devices for survey completion. We find that mobile device users spent more time than desktop/laptop users to answer the survey. Yet, desktop/laptop users and mobile device users do not differ in acquiescence tendency as an indicator of extreme response patterns. For mobile device users only, we find a negative correlation between screen size and interview length and a positive correlation between screen size and acquiescence tendency. In the choice experiment data, we do not find significant differences in the tendency to choose the status quo option and scale between both subsamples. However, some of the estimates of implicit prices differ, albeit not in a unidirectional fashion. Model results for mobile device users indicate a U-shaped relationship between error variance and screen size. Together, the results suggest that using mobile devices is not detrimental to survey quality. (C) 2015 Elsevier Ltd. All rights reserved.},
	pages = {17--31},
	journaltitle = {Journal of choice modelling},
	author = {Liebe, Ulf and Glenk, Klaus and Oehlmann, Malte and Meyerhoff, Jürgen},
	date = {2015},
	keywords = {Smartphone, {SCALE}, {INTERNET}, Mobile device, Choice experiment, Renewable energy, {FACE}-{TO}-{FACE}, Propensity score matching, Survey quality, Acquiescence bias, {COMPLEXITY}, Sample selection bias, Survey format, {VALUATION}},
}

@article{toninelli_smartphones_2016,
	title = {Smartphones vs {PCs}: Does the Device Affect the Web Survey Experience and the Measurement Error for Sensitive Topics? A Replication of the Mavletova \& Couper's 2013 Experiment},
	volume = {10},
	doi = {10.18148/srm/2016.v1012.6274},
	abstract = {More and more respondents use mobile devices to complete web surveys. These devices have different characteristics, if compared to {PCs} (e.g. smaller screen sizes and higher portability). These characteristics can affect the survey responses, mostly when a questionnaire includes sensitive questions. This topic was already studied by Mavletova and Couper (2013), through a two-wave experiment comparing {PCs} and mobile devices results for the same respondents in a Russian opt-in panel. We replicated this cross-over design, focusing on an opt-in panel for Spain, involving 1,800 panellists and comparing {PCs} and smartphones. Our results support most of Mavletova and Couper's (2013) findings (e.g. generally the used device does not significantly affect the reporting of sensitive information), confirming their robustness over the two studied countries. For other results (e.g. trust in data confidentiality), we found differences that can be justified by the diverse context/culture or by the quick changes that are still characterizing the mobile web survey participation.},
	pages = {153--169},
	number = {2},
	journaltitle = {Survey Research Methods},
	author = {Toninelli, Daniele and Revilla, Melanie},
	date = {2016},
	keywords = {smartphones, {BIAS}, measurement error, {COMPUTERS}, {MODE}, {NONRESPONSE}, Web surveys, {SCREEN} {SIZE}, mobile participation, sensitive questions, survey optimization},
}

@article{revilla_online_2016,
	title = {Do online access panels need to adapt surveys for mobile devices?},
	volume = {26},
	doi = {10.1108/IntR-02-2015-0032},
	abstract = {Purpose - Despite the quick spread of the use of mobile devices in survey participation, there is still little knowledge about the potentialities and challenges that arise from this increase. The purpose of this paper is to study how respondents' preferences drive their choice of a certain device when participating in surveys. Furthermore, this paper evaluates the tolerance of participants when specifically asked to use mobile devices and carry out other specific tasks, such as taking photographs.
Design/methodology/approach - Data were collected by surveys in Spain, Portugal and Latin America by Netquest, an online fieldwork company.
Findings - Netquest panellists still mainly preferred to participate in surveys using personal computers. Nevertheless, the use of tablets and smartphones in surveys showed an increasing trend; more panellists would prefer mobile devices, if the questionnaires were adapted to them. Most respondents were not opposed to the idea of participating in tasks such as taking photographs or sharing {GPS} information.
Research limitations/implications - The research concerns an opt-in online panel that covers a specific area. For probability-based panels and other areas the findings may be different.
Practical implications - The findings show that online access panels need to adapt their surveys to mobile devices to satisfy the increasing demand from respondents. This will also allow new, and potentially very interesting data collection methods.
Originality/value - This study contributes to survey methodology with updated findings focusing on a currently underexplored area. Furthermore, it provides commercial online panels with useful information to determine their future strategies.},
	pages = {1209--1227},
	number = {5},
	journaltitle = {Internet Research},
	author = {Revilla, Melanie and Toninelli, Daniele and Ochoa, Carlos and Loewe, Germán},
	date = {2016},
	keywords = {Survey, Methodology, {ADOPTION}, World Wide Web, {PC}, {WEB} {SURVEYS}, {COMPUTERS}, Mobile communications, Netquest, Technological innovation},
}

@article{brosnan_pc_2017,
	title = {{PC}, phone or tablet? Use, preference and completion rates for web surveys},
	volume = {59},
	doi = {10.2501/IJMR-2016-049},
	abstract = {This study investigates whether it is the case that representativity is undermined if personal computer, tablet and smartphone respondents differ in sociodemographic characteristics and display different survey completion rates. Online market research is struggling with sample representativity. The analysis of more than ten million survey invitations, as well as stated device preference information, suggests that web survey respondents who are members of online panels still mostly use their personal computers, but do express increasing interest in using smartphones and tablets. Survey completion rates do vary across devices, and device use is significantly associated with socio-demographic characteristics and length of membership on a panel. Therefore, researchers must not limit respondents to use a specific device for completing a survey as this may compromise the quality of the survey completion experience, increase non response error and negatively affect representativity.},
	pages = {35--55},
	number = {1},
	journaltitle = {International Journal of Market Research},
	author = {Brosnan, Kylie and Grün, Bettina and Dolnicar, Sara},
	date = {2017},
	keywords = {Mobile Devices, mobile devices, {QUALITY}, online research, Preferences, Tablet Computers, Consumer Attitudes, {DESIGN}, {MOBILE}, web surveys, {METAANALYSIS}, {PANELS}, {MODES}, Consumer Research, {COMPARING} {RESPONSE} {RATES}, {MAIL} {SURVEYS}, Consumer Surveys, completion rates, device preference, device use},
}

@article{bansal_shorter_2017,
	title = {Shorter interviews, longer surveys Optimising the survey participant experience while accommodating ever expanding client demands},
	volume = {59},
	doi = {10.2501/IJMR-2017-016},
	abstract = {This paper explores strategies on how to best balance expanding survey length with the need for concise, relevant and engaging surveys, deployed in a device agnostic format. When designing a survey we, as an industry, are often seeking a balance between competing design challenges: clients have diverse and extensive objectives, survey participants have short attention spans and an ever increasing suite of connected devices to choose from. Survey participants are voting with their feet when surveys are not compatible with the device they want to use, whether that is the smart device in their pocket or laptop they are working on, and this is very real for online panels. We are seeing increased abandon rates, with the effects of extended fieldwork times, smaller pools of sample to draw from and the possibility of introducing bias into our data. Having spent much of 2015 working with clients to design more smart-device friendly surveys, Research Now has explored innovative ways to shorten survey length without compromising on the amount of material covered. Following on from work by Johnson et al. (2014), Research Now conducted a piece of primary research exploring survey modularisation as discussed in the current paper. The approach splits questionnaires into modules, with participants receiving only a specific module, a subset of the overall survey. It is expected that a long questionnaire can be split and when applied appropriately, designed properly and implemented effectively data can yield results comparable with a full non-modular survey. Building on previous industry work on this topic, and primary research conducted by Research Now, we discuss our methodology, the results and conclusions from this work, and explore opportunities to automate the approach. The overall goal of this study and resulting paper is to explore how adapting survey research in this way improves rather than complicates the lives of both researchers and research participants. If we are not able to shorten our surveys, then survey modularisation may prove to be our best hope for a complete, representative dataset and we need to ensure that this is achieved accurately, confidently and efficiently at scale.},
	pages = {221--238},
	number = {2},
	journaltitle = {International Journal of Market Research},
	author = {Bansal, Harvir S. and Eldridge, James and Halder, Avik and Knowles, Roddy and Murray, Michael and Sehmer, Luke and Turner, David},
	date = {2017},
	keywords = {{DESIGN}},
}

@article{tourangeau_web_2017,
	title = {{WEB} {SURVEYS} {BY} {SMARTPHONE} {AND} {TABLETS} {EFFECTS} {ON} {SURVEY} {RESPONSES}},
	volume = {81},
	doi = {10.1093/poq/nfx035},
	abstract = {With respondents increasingly completing web surveys on tablet computers and smartphones, several studies have examined the potential effects of the switch from {PCs} to mobile devices. The studies have looked at a range of outcomes, including completion rates, breakoffs, and item nonresponse. We carried out a field experiment that compared responses obtained by smartphones, tablets, and laptop computers, focusing on the potential effects of the different devices on measurement errors. We examined whether the differences across devices in screen size (and the related need to scroll to see the entire question or the full set of response options) might moderate the effects of response order, affect the strategy respondents used to decide which of two options was preferable, change the effect of question context, or influence the use of definitions. Our experiments were based on the principle of visual prominence-the idea that respondents are more likely to notice and consider information that is easy to see. The experiments were deliberately designed to maximize the impact of screen size on the results, since the screen size would affect the visual prominence of key information. However, like many of the prior studies examining mobile devices, although response order, context, and evaluation strategy affected the answers respondents gave, few device effects emerged.},
	pages = {896--929},
	number = {4},
	journaltitle = {Public Opinion Quarterly},
	author = {Tourangeau, Roger and Maitland, Aaron and Rivero, Gonzalo and Sun, Hanyu and Williams, Douglas and Yan, Ting},
	date = {2017},
	keywords = {{DESIGN}, {MOBILE}, {COMPUTER}, {SENSITIVE} {TOPICS}, {OPTIONS}, {PREFERENCE} {REVERSALS}},
}

@article{toepoel_sliders_2018,
	title = {Sliders, visual analogue scales, or buttons: Influence of formats and scales in mobile and desktop surveys},
	volume = {25},
	doi = {10.1080/08898480.2018.1439245},
	abstract = {In an experiment dealing with the use of personal computer, tablet, or mobile, scale points (up to 5, 7, or 11) and response formats (bars or buttons) are varied to examine differences in mean scores and nonresponse. The total number of not applicable answers does not vary significantly. Personal computer has the lowest item nonresponse, followed by mobile and tablet, and a lower mean score than for mobile. Slider bars showed lower mean scores and more nonresponses than buttons, indicating that they are more prone to bias and difficult in use. Sider bars, which work with a drag-and-drop principle, perform worse than visual analogue scales working with a point-and-click principle and buttons. Five-point scales have more nonresponses than eleven-point scales. Respondents evaluate 11-point scales more positively than shorter scales.},
	pages = {112--122},
	number = {2},
	journaltitle = {Mathematical Population Studies},
	author = {Toepoel, Vera and Funke, Frederik},
	date = {2018},
	keywords = {visual analogue scales, mobile surveys, {DESIGN}, {WEB} {SURVEYS}, {RELIABILITY}, questionnaire design, {OPTIMAL} {NUMBER}, {RATING}-{SCALES}, Likert scale, {POINTS}, response formats, slider bars},
}

@article{deleeuw_mixed-mode_2018,
	title = {Mixed-Mode: Past, Present, and Future},
	volume = {12},
	doi = {10.18148/srm/2018.v12i2.7402},
	abstract = {Mixed-mode surveys have been around since the late 1980s. In the past thirty years, major changes in technology and society influenced and changed data collection and survey methodology. However, in those years, mixed-mode strategies remained part of the daily survey practice, although the type of mix implemented followed the changes in technology and data collection methods. In this paper, I summarize the state of the art in traditional mixed-mode surveys and discuss implications for mixed device surveys.},
	pages = {75--89},
	number = {2},
	journaltitle = {Survey Research Methods},
	author = {{DeLeeuw}, Edith D.},
	date = {2018},
	keywords = {{IMPACT}, mobile surveys, {METAANALYSIS}, prevention, {SOCIAL} {DESIRABILITY} {BIAS}, online surveys, {FACE}-{TO}-{FACE}, {WEB} {SURVEYS}, {ASSOCIATION}, adjustment, {DESIGNS}, equivalence, mode measurement effect, mode selection effect, multiple devices, multiple modes, {NONRESPONSE} {BIAS}, offline surveys, {PARTICIPATION} {RATES}, {TELEPHONE} {SURVEYS}},
}

@article{de_bruijne_improving_2014,
	title = {Improving Response Rates and Questionnaire Design for Mobile Web Surveys},
	volume = {78},
	url = {https://www.proquest.com/scholarly-journals/improving-response-rates-questionnaire-design/docview/1636484190/se-2?accountid=14570},
	doi = {10.1093/poq/nfu046},
	abstract = {This research note presents the results of an experiment that investigated how response rates and data quality could be improved for smartphone web surveys. First, we compare how invitations by text message versus by e-mail affect response rate. Text message invitations result in a similar total response rate as e-mail invitations when considering response via all types of online devices. When considering response via smartphones only, the survey completion rate is significantly higher. The text message contact mode also leads to a faster speed of initial response. Second, we examine the response effects of several questionnaire-design choices when using smartphones: paging versus scrolling, horizontal versus vertical question layout, number of answer options, and open-ended versus closed-ended questions. According to our findings, a scrolling layout leads to a shorter completion time than a paging layout. We further suggest that caution be used with horizontal and long-answer scales as well as open-ended text answer fields in smartphone surveys.},
	pages = {951--962},
	number = {4},
	journaltitle = {Public Opinion Quarterly},
	author = {De Bruijne, Marika and Wijnant, Arnaud},
	date = {2014},
	keywords = {Studies, Internet, Smartphones, Polls \& surveys, Questionnaires, Surveys, Research, Comparative analysis, Experiments, Response rates, {ORDER}, 9130:Experiment/theoretical treatment, political behavior, article, 7100:Market research, 0827: mass phenomena, Data Quality, public opinion, Choices, Research Design, Research Responses, 9121: political behavior, Electronic Mail, Political Science},
}

@article{ha_are_2019,
	title = {Are computers better than smartphones for web survey responses?},
	volume = {43},
	doi = {10.1108/OIR-11-2017-0322},
	abstract = {Purpose The purpose of this paper is to examine the effect of smartphones and computers as web survey entry response devices on the quality of responses in different question formats and across different survey invitations delivery modes. The respondents' preference of device and the response immediacy were also compared.
Design/methodology/approach Two field experiments were conducted with a cluster sampling and a census of all students in a public university in the {USA}.
Findings Device effect on response quality was only found when using computer-aided self-interviews, but not in e-mail delivered web surveys. Even though the computer was the preferred device, but the smartphone's immediate response was significantly higher than the computer.
Research limitations/implications The sample was restricted to college students who are more proficient users of smartphones and have high access to computers. But the direct comparison in the two studies using the same population increases the internal validity of the study comparing different web survey delivery modes.
Practical implications Because of the minor differences in device on response quality, researchers can consider using more smartphones for field work such as computer-aided self-interviews to complement e-mail delivered surveys.
Originality/value This is the first study that compares the response device effects of computer-aided self-interviews and e-mailed delivered web surveys. Because web surveys are increasingly used and various devices are being used to collect data, how respondents behave in different devices and the strengths and weaknesses of different methods of delivery survey help researchers to improve data quality and develop effective web survey delivery and participant recruitment.},
	pages = {350--368},
	number = {3},
	journaltitle = {Online Information Review},
	author = {Ha, , Louisa and Zhang, Chenjie},
	date = {2019},
	keywords = {Smartphones, {QUALITY}, {DESIGN}, {MOBILE}, {PC} {WEB}, {MODES}, Computer-aided self-interviews, {DATA}-{COLLECTION}, Mobile survey, Survey response quality, Web survey},
}

@article{haan_can_2019,
	title = {Can we predict device use? An investigation into mobile device use in surveys},
	volume = {22},
	doi = {10.1080/13645579.2019.1593340},
	abstract = {In this study, we investigate whether mobile device use in surveys can be predicted. We aim to identify possible motives for device use and build a model by drawing on theory from technology acceptance research and survey research. We then test this model with a Structural Equation Modeling approach using data of seven waves of the {GESIS} panel. We test whether our theoretical model fits the data by focusing on measures of fit, and by studying the standardized effects of the model. Results reveal that intention to use a particular device can predict actual use quite well. Ease of smartphone use is the most meaningful variable: if people use a smartphone for specific tasks, their intention to use a smartphone for survey completion is also more likely. In conclusion, investing in ease of use of mobile survey completion could encourage respondents to use mobile devices. This can foremost be established by building well-designed surveys for mobile devices.},
	pages = {517--531},
	number = {5},
	journaltitle = {International Journal of Social Research Methodology},
	author = {Haan, Marieke and Lugtig, Peter and Toepoel, Vera},
	date = {2019},
	keywords = {{ACCEPTANCE}, {TECHNOLOGY}, technology acceptance, {WEB}, mixed device, Mobile device use, {ONLINE} {SURVEYS}, panel survey},
}

@article{lee_experimental_2019,
	title = {Experimental comparison of {PC} web, smartphone web, and telephone surveys in the new technology era},
	volume = {37},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2019-13380-006%26site%3dehost-live},
	doi = {10.1177/0894439318756867},
	abstract = {Smartphones have become very popular globally, and smartphone ownership has overtaken conventional cell phone ownership in many countries in recent years. With this rapid rise in smartphone penetration, researchers are looking at ways to conduct web surveys using smartphones. This is particularly true of student populations where smartphone penetration is very high and web surveys are already the norm. However, researchers are raising concerns about selection biases and measurement differences between {PC} and smartphone respondents. Questions also remain about comparisons to traditional interviewer-administered approaches. We designed an experimental comparison between a {PC} web survey, a smartphone web survey and a computer-assisted telephone interviewing ({CATI}) survey. This study was conducted using an annual survey of students at a large university in South Korea. The {CATI} (interviewer-administered) survey had a higher response rate, lower margins of error, and better representation of the student population than the two web (self-administered) modes, but at a higher cost. The {CATI} survey also had lower rates of item nonresponse. More significant differences were found between the modes for sensitive questions than for nonsensitive ones. This suggests that {CATI} surveys may still have a role to play in surveys of college students, even in a country with high rates of mobile technology adoption. ({PsycINFO} Database Record (c) 2019 {APA}, all rights reserved)},
	pages = {234--247},
	number = {2},
	journaltitle = {Social Science Computer Review},
	author = {Lee, Hana and Kim, Sunwoong and Couper, Mick P. and Woo, Youngje},
	date = {2019},
	keywords = {Smartphones, telephone survey, coverage, Computers, Online Surveys, College Students, response rate, measurement error, completion times, Error of Measurement, item nonresponse, smartphone survey, survey costs, Telephone Surveys, Test Administration, web survey, {FACE}-{TO}-{FACE}, {SENSITIVE} {TOPICS}, {ALCOHOL}, {MODE}},
}

@article{cernat_radio_2019,
	title = {Radio buttons in web surveys: Searching for alternatives},
	volume = {61},
	doi = {10.1177/1470785318813520},
	abstract = {Web surveys are struggling to attract and retain respondents due to high burden and competition for the users(') attention. One possible solution to this issue is the improvement of the visual design of surveys. In this article, we evaluate the impact of visual aids such as smiley faces, stars, hearts, and thumbs as alternatives to traditional radio buttons. We use an experimental design in a nonprobability online survey to investigate how the new designs compare with radio buttons and how the results might interact with device used for completion ({PC} vs mobile), the use of labels, the type of response scale (bipolar vs unipolar), and the number of response categories (5 vs 7 point). While we do not find big differences in response, quality and experience, there seem to be some indication that the use of smiley faces leads to worse data quality.},
	pages = {266--286},
	number = {3},
	journaltitle = {International Journal of Market Research},
	author = {Cernat, Alexandru and Liu, Mingna},
	date = {2019},
	keywords = {experimental design, response scales, web surveys, {DATA} {QUALITY}, {VISUAL} {ANALOG} {SCALES}, gamification, mobile survey, {SURVEY} {DESIGN}, visual design},
}

@article{link_mobile_2014,
	title = {Mobile technologies for conducting, augmenting and potentially replacing surveys},
	volume = {78},
	doi = {10.1093/POQ/NFU054},
	pages = {779--787},
	number = {4},
	journaltitle = {Public Opinion Quarterly},
	author = {Link, Michael W. and Murphy, Joe and Schober, Michael F. and Buskirk, Trent D. and Hunter Childs, Jennifer and Langer Tesfaye, Casey},
	date = {2014},
}

@article{bucher_exploring_2021,
	title = {Exploring the Feasibility of Recruiting Respondents and Collecting Web Data Via Smartphone: A Case Study of Text-To-Web Recruitment for a General Population Survey in Germany},
	doi = {10.1093/JSSAM/SMAB006},
	abstract = {The widespread usage of smartphones, as well as their technical features, offers many opportunities for survey research. As a result, the importance and popularity of smartphone surveys is steadily increasing. To explore the feasibility of a new text-to-web approach for surveying people directly via their smartphones, we conducted a case study in Germany in which we recruited respondents from a mobile random digit dialing sample via text messages that included a link to a web survey. We show that, although this survey approach is feasible, it is hampered by a number of issues, namely a high loss of numbers at the invitation stage, and a high rate of implicit refusals on the landing page of the survey.},
	journaltitle = {Journal of Survey Statistics and Methodology},
	author = {Bucher, Hannah and Sand, Matthias},
	date = {2021},
}

@article{revilla_open_2016,
	title = {Open narrative questions in {PC} and smartphones: is the device playing a role?},
	volume = {50},
	url = {https://www.proquest.com/scholarly-journals/open-narrative-questions-pc-smartphones-is-device/docview/1827074964/se-2?accountid=14570},
	doi = {10.1007/s11135-015-0273-2},
	abstract = {Most survey questions are closed questions, where respondents have to select an answer from a proposed set of alternatives. However, a lot of surveys also include, at least occasionally, some open questions. Open questions that call for elaborated and developed answers, called "open narrative questions", are used when the researchers want to go deeper into what the respondents think. This paper compares the answers to open narrative questions when the respondent is participating in a {PC} survey, in a smartphone-not-optimised survey or in a smartphone-optimised survey. The experiment was carried out in Spain using data collected by the Netquest online access panel. Respondents were assigned randomly to each type of device and survey format, in two successive waves. Because respondents have to type in their answer, we expect differences between devices, linked with the size and the kind of keyboards (i.e. physical versus digital, touch-screen or not). Differences are observed between answers that come from {PCs} and smartphones for the response time per written character, for the number of total characters and for the use of abbreviations, but not for the non-answer and non-substantive responses. No differences are observed between optimised and not optimised versions for smartphones, except for the response time per character written.},
	pages = {2495--2513},
	number = {6},
	journaltitle = {Quality \& Quantity},
	author = {Revilla, Melanie and Ochoa, Carlos},
	date = {2016},
	keywords = {Data collection, Statistics, Studies, Internet, Smartphones, Telephone communications, Polls \& surveys, Research responses, Questionnaires, Hypotheses, {DESIGN}, 33411:Computer and Peripheral Equipment Manufacturing, Spain, {COMPUTER}, Experiments, Response time, {RESPONSES}, Personal computers, Web surveys, Narratives, Keyboards, Mobile optimised questionnaires, {MOBILE} {WEB} {SURVEYS}, Open narrative questions, {OPEN}-{ENDED} {QUESTIONS}, {SIZE}, 0104:methodology and research technology, Analysis, Interactive computer systems, Israel, research methods/tools},
}

@article{antoun_simultaneous_2019,
	title = {Simultaneous estimation of multiple sources of error in a smartphone-based survey},
	volume = {7},
	doi = {10.1093/JSSAM/SMY002},
	abstract = {Although web surveys in which respondents are encouraged to use smartphones have started to emerge, it is still unclear whether they are a promising alternative to traditional web surveys in which most respondents use desktop computers. For sample members to participate in smartphone-based surveys, they need to have access to a smartphone and agree to use it to complete the survey; this raises concerns about coverage and nonresponse, as well as measurement if those who agree to participate have any difficulty using smartphones. In an analysis of data from a smartphone versus desktop (within-subjects) experiment conducted in a probability-based web panel, we compare estimates produced by the smartphone web survey (one condition) and {PC} web survey (other condition). We estimate mode effects and then examine the extent to which these effects are attributable to coverage, nonresponse, and measurement errors in the smartphone-based survey. While mode effects were generally small, we find that the smartphone web survey produced biased estimates relative to {PC} web for a subset of survey variables. This was largely due to noncoverage and, to a lesser extent, nonresponse. We find no evidence of measurement effects. Our findings point to the trade-off of the advanced data collection opportunities of smartphones and the potential selection errors that such devices may introduce.},
	pages = {93--117},
	number = {1},
	journaltitle = {Journal of Survey Statistics and Methodology},
	author = {Antoun, Christopher and Conrad, Frederick G. and Couper, Mick P. and West, Brady T.},
	date = {2019},
	keywords = {{QUALITY}, {BIAS}, {PC} {WEB}, {COMPUTER}, {SENSITIVE} {TOPICS}, {SELECTION}, {MOBILE} {WEB} {SURVEYS}, Coverage error, Measurement error, Mobile web survey, Nonresponse error, Total survey error},
}

@article{antoun_effects_2017,
	title = {{EFFECTS} {OF} {MOBILE} {VERSUS} {PC} {WEB} {ON} {SURVEY} {RESPONSE} {QUALITY}: A {CROSSOVER} {EXPERIMENT} {IN} A {PROBABILITY} {WEB} {PANEL}.},
	volume = {81},
	abstract = {Survey participants are increasingly responding to Web surveys on their smartphones as opposed to their personal computers ({PCs}), and this change brings with it some potential data-quality issues. This study reports on a randomized crossover experiment to compare the effect of two different devices, smartphones and {PCs}, on response quality in a Web survey conducted in a probability-based panel. Participants (n = 1,390) were invited to complete an online questionnaire on both a smartphone (mobile Web) and {PC} ({PC} Web) in sequence. We hypothesized that smartphone use would result in lower-quality responses because people are more likely to use smartphones while multitasking or while around other people and because they could have difficulty recording their answers using a small touchscreen. While we found that respondents who participated in this study were more likely to multitask and more likely to be around other people when using smartphones, these factors had little impact on data quality. Respondents were at least as likely to provide conscientious and thoughtful answers and to disclose sensitive information on smartphones as on {PCs}. When using smartphones, however, respondents seemed to have trouble accurately moving a small-sized slider handle and a date-picker wheel to the intended values. Overall, we find that people using smartphones can provide high-quality responses, even when their context is more distracting, as long as they are presented with question formats that are easy to use on small touchscreens. [{ABSTRACT} {FROM} {AUTHOR}]},
	pages = {280--306},
	issue = {S1},
	journaltitle = {Public Opinion Quarterly},
	author = {Antoun, Christopher and Couper, Mick P. and Conrad, Frederick G.},
	date = {2017},
	keywords = {Respondents, Smartphones, Data quality, {INTERNET}, {COMPUTER}, Personal computers, Internet surveys, Probability theory},
}

@article{trubner_effects_2020,
	title = {Effects of Header Images on Different Devices in Web Surveys},
	volume = {14},
	doi = {10.18148/srm/2020.v14i1.7367},
	abstract = {Header images are typically included in web surveys to make surveys more appealing for respondents. However, headers might also induce a systematic bias in response behavior. In order to examine both the potential effects (more specifically, effects on motivation and context effects) of header images with respondents using different devices, an experiment embedded in a web survey on students' time use and stress was conducted using a probability sample of 1,326 students at the University of Bonn. Respondents were presented either with a picture of an auditorium with students sitting in a class, a picture of leisure activities on campus, or no picture, respectively. To control for position effects, pictures were placed either in the upper right or upper left of the questionnaire. The results indicate that header images attract attention in the beginning of a survey, but do not significantly increase motivation over the course of the survey. When faced with a header picture, respondents in the picture conditions evaluate their time in class differently compared to respondents in the control group. While the device providing the visibility makes no difference, effects are only significant when the picture is placed on the left side of the screen. In sum, the interaction of header placement and the content-related proximity of header content and question may alter response behavior.},
	pages = {43--53},
	number = {1},
	journaltitle = {Survey Research Methods},
	author = {Trübner, Miriam},
	date = {2020},
	keywords = {{EXPOSURE}, web surveys, Context effects, device effects, header images, {SURVEY} {PARTICIPATION}},
}

@article{hohne_motion_2020,
	title = {Motion instructions in surveys: Compliance, acceleration, and response quality},
	volume = {62},
	doi = {10.1177/1470785319858587},
	abstract = {The increased use of smartphones in web survey responding did not only raise new research questions but also fostered new ways to research survey completion behavior. Smartphones have many built-in sensors, such as accelerometers that measure acceleration (i.e., the rate of change of velocity of an object over time). Sensor data establish new research opportunities by providing information about physical completion conditions that, for instance, can affect response quality. In this study, we explore three research questions: (1) To what extent do respondents accept to comply with motion instructions? (2) What variables affect the acceleration of smartphones? (3) Do different motion levels affect response quality? We conducted a smartphone web survey experiment using the Netquest opt-in panel in Spain and asked respondents to stand at a fix point or walk around while answering five single questions. The results reveal high compliance with motion instructions, with compliance being higher in the standing than in the walking condition. We also discovered that several variables, such as the presence of third parties, increase the acceleration of smartphones. However, the quality of responses to the five single questions did not differ significantly between the motion conditions, a finding that is in line with previous research. Our findings provide new insights into how compliance changes with motion tasks and suggest that the collection of acceleration data is a feasible and fruitful way to explore survey completion behavior. The findings also indicate that refined research on the connection between motion levels and response quality is necessary.},
	pages = {43--57},
	number = {1},
	journaltitle = {International Journal of Market Research},
	author = {Höhne, Jan K. and Revilla, Melanie and Schlosser, Stephan},
	date = {2020},
	keywords = {smartphones, response quality, web survey, {QUESTIONS}, {MOBILE} {DEVICES}, {PC}, accelerometer, compliance, {FORMATS}, survey completion behavior, {SurveyMotion}, {TIMES}, {WEB} {SURVEYS}},
}

@article{mavletova_device_2016,
	title = {Device use in web surveys: The effect of differential incentives},
	volume = {58},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2017-05795-002%26site%3dehost-live},
	doi = {10.2501/IJMR-2016-034},
	abstract = {This study should be considered as a preliminary exploration of the effect of differential incentives on participation rates, the proportion of mobile respondents, and sample composition in web surveys. The experiment has some limitations, which should be taken into consideration in subsequent studies. First, the experiment is based on a sample of frequent mobile web users, and the results could be different from a sample of those who use mobile internet less frequently. Second, the experiment is based on a volunteer online access panel and the results could be different in a representative online panel. Third, we tested the differential incentives starting with incentives 50\% higher than typical incentives. We suggest that it is worth exploring the effect of other incentives (e.g. 20\% or 30\% higher). Finally, we suggest that it is worth exploring the difference in participation rates between the conditions in which higher-than-typical incentives would be offered for all participants and when offered only for using a particular device. ({PsycINFO} Database Record (c) 2018 {APA}, all rights reserved)},
	pages = {523--544},
	number = {4},
	journaltitle = {International Journal of Market Research},
	author = {Mavletova, Aigul and Couper, Mick P.},
	date = {2016},
	keywords = {Marketing, Mobile Devices, Incentives, Internet, mobile devices, Surveys, market research, web surveys},
}

@article{revilla_testing_2018,
	title = {Testing different rank order question layouts for {PC} and smartphone respondents},
	volume = {21},
	url = {https://www.proquest.com/scholarly-journals/testing-different-rank-order-question-layouts-pc/docview/2113030466/se-2?accountid=14570},
	doi = {10.1080/13645579.2018.1471371},
	abstract = {We studied the impact of different layouts for rank order questions on respondent effort, data quality, and substantive results among {PC} and smartphone respondents, in an experiment in an opt-in online panel in Spain, using an order-by-click design. We experimentally varied the device, the number of columns, and, for smartphone respondents, the position of the 'next' button in questions on trust in institutions.We found some evidence of lower data quality for smartphone users but no evidence that presenting ranking items in one column performs differently than two columns. We also find little evidence that these effects differ by the number of response options presented or the number to be ranked. The placement of the 'next' button had little effect on performance on ranking items. Overall, our findings suggest that the format and layout of order-by-click questions has little effect on data quality, regardless of device used.},
	pages = {695--712},
	number = {6},
	journaltitle = {International Journal of Social Research Methodology},
	author = {Revilla, Melanie and Couper, Mick P.},
	date = {2018},
	keywords = {Internet, Respondents, Smartphones, data quality, Data quality, Layout, {EXPERIENCE}, Social Sciences: Comprehensive Works, {VALUES}, {PANELS}, {SENSITIVE} {TOPICS}, {WEB} {SURVEYS}, {PARADATA}, Web surveys, order-by-click questions, rank order questions, Ranking, smartphone optimization, {RATINGS}},
}

@article{mason_effect_2019,
	title = {The effect of format and device on the performance and usability of web-based questionnaires},
	volume = {22},
	url = {https://www.proquest.com/scholarly-journals/effect-format-device-on-performance-usability-web/docview/2196500543/se-2?accountid=14570},
	doi = {10.1080/13645579.2018.1542150},
	abstract = {This article explores the comparability of assessment tools under different format conditions. Prior studies have not considered the interaction of format and device on time to complete an assessment and have instead treated each of them separately with conflicting results. This study assesses, by linear regressions using web-based data, the performance of multiple devices under varying formats while controlling for non-device factors such as demographic information. The results of this study add to the growing literature on the equivalence among devices and formats used to collect and interpret performance in a variety of organizational settings.},
	pages = {271--280},
	number = {3},
	journaltitle = {International Journal of Social Research Methodology},
	author = {Mason, Robert and Huff, Kyle C.},
	date = {2019},
	keywords = {Internet, Evaluation, Questionnaires, questionnaire, survey, usability, {DESIGN}, {MOBILE}, Social Sciences: Comprehensive Works, Demographic aspects, Computer based, {TESTS}, {ABILITY}, {COMPUTERS}, {EQUIVALENCE}, format, Mobile, {PAPER}, web, Organizational effectiveness},
}

@article{ha_data_2020,
	title = {Data quality comparison between computers and smartphones in different web survey modes and question formats},
	volume = {30},
	doi = {10.1108/INTR-09-2018-0417},
	abstract = {Purpose Low response rates in web surveys and the use of different devices in entering web survey responses are the two main challenges to response quality of web surveys. The purpose of this study is to compare the effects of using interviewers to recruit participants in computer-assisted self-administered interviews ({CASI}) vs computer-assisted personal interviews ({CAPI}) and smartphones vs computers on participation rate and web survey response quality. Design/methodology/approach Two field experiments using two similar media use studies on {US} college students were conducted to compare response quality in different survey modes and response devices. Findings Response quality of computer entry was better than smartphone entry in both studies for open-ended and closed-ended question formats. Device effect was only significant on overall completion rate when interviewers were present. Practical implications Survey researchers are given guidance how to conduct online surveys using different devices and choice of question format to maximize survey response quality. The benefits and limitations of using an interviewer to recruit participants and smartphones as web survey response devices are discussed. Social implications It shows how computer-assisted self-interviews and smartphones can improve response quality and participation for underprivileged groups. Originality/value This is the first study to compare response quality in different question formats between {CASI}, e-mailed delivered online surveys and {CAPI}. It demonstrates the importance of human factor in creating sense of obligation to improve response quality.},
	pages = {1763--1781},
	number = {6},
	journaltitle = {Internet Research},
	author = {Ha, , Louisa and Zhang, Chenjie and Jiang, Weiwei},
	date = {2020},
	keywords = {Smartphone, Data quality, {DESIGN}, {FACE}-{TO}-{FACE}, {MAIL}, {INDICATORS}, {ONLINE} {SURVEYS}, {NONRESPONSE}, Computer-assisted personal Interview, Computer-assisted self-interview, Interviewer, Survey mode},
}

@article{hohne_surveymotion_2019,
	title = {{SurveyMotion}: what can we learn from sensor data about respondents' completion and response behavior in mobile web surveys?},
	volume = {22},
	url = {https://www.proquest.com/scholarly-journals/surveymotion-what-can-we-learn-sensor-data-about/docview/2220295195/se-2?accountid=14570},
	doi = {10.1080/13645579.2018.1550279},
	abstract = {Participation in web surveys via smartphones increased continuously in recent years. The reasons for this increase are a growing proportion of smartphone owners and an increase in mobile Internet access. However, research has shown that smartphone respondents are frequently distracted and/or multitasking, which might affect completion and response behavior in a negative way. We propose '{SurveyMotion} ({SMotion})', a {JavaScript}-based tool for mobile devices that can gather information about respondents' motions during web survey completion by using sensor data. Specifically, we collect data about the total acceleration ({TA}) of smartphones. We conducted a lab experiment and varied the form of survey completion (e.g. standing or walking). Furthermore, we employed questions with different response formats (e.g. radio buttons and sliders) and measured response times. The results reveal that {SMotion} detects higher {TAs} of smartphones for respondents with comparatively higher motion levels. In addition, respondents' motion level affects response times and the quality of responses given. The {SMotion} tool promotes the exploration of how respondents complete mobile web surveys and could be employed to understand how future mobile web surveys are completed.},
	pages = {379--391},
	number = {4},
	journaltitle = {International Journal of Social Research Methodology},
	author = {Höhne, Jan K. and Schlosser, Stephan},
	date = {2019},
	keywords = {Respondents, Smartphones, Behavior, smartphones, Telephone communications, Polls \& surveys, Internet access, {QUALITY}, Walking, Participation, Social Sciences: Comprehensive Works, {MULTITASKING}, response quality, web survey, Reaction time, {FORMATS}, Acceleration, {JavaScript}, mobile sensors, passive data collection, {GRIDS}, {PARADATA}, {PCS}, Completion, Owners, Radio},
}

@article{weigold_computerized_2021,
	title = {Computerized Device Equivalence: A Comparison of Surveys Completed Using A Smartphone, Tablet, Desktop Computer, and Paper-and-Pencil},
	volume = {37},
	doi = {10.1080/10447318.2020.1848159},
	abstract = {There is a limited body of experimental research examining the comparability of completing self-report surveys using different computerized devices. Additionally, available literature has not used complete or optimal procedures for determining device equivalence. The current study examined the comparability of surveys completed using paper-and-pencil and three popular devices: smartphone, tablet, and desktop computer. Participants consisted of 211 college students randomly assigned to conditions who completed measures of personality, social desirability, and computer self-efficacy. Results showed evidence of qualitative equivalence (internal consistency and subscale intercorrelations) across conditions. For quantitative and auxiliary equivalence, both equivalence testing and Bayesian analyses were conducted. Equivalence testing indicated quantitative (mean score) equivalence, as well as comparability for one aspect of auxiliary equivalence (missing data). Other aspects of auxiliary equivalence (completion time and comfort completing questionnaires) suggested potentially meaningful differences. Bayesian analyses typically replicated these results, with some notable exceptions regarding auxiliary equivalence.},
	pages = {803--814},
	number = {8},
	journaltitle = {International Journal of Human-Computer Interaction},
	author = {Weigold, Arne and Weigold, Ingrid K. and Dykema, Stephanie A. and Drakeford, Naomi M. and Martin-Wagar, Caitlin A.},
	date = {2021},
	keywords = {{VALIDATION}, {QUALITY}, {TECHNOLOGY}, {INTERNET}, {MOBILE}, {SELF}-{EFFICACY}, {WEB}, {SOCIAL} {DESIRABILITY}, {SHORT} {FORMS}},
}

@article{mavletova_grouping_2016,
	title = {Grouping of items in mobile web questionnaires},
	volume = {28},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpsyh%26AN%3d2016-17809-005%26site%3dehost-live},
	doi = {10.1177/1525822X15595151},
	abstract = {There is some evidence that a scrolling design may reduce breakoffs in mobile web surveys compared to a paging design, but there is little empirical evidence to guide the choice of the optimal number of items per page. We investigate the effect of the number of items presented on a page on data quality in two types of questionnaires: with or without user-controlled skips. Three versions of a 30-item instrument were compared, with 5, 15, or all 30 questions presented on a page, in two different surveys, one with skips and one without. We found that displaying 30 items on a page reduced the breakoff rate by almost one-third compared to presenting five items per page in the questionnaire without skips, but the difference was not statistically significant. In both surveys with and without skips, the completion times were significantly lower in the 30-item per page condition; however, item nonresponse rates were also higher. We give some practical recommendations to guide choices while designing questionnaires for mobile web surveys. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {170--193},
	number = {2},
	journaltitle = {Field methods},
	author = {Mavletova, Aigul and Couper, Mick P.},
	date = {2016},
	keywords = {Internet, Data quality, Polls \& surveys, Questionnaires, Surveys, mobile web surveys, {SMARTPHONE}, {DESIGN}, Anthropology, breakoff rates, {COMPUTER}, Item Analysis (Test), Online Experiments, {PC}, {DEVICES}, Test Items, 0104:methodology and research technology, research methods/tools, social anthropology, Choices, scrolling, skips, 0514:culture and social structure},
}

@article{grady_what_2019,
	title = {What Is the Best Size for Matrix-Style Questions in Online Surveys?},
	volume = {37},
	url = {https://doi.org/10.1177/0894439318773733},
	doi = {10.1177/0894439318773733},
	abstract = {Across two studies, we aimed to determine the row and column size in matrix-style questions that best optimizes participant experience and data quality for computer and mobile users. In Study 1 (N = 2,492), respondents completed 20 questions (comprising four short scales) presented in a matrix grid (converted to item-by-item format on mobile phones). We varied the number of rows (5, 10, or 20) and columns (3, 5, or 7) of the matrix on each page. Outcomes included both data quality (straightlining, item skip rate, and internal reliability of scales) and survey experience measures (dropout rate, rating of survey experience, and completion time). Results for row size revealed dropout rate and reported survey difficulty increased as row size increased. For column size, seven columns increased the completion time of the survey, while three columns produced lower scale reliability. There was no interaction between row and column size. The best overall size tested was a 5 ? 5 matrix. In Study 2 (N = 2,570), we tested whether the effects of row size replicated when using a single 20-item scale that crossed page breaks and found that participant survey ratings were still best in the five-row condition. These results suggest that having around five rows or potentially fewer per page, and around five columns for answer options, gives the optimal survey experience, with equal or better data quality, when using matrix-style questions in an online survey. These recommendations will help researchers gain the benefits of using matrices in their surveys with the least downsides of the format.},
	pages = {435--445},
	number = {3},
	journaltitle = {Social Science Computer Review},
	author = {Grady, Rebecca H. and Greenspan, Rachel L. and Liu, Mingna},
	date = {2019},
	keywords = {Reliability, Internet, Education--Computer Applications, data quality, Data quality, Polls \& surveys, Mobile computing, {DESIGN}, web survey, {RESPONSES}, {WEB}, Completion time, {GRIDS}, matrix format, matrix size, {NUMBER}, Balances (scales), Format, Matrices},
}

@article{antoun_factors_2020,
	title = {Factors Affecting Completion Times: A Comparative Analysis of Smartphone and {PC} Web Surveys},
	volume = {38},
	doi = {10.1177/0894439318823703},
	abstract = {This article compares the factors affecting completion times ({CTs}) to web survey questions when they are answered using two different devices: personal computers ({PCs}) and smartphones. Several studies have reported longer {CTs} when respondents use smartphones than {PCs}. This is a concern to survey researchers because longer {CTs} may increase respondent burden and the risk of breakoff. However, few studies have analyzed the specific reasons for the time difference. In this analysis, we analyzed timing data from 836 respondents who completed the same web survey twice, once using a smartphone and once using {PC}, as part of a randomized crossover experiment in the Longitudinal Internet Studies for the Social Sciences panel. The survey contained a mix of questions (single choice, numeric entry, and text entry) that were displayed on separate pages. We included both page-level and respondent-level factors that may have contributed to the time difference between devices in cross-classified multilevel models. We found that respondents took about 1.4 times longer when using smartphones than {PCs}. This difference was larger when a page had more than one question or required text entry. The difference was also larger among respondents who had relatively low levels of familiarity and experience using smartphones. Respondent multitasking was associated with slower {CTs}, regardless of the device used. Practical implications and avenues for future research are discussed.},
	pages = {477--489},
	number = {4},
	journaltitle = {Social Science Computer Review},
	author = {Antoun, Christopher and Cernat, Alexandru},
	date = {2020},
	keywords = {Smartphones, mobile web surveys, Computers, Online Surveys, {MOBILE}, response times, smartphone surveys, Reaction Time, Experimental Subjects, Testing Methods},
}

@article{revilla_experiment_2017,
	title = {An experiment comparing grids and item-by-item formats in web surveys completed through {PCs} and smartphones.},
	volume = {34},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dufh%26AN%3d117440989%26site%3dehost-live},
	abstract = {Some respondents already complete web surveys via mobile devices. These devices vary at several levels from {PCs}. In particular, we expect differences when grid questions are used due to the lower visibility on mobile devices and because in questionnaires optimized to be completed through smartphones, grids are split up into an item-by-item format. This paper reports the results of a two-wave experiment conducted in Spain in 2015, comparing three groups: {PCs}, smartphones not-optimized, or smartphones optimized. We found similar levels of interitem correlations, longer completion times for grid questions for smartphone respondents, and sometimes less non-differentiation for {PCs}. Thus, using the item-by-item format for smartphones and {PCs} seems the most appropriate way to improve comparability. [{ABSTRACT} {FROM} {AUTHOR}]},
	pages = {30--42},
	number = {1},
	journaltitle = {Telematics \& Informatics},
	author = {Revilla, Melanie and Toninelli, Daniele and Ochoa, Carlos},
	date = {2017},
	keywords = {Smartphones, Mobile communication systems, Completion time, Personal computers, Web surveys, Comparative studies, Grids, Interitem correlation, Non-differentiation, Internet surveys, Grid computing},
}

@article{bosch_using_2021,
	title = {Using emojis in mobile web surveys for Millennials? A study in Spain and Mexico},
	volume = {55},
	url = {https://www.proquest.com/scholarly-journals/using-emojis-mobile-web-surveys-millennials-study/docview/2486883703/se-2},
	doi = {10.1007/s11135-020-00994-8},
	abstract = {To involve Millennials in survey participation, and obtain high-quality answers from them, survey designers may require new tools that better catch Millennials' interest and attention. One key new tool that could improve the communication and make the survey participation more attractive to young respondents are the emojis. We used data from a survey conducted among Millennials by the online fieldwork company Netquest in Spain and Mexico (n = 1614) to determine how emojis can be used in mobile web surveys, in particular in open-ended questions, and how their use can affect data quality, completion time, and survey evaluation. Overall, results show a high willingness of Millennials to use emojis in surveys (both stated and actual use) and a positive impact of encouraging Millennials to use emojis in open-ended questions on the amount of information conveyed, the completion time and the survey enjoyment.},
	pages = {39--61},
	journaltitle = {Quality \& Quantity},
	author = {Bosch, Oriol J. and Revilla, Melanie},
	date = {2021},
	keywords = {Statistics, Emojis, Internet, Data quality, Polls \& surveys, Participation, Millennials, Mexico, Mobile web surveys, Spain, Survey evaluation},
}

@article{keusch_coverage_2020,
	title = {Coverage Error in Data Collection Combining Mobile Surveys With Passive Measurement Using Apps: Data From a German National Survey},
	url = {https://doi.org/10.1177/0049124120914924},
	doi = {10.1177/0049124120914924},
	abstract = {Researchers are combining self-reports from mobile surveys with passive data collection using sensors and apps on smartphones increasingly more often. While smartphones are commonly used in some groups of individuals, smartphone penetration is significantly lower in other groups. In addition, different operating systems ({OSs}) limit how mobile data can be collected passively. These limitations cause concern about coverage error in studies targeting the general population. Based on data from the Panel Study Labour Market and Social Security ({PASS}), an annual probability-based mixed-mode survey on the labor market and poverty in Germany, we find that smartphone ownership and ownership of smartphones with specific {OSs} are correlated with a number of sociodemographic and substantive variables. The use of weighting techniques based on sociodemographic information available for both owners and nonowners reduces these differences but does not eliminate them.},
	journaltitle = {Sociological Methods \& Research},
	author = {Keusch, Florian and Bähr, Sebastian and Haas, Georg-Christoph and Kreuter, Frauke and Trappmann, Mark},
	date = {2020},
	keywords = {smartphones, mobile web surveys, passive mobile data collection, coverage error, operating systems},
}

@article{tourangeau_web_2018,
	title = {Web Surveys by Smartphones and Tablets: Effects on Data Quality},
	volume = {36},
	url = {https://doi.org/10.1177/0894439317719438},
	doi = {10.1177/0894439317719438},
	abstract = {Does completing a web survey on a smartphone or tablet computer reduce the quality of the data obtained compared to completing the survey on a laptop computer? This is an important question, since a growing proportion of web surveys are done on smartphones and tablets. Several earlier studies have attempted to gauge the effects of the switch from personal computers to mobile devices on data quality. We carried out a field experiment in eight counties around the United States that compared responses obtained by smartphones, tablets, and laptop computers. We examined a range of data quality measures including completion times, rates of missing data, straightlining, and the reliability and validity of scale responses. A unique feature of our study design is that it minimized selection effects; we provided the randomly determined device on which respondents completed the survey after they agreed to take part. As a result, respondents may have been using a device (e.g., a smartphone) for the first time. However, like many of the prior studies examining mobile devices, we find few effects of the type of device on data quality.},
	pages = {542--556},
	number = {5},
	journaltitle = {Social Science Computer Review},
	author = {Tourangeau, Roger and Sun, Hanyu and Yan, Ting and Maitland, Aaron and Rivero, Gonzalo and Williams, Douglas},
	date = {2018},
	keywords = {Internet, Smartphones, Education--Computer Applications, smartphones, Telephone communications, data quality, Data quality, Electronic devices, Polls \& surveys, Research responses, mobile devices, Mobile computing, {DESIGN}, Tablet computers, measurement error, {COMPUTER}, {SENSITIVE} {TOPICS}, {MOBILE} {DEVICES}, {PC}, Personal computers, Missing data, Rangefinding},
}

@article{revilla_are_2017,
	title = {Are There Differences Depending on the Device Used to Complete a Web Survey ({PC} or Smartphone) for Order-by-click Questions?},
	volume = {29},
	url = {https://doi.org/10.1177/1525822X16674701},
	doi = {10.1177/1525822X16674701},
	abstract = {The development of web surveys has been accompanied by the emergence of new scales, taking advantages of the visual and interactive features provided by the Internet like drop-down menus, sliders, drag-and-drop, or order-by-click scales. This article focuses on the order-by-click scales, studying the comparability of the data obtained for this scale when answered through {PCs} versus smartphones. I used data from an experiment where panelists from the Netquest opt-in panel in Spain were randomly assigned to a {PC}, smartphone optimized, or smartphone not-optimized version of the same questionnaire in two waves. I found significant differences due to the device and optimization at least for some indicators and questions.},
	pages = {266--280},
	number = {3},
	journaltitle = {Field methods},
	author = {Revilla, Melanie},
	date = {2017},
	keywords = {Internet, Smartphones, smartphones, Polls \& surveys, Research responses, Questionnaires, Mobile Phones, Surveys, Websites, {MOBILE}, web surveys, Anthropology, Microcomputers, {COMPUTER}, {PANELS}, {PC}, social anthropology, 0514:culture and social structure},
}

@article{couper_why_2017,
	title = {Why Do Web Surveys Take Longer on Smartphones?},
	volume = {35},
	url = {https://doi.org/10.1177/0894439316629932},
	doi = {10.1177/0894439316629932},
	abstract = {Surveys completed on mobile web devices (smartphones) have been found to take longer than surveys completed on a {PC}. This has been found both in surveys where respondents can choose which device they use and in surveys where respondents are randomly assigned to devices. A number of potential explanations have been offered for these findings, including (1) slower transmission over cellular or Wi-Fi networks, (2) the difficulty of reading questions and selecting responses on a small device, and (3) the increased mobility of mobile web users who have more distractions while answering web surveys. In a secondary analysis of student surveys, we find that only about one-fifth of the time difference can be accounted for by transmission time (between-page time) with the balance being within-page time differences. Using multilevel models, we explore possible page-level (question-level) and respondent-level factors that may contribute to the time difference. We find that much of the time difference can be accounted for by the additional scrolling required on mobile devices, especially for grid questions.},
	pages = {357--377},
	number = {3},
	journaltitle = {Social Science Computer Review},
	author = {Couper, Mick P. and Peterson, Gregg J.},
	date = {2017},
	keywords = {Data collection, Internet, Respondents, Smartphones, Education--Computer Applications, Electronic devices, Polls \& surveys, Web sites, {QUALITY}, {AGE}, {MOBILE}, web surveys, Computer software, {COMPUTER}, online surveys, smartphone surveys, Response time, {PC}, Time, {PARADATA}, {ONLINE} {SURVEYS}, survey completion times, {RESPONSE}-{TIMES}, 83:Social and Behavioral Sciences ({CI}), 0188:methodology and research technology, computer methods, media, \& applications, Cellular communication, Scrolling},
}

@article{gummer_explaining_2015,
	title = {Explaining Interview Duration in Web Surveys: A Multilevel Approach},
	volume = {33},
	url = {https://doi.org/10.1177/0894439314533479},
	doi = {10.1177/0894439314533479},
	abstract = {Interview duration is an important variable in web surveys because it is a direct measure of the response burden. In this article, we analyze the effects of the survey design, respondent characteristics, and the interaction between these effects on interview duration. For that purpose, we applied multilevel analysis to a data set of 21 web surveys on political attitudes and behavior. Our results showed that factors on both levels, the individual and the survey level, had effects on interview duration. However, the larger share of the variation in interview duration is explained by the characteristics of the respondents. In this respect, we illustrate the impact of mobile devices and panel recruitment on interview duration. In addition, we found important relationships between the respondents? attitudes and how a web survey is designed: Highly motivated respondents spent significantly more time answering cognitively demanding questions than less motivated respondents. When planning a survey, not only the number and formats of questions need to be taken into account but also the expected sample composition and how the participants will respond to the design of the web survey.},
	pages = {217--234},
	number = {2},
	journaltitle = {Social Science Computer Review},
	author = {Gummer, Tobias and Roßmann, Joss},
	date = {2015},
	keywords = {Data analysis, Web services, Research methodology, Internet, Education--Computer Applications, mobile devices, Mobile Phones, Interviews, Political Attitudes, {AGE}, {EXPERIENCE}, {DESIGN}, interview duration, multilevel analysis, online panels, web survey design, {DATA} {QUALITY}, {INDICATORS}, {LENGTH}, {RESPONSE}-{TIMES}, Political behavior, {BURDEN}, {ORDER}, {SURVEY} {QUESTIONS}, Attitude surveys},
}

@article{mendelson_displaying_2017,
	title = {Displaying Videos in Web Surveys: Implications for Complete Viewing and Survey Responses},
	volume = {35},
	url = {https://doi.org/10.1177/0894439316662439},
	doi = {10.1177/0894439316662439},
	abstract = {Videos are often used in web surveys to assess attitudes. While including videos may allow researchers to test immediate reactions, there may be issues associated with displaying videos that are overlooked. In this article, we examine the effects of using video stimuli on responses in a probability-based web survey. Specifically, we evaluate the association between demographics, mobile device usage, and the ability to view videos; differences in ad recall based on whether respondents saw a video or still images of the video; whether respondents? complete viewing of videos is related to presentation order; and the data quality of follow-up questions to the videos as a function of presentation order and complete viewing. Overall, we found that respondents using mobile browsers were less likely to be able to view videos in the survey. Those who could view videos were more likely to indicate recall compared to those who viewed images, and videos that were shown later in the survey were viewed in their entirety less frequently than those shown earlier. These results directly pertain to the legitimacy of using videos in web surveys to gather data about attitudes.},
	pages = {654--665},
	number = {5},
	journaltitle = {Social Science Computer Review},
	author = {Mendelson, Jonathan and Gibson, Jennifer L. and Romano-Bergstrom, Jennifer},
	date = {2017},
	keywords = {Mobile Devices, data quality, Test Construction, Surveys, web surveys, advertising, Attitude Measures, Digital Video, measurement, Video Display Units, videos},
}

@article{bosch_answering_2019,
	title = {Answering Mobile Surveys With Images: An Exploration Using a Computer Vision {API}},
	volume = {37},
	url = {https://doi.org/10.1177/0894439318791515},
	doi = {10.1177/0894439318791515},
	abstract = {Most mobile devices nowadays have a camera. Besides, posting and sharing images have been found as one of the most frequent and engaging Internet activities. However, to our knowledge, no research has explored the feasibility of asking respondents of online surveys to upload images to answer survey questions. The main goal of this article is to investigate the viability of asking respondents of an online opt-in panel to upload during a mobile web survey: First, a photo taken in the moment, and second, an image already saved on their smartphone. In addition, we want to test to what extent the Google Vision application programming interface ({API}), which can label images into categories, produces similar tags than a human coder. Overall, results from a survey conducted among millennials in Spain and Mexico (N = 1,614) show that more than half of the respondents uploaded an image. Of those, 77.3\% and 83.4\%, respectively, complied with what the question asked. Moreover, respectively, 52.4\% and 65.0\% of the images were similarly codified by the Google Vision {API} and the human coder. In addition, the {API} codified 1,818 images in less than 5 min, whereas the human coder spent nearly 35 hours to complete the same task.},
	pages = {669--683},
	number = {5},
	journaltitle = {Social Science Computer Review},
	author = {Bosch, Oriol J. and Revilla, Melanie and Paura, Ezequiel},
	date = {2019},
	keywords = {Mobile Devices, computer vision, Cameras, Computer vision, Internet, Smartphones, Education--Computer Applications, Electronic devices, mobile web survey, Polls \& surveys, Questions, {API}, Computers, image recognition, new data types, Online Surveys, smartphone, {COMMUNICATION}, Millennials, {MILLENNIALS}, {PHOTO}, Photography, Application programming interface, Codification, Images, Viability},
}

@article{bosch_measurement_2019,
	title = {Measurement Reliability, Validity, and Quality of Slider Versus Radio Button Scales in an Online Probability-Based Panel in Norway},
	volume = {37},
	url = {https://doi.org/10.1177/0894439317750089},
	doi = {10.1177/0894439317750089},
	abstract = {Little is known about the reliability and validity in web surveys, although this is crucial information to evaluate how accurate the results might be and/or to correct for measurement errors. In particular, there are few studies based on probability-based samples for web surveys, looking at web-specific response scales and considering the impact of having smartphone respondents. In this article, we start filling these gaps by estimating the measurement quality of sliders compared to radio button scales controlling for the device respondents used. We conducted therefore two multitrait?multimethod ({MTMM}) experiments in the Norwegian Citizen Panel ({NCP}), a probability-based online panel. Overall, we find that if smartphone respondents represent a nonnegligible part of the whole sample, offering the response options in form of a slider or a radio button scale leads to a quite similar measurement quality. This means that sliders could be used more often without harming the data quality. Besides, if there are no smartphone respondents, we find that sliders can also be used, but that the marker should be placed initially in the middle rather than on the left side. However, in practice, there is no need to shift from radio buttons to sliders since the quality is not highly improved by providing sliders.},
	pages = {119--132},
	number = {1},
	journaltitle = {Social Science Computer Review},
	author = {Bosch, Oriol J. and Revilla, Melanie and {DeCastellarnau}, Anna and Weber, Wiebke},
	date = {2019},
	keywords = {Measurement, Reliability, Quality, Methodology, Measurement errors, Internet, Smartphones, Education--Computer Applications, Data quality, Polls \& surveys, Test Reliability, Test Validity, Online Surveys, Test Forms, web surveys, {COMPUTER}, {MODELS}, {WEB} {SURVEYS}, {VISUAL} {ANALOG} {SCALES}, Radio, Scales, probability-based online panel, Buttons, measurement quality, multitrait–multimethod experiment, radio buttons, reliability and validity, slider scales, multitrait-multimethod experiment, Research Quality},
}

@article{schlosser_mobile_2018,
	title = {Mobile and Dirty: Does Using Mobile Devices Affect the Data Quality and the Response Process of Online Surveys?},
	volume = {36},
	url = {https://doi.org/10.1177/0894439317698437},
	doi = {10.1177/0894439317698437},
	abstract = {In this article, we present a study on the data quality and the response process of mobile online surveys using an experimental design as compared to a standard computer. We used the following indicators to measure data quality and response properties: reaction time to survey invitation, break-off rate, item nonresponse, length of responses to open-ended questions and survey transmission, processing, and completion time. With regard to completion time, we also explored the significance of the place as well as the situation in which the survey was completed, the kind of Internet connection the respondents had as well as the hardware properties of the devices used to answer the online survey. Our results suggest comparable data quality and response properties in most aspects: There were no noticeable differences between computer and mobile users as regards break-off rate, item nonresponse, and length of responses to open-ended questions, nor the place where the survey was completed. However, it took respondents in the mobile group longer to complete the survey as compared to respondents answering the online survey on their computer. In terms of the completion time, there was a significant decrease in the differences between mobile devices and {PCs} when respondents used technically advanced mobile devices and had access to a fast Internet connection.},
	pages = {212--230},
	number = {2},
	journaltitle = {Social Science Computer Review},
	author = {Schlosser, Stephan and Mays, Anja},
	date = {2018},
	keywords = {Technology, Mobile Devices, Studies, Internet, online survey, Education--Computer Applications, data quality, Data quality, Electronic devices, Polls \& surveys, Research responses, Mobile computing, Surveys, Computers, {SMARTPHONE}, mobile device, Data Sets, item nonresponse, Reaction time, response time, {COMPUTER}, paradata, break-off rate, length of responses to open-ended questions, mode effect, reaction time to survey invitation, {TIMES}, Access, Research design, Completion time, {WEB} {SURVEY}, Properties (attributes)},
}

@article{revilla_testing_2020,
	title = {Testing the Use of Voice Input in a Smartphone Web Survey},
	volume = {38},
	url = {https://doi.org/10.1177/0894439318810715},
	doi = {10.1177/0894439318810715},
	abstract = {We implemented an experiment within a smartphone web survey to explore the feasibility of using voice input ({VI}) options. Based on device used, participants were randomly assigned to a treatment or control group. Respondents in the {iPhone} operating system ({iOS}) treatment group were asked to use the dictation button, in which the voice was translated automatically into text by the device. Respondents with Android devices were asked to use a {VI} button which recorded the voice and transmitted the audio file. Both control groups were asked to answer open-ended questions using standard text entry. We found that the use of {VI} still presents a number of challenges for respondents. Voice recording (Android) led to substantially higher nonresponse, whereas dictation ({iOS}) led to slightly higher nonresponse, relative to text input. However, completion time was significantly reduced using {VI}. Among those who provided an answer, when dictation was used, we found fewer valid answers and less information provided, whereas for voice recording, longer and more elaborated answers were obtained. Voice recording (Android) led to significantly lower survey evaluations, but not dictation ({iOS}).},
	pages = {207--224},
	number = {2},
	journaltitle = {Social Science Computer Review},
	author = {Revilla, Melanie and Couper, Mick P. and Bosch, Oriol J. and Asensio, Marc},
	date = {2020},
	keywords = {Internet, Smartphones, Education--Computer Applications, data quality, Polls \& surveys, Recording, Research responses, Voice, voice recording, mobile web surveys, {PROBABILITY}-{BASED} {PANEL}, {COMPUTER}, {QUESTIONS}, dictation, {MOBILE} {DEVICES}, {PC}, {RESPONSE} {QUALITY}, speech-to-text, Audio data, Completion time},
}

@article{mavletova_data_2013,
	title = {Data Quality in {PC} and Mobile Web Surveys},
	volume = {31},
	url = {https://doi.org/10.1177/0894439313485201},
	doi = {10.1177/0894439313485201},
	abstract = {The considerable growth in the number of smart mobile devices with a fast Internet connection provides new challenges for survey researchers. In this article, I compare the data quality between two survey modes: self-administered web surveys conducted via personal computer and those conducted via mobile phones. Data quality is compared based on five indicators: (a) completion rates, (b) response order effects, (c) social desirability, (d) non-substantive responses, and (e) length of open answers. I hypothesized that mobile web surveys would result in lower completion rates, stronger response order effects, and less elaborate answers to open-ended questions. No difference was expected in the level of reporting in sensitive items and in the rate of non-substantive responses. To test the assumptions, an experiment with two survey modes was conducted using a volunteer online access panel in Russia. As expected, mobile web was associated with a lower completion rate, shorter length of open answers, and similar level of socially undesirable and non-substantive responses. However, no stronger primacy effects in mobile web survey mode were found.},
	pages = {725--743},
	number = {6},
	journaltitle = {Social Science Computer Review},
	author = {Mavletova, Aigul},
	date = {2013},
	keywords = {Data analysis, Quality, Mobile Devices, Internet, Smartphones, Education--Computer Applications, data quality, Polls \& surveys, mobile devices, Internet access, Surveys, mobile web surveys, {BIAS}, personal computers, web surveys, {METAANALYSIS}, Microcomputers, social desirability, Data Collection, {MODE}, {NONRESPONSE}, Russia, Comparative studies, {SENSITIVE} {QUESTIONS}, computer methods, media, \& applications, 0188: methodology and research technology, article, Methodology (Data Collection), Data Quality, {RESPONSE} {RATES}, Social Desirability, Volunteers, web surveys mobile web surveys data quality completion rates response order effects primacy effects social desirability non-substantive responses length of open-ended questions, completion rates, {AUDIO}, length of open-ended questions, non-substantive responses, primacy effects, response order effects},
}

@article{mavletova_grid_2018,
	title = {Grid and Item-by-Item Formats in {PC} and Mobile Web Surveys},
	volume = {36},
	url = {https://doi.org/10.1177/0894439317735307},
	doi = {10.1177/0894439317735307},
	abstract = {While grids or matrix questions are a widely used format in {PC} web surveys, there is no agreement on the format in mobile web surveys. We conducted a two-wave experiment in an opt in panel in Russia, varying the question format (grid format and item-by-item format) and device respondents used for survey completion (smartphone and {PC}). The 1,678 respondents completed the survey in the assigned conditions in the first wave and 1,079 in the second wave. Overall, we found somewhat higher measurement error in the grid format in both mobile and {PC} web conditions. We found almost no significant effect of the question format on test?retest correlations between the latent scores in two waves and no differences in breakoff rates between the question formats. The multigroup comparison showed some measurement equivalence between the question formats. However, the difference varied depending on the length of a scale with a longer scale producing some differences in the measurement equivalence between the conditions. The levels of straightlining were higher in the grid than in the item-by-item format. In addition, concurrent validity was lower in the grid format in both {PC} and mobile web conditions. Finally, subjective indicators of respondent burden showed that the grid format increased reported technical difficulties and decreased subjective evaluation of the survey.},
	pages = {647--668},
	number = {6},
	journaltitle = {Social Science Computer Review},
	author = {Mavletova, Aigul and Couper, Mick P. and Lebedev, Daniil},
	date = {2018},
	keywords = {Mobile Devices, Measurement errors, Error analysis, Internet, Smartphones, Education--Computer Applications, Polls \& surveys, Psychometrics, Test Validity, mobile web surveys, Online Surveys, {DESIGN}, web surveys, measurement error, concurrent validity, grid questions, item-by-item format, matrix questions, measurement equivalence, reliability, {MEASUREMENT} {INVARIANCE}, Format, Equivalence, Scales},
}

@article{lugtig_use_2016,
	title = {The Use of {PCs}, Smartphones, and Tablets in a Probability-Based Panel Survey: Effects on Survey Measurement Error},
	volume = {34},
	url = {https://doi.org/10.1177/0894439315574248},
	doi = {10.1177/0894439315574248},
	abstract = {Respondents in an Internet panel survey can often choose which device they use to complete questionnaires: a traditional {PC}, laptop, tablet computer, or a smartphone. Because all these devices have different screen sizes and modes of data entry, measurement errors may differ between devices. Using data from the Dutch Longitudinal Internet Study for the Social sciences panel, we evaluate which devices respondents use over time. We study the measurement error associated with each device and show that measurement errors are larger on tablets and smartphone than on {PCs}. To gain insight into the causes of these differences, we study changes in measurement error over time, associated with a switch of devices over two consecutive waves of the panel. We show that within individuals, measurement errors do not change with a switch in device. Therefore, we conclude that the higher measurement error in tablets and smartphones is associated with self-selection of the sample into using a particular device.},
	pages = {78--94},
	number = {1},
	journaltitle = {Social Science Computer Review},
	author = {Lugtig, Peter and Toepoel, Vera},
	date = {2016},
	keywords = {mobile phones, tablets, measurement error, mixed device, panel survey, {MOBILE} {WEB} {SURVEYS}},
}

@article{toepoel_probing_2021,
	title = {Probing in online mixed-device surveys: Is a research messenger layout more effective than a traditional online layout, especially on mobile devices?},
	volume = {151},
	url = {https://doi.org/10.1177/07591063211019953},
	doi = {10.1177/07591063211019953},
	abstract = {This article compares the effectiveness of a research messenger layout to a traditional online layout with regards to probing. Responses to different types of probes (explanation, elaboration and category selection probes) were examined in terms of length and quality, measured by number of characters, number of themes, and an indicator for response quality. The research messenger layout, regardless of device being used, had a negative effect on both response length, number of themes and response quality. Further, we found that in both the traditional and research messenger layout, using a mobile device negatively affects the number of characters and themes used in probed responses. We conclude that probing is most effective when a traditional survey is completed on a computer. The research messenger layout was not able to generate responses of similar quality compared to the traditional layout, regardless of device being used.},
	pages = {74--95},
	number = {1},
	journaltitle = {Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique},
	author = {Toepoel, Vera and Mathon, Karlijn and Tussenbroek, Puck and Lugtig, Peter},
	date = {2021},
}

@article{wells_comparison_2014,
	title = {Comparison of Smartphone and Online Computer Survey Administration},
	volume = {32},
	url = {https://doi.org/10.1177/0894439313505829},
	doi = {10.1177/0894439313505829},
	abstract = {The dramatic rise of smartphones has profound implications for survey research. Namely, can smartphones become a viable and comparable device for self-administered surveys? The current study is based on approximately 1,500 online U.S. panelists who were smartphone users and who were randomly assigned to the mobile app or online computer mode of a survey. Within the survey, we embedded several experiments that had been previously tested in other modes (mail, {PC} web, mobile web). First, we test whether responses in the mobile app survey are sensitive to particular experimental manipulations as they are in other modes. Second, we test whether responses collected in the mobile app survey are similar to those collected in the online computer survey. Our mobile survey experiments show that mobile survey responses are sensitive to the presentation of frequency scales and the size of open-ended text boxes, as are responses in other survey modes. Examining responses across modes, we find very limited evidence for mode effects between mobile app and {PC} web survey administrations. This may open the possibility for multimode (mobile and online computer) surveys, assuming that certain survey design recommendations for mobile surveys are used consistently in both modes.},
	pages = {238--255},
	number = {2},
	journaltitle = {Social Science Computer Review},
	author = {Wells, Tom and Bailey, Justin T. and Link, Michael W.},
	date = {2014},
	keywords = {Mobile Devices, Internet, Smartphones, Education--Computer Applications, smartphones, Polls \& surveys, computers, United States--{US}, Human Computer Interaction, Mobile Phones, Surveys, Human Machine Systems Design, Computers, Software, Websites, mobile surveys, test administration format, Test Forms, text boxes, Text Structure, websites, online surveys, {RESPONSES}, {MAIL}, {MOBILE} {WEB} {SURVEY}, {OPEN}-{ENDED} {QUESTIONS}, {ORDER}, experiments, computer methods, media, \& applications, 0188: methodology and research technology, article, mobile surveys online surveys smartphones experiments, Users},
}

@article{antoun_design_2018,
	title = {Design Heuristics for Effective Smartphone Questionnaires},
	volume = {36},
	doi = {10.1177/0894439317727072},
	abstract = {Design principles for survey questionnaires viewed on desktop and laptop computers are increasingly being seen as inadequate for the design of questionnaires viewed on smartphones. Insights gained from empirical research can help those conducting mobile surveys to improve their questionnaires. This article reports on a systematic literature review of research presented or published between 2007 and 2016 that evaluated the effect of smartphone questionnaire design features on indicators of response quality. The evidence suggests that survey designers should make efforts to ?optimize? their questionnaires to make them easier to complete on smartphones, fit question content to the width of smartphone screens to prevent horizontal scrolling, and choose simpler types of questions (single-choice questions, multiple-choice questions, text-entry boxes) over more complicated types of questions (large grids, drop boxes, slider questions). Based on these results, we identify design heuristics, or general principles, for creating effective smartphone questionnaires. We distinguish between five of them: readability, ease of selection, visibility across the page, simplicity of design elements, and predictability across devices. They provide an initial framework by which to evaluate smartphone questionnaires, though empirical testing and further refinement of the heuristics is necessary.},
	pages = {557--574},
	number = {5},
	journaltitle = {Social Science Computer Review},
	author = {Antoun, Christopher and Katz, Jonathan and Argueta, Josef and Wang, Lin},
	date = {2018},
	keywords = {Literature reviews, Visibility, Smartphones, Education--Computer Applications, Polls \& surveys, Questionnaires, {QUALITY}, Design, mobile web surveys, response quality, {COMPUTER}, smartphone surveys, {PC}, Research design, Heuristic, {DEVICE} {AFFECT}, {MOBILE} {WEB} {SURVEY}, {PANEL}, questionnaire design, {SCALES}, {SLIDER}, Scrolling, Boxes, Screens},
}

@article{peytchev_experiments_2010,
	title = {Experiments in Mobile Web Survey Design: Similarities to Other Modes and Unique Considerations},
	volume = {28},
	url = {https://doi.org/10.1177/0894439309353037},
	doi = {10.1177/0894439309353037},
	abstract = {Self-administered surveys can be conducted on mobile web-capable devices, yet these devices have unique features that can affect response processes. Ninety-two adults were randomly selected and provided with mobile devices to complete weekly web surveys. Experiments were designed to address three main objectives. First, the authors test fundamental findings which have been found robust across other modes, but whose impact may be diminished in mobile web surveys (due largely to the device), by manipulating question order and scale frequencies. Second, the authors test findings from experiments in computer-administered web surveys, altering the presentation of images and the number of questions per page. Third, the authors experiment with the unique display, navigation, and input methods, through the need to scroll, the vertical versus horizontal orientation of scales, and the willingness to provide open-ended responses. Although most findings from other modes are upheld, the small screen and keyboard introduce undesirable differences in responses.},
	pages = {319--335},
	number = {3},
	journaltitle = {Social Science Computer Review},
	author = {Peytchev, Andy and Hill, Craig A.},
	date = {2010},
	keywords = {Data collection, Internet, Smartphones, Education--Computer Applications, smartphones, Polls \& surveys, mobile devices, Surveys, mobile web surveys, cell phones, {CONTEXT}, {QUESTIONS}, Product design, {DATA}-{COLLECTION}, computer methods, media, \& applications, survey design, {SCREEN} {SIZE}, 0188: methodology and research technology, article, mobile devices cell phones smartphones survey design mobile web surveys, Personal digital assistants},
}

@article{revilla_comparing_2018,
	title = {Comparing Grids With Vertical and Horizontal Item-by-Item Formats for {PCs} and Smartphones},
	volume = {36},
	url = {https://doi.org/10.1177/0894439317715626},
	doi = {10.1177/0894439317715626},
	abstract = {Much research has been done comparing grids and item-by-item formats. However, the results are mixed, and more research is needed especially when a significant proportion of respondents answer using smartphones. In this study, we implemented an experiment with seven groups (n = 1,476), varying the device used ({PC} or smartphone), the presentation of the questions (grids, item-by-item vertical, item-by-item horizontal), and, in the case of smartphones only, the visibility of the ?next? button (always visible or only visible at the end of the page, after scrolling down). The survey was conducted by the Netquest online fieldwork company in Spain in 2016. We examined several outcomes for three sets of questions, which are related to respondent behavior (completion time, lost focus, answer changes, and screen orientation) and data quality (item missing data, nonsubstantive responses, instructional manipulation check failure, and nondifferentiation). The most striking difference found is for the placement of the next button in the smartphone item-by-item conditions: When the button is always visible, item missing data are substantially higher.},
	pages = {349--368},
	number = {3},
	journaltitle = {Social Science Computer Review},
	author = {Revilla, Melanie and Couper, Mick P.},
	date = {2018},
	keywords = {Technology, Visibility, Internet, Smartphones, Education--Computer Applications, smartphones, Telephone communications, data quality, Data quality, Mobile Phones, {DESIGN}, web surveys, Microcomputers, Product Design, Human Factors Engineering, {WEB} {SURVEYS}, Completion time, {PARADATA}, Missing data, Scrolling, grids, item-by-item, respondent behavior, scale orientation, {LAYOUT}},
}

@article{gummer_does_2019,
	title = {Does Increasing Mobile Device Coverage Reduce Heterogeneity in Completing Web Surveys on Smartphones?},
	volume = {37},
	url = {https://doi.org/10.1177/0894439318766836},
	doi = {10.1177/0894439318766836},
	abstract = {Mobile coverage recently has reached an all-time high, and in most countries, high-speed Internet connections are widely available. Due to technological development, smartphones and tablets have become increasingly popular. Accordingly, we have observed an increasing use of mobile devices to complete web surveys and, hence, survey methodologists have shifted their attention to the challenges that stem from this development. The present study investigated whether the growing use of smartphones has decreased how systematically this choice of device varies between groups of respondents (i.e., how selective smartphone usage for completing web surveys is). We collected a data set of 18,520 respondents from 18 web surveys that were fielded in Germany between 2012 and 2016. Based on these data, we show that while the use of smartphones to complete web surveys has considerably increased over time, selectivity with respect to using this device has remained stable.},
	pages = {371--384},
	number = {3},
	journaltitle = {Social Science Computer Review},
	author = {Gummer, Tobias and Quoß, Franziska and Roßmann, Joss},
	date = {2019},
	keywords = {Mobile Devices, Internet, Smartphones, Education--Computer Applications, smartphones, Electronic devices, Polls \& surveys, mobile devices, Surveys, {BIAS}, smartphone, Mobile communication systems, Tablet computers, web surveys, Choice Behavior, device choice, Microcomputers, {PROBABILITY}-{BASED} {PANEL}, {DATA} {QUALITY}, survey error, {PC}, {TABLETS}, {IMPROVING} {RESPONSE}, selectivity, Technological change, Focus groups, Selectivity},
}

@article{daikeler_motivated_2020,
	title = {Motivated Misreporting in Smartphone Surveys},
	url = {https://doi.org/10.1177/0894439319900936},
	doi = {10.1177/0894439319900936},
	abstract = {Filter questions are used to administer follow-up questions to eligible respondents while allowing respondents who are not eligible to skip those questions. Filter questions can be asked in either the interleafed or the grouped formats. In the interleafed format, the follow-ups are asked immediately after the filter question; in the grouped format, follow-ups are asked after the filter question block. Underreporting can occur in the interleafed format due to respondents? desire to reduce the burden of the survey. This phenomenon is called motivated misreporting. Because smartphone surveys are more burdensome than web surveys completed on a computer or laptop, due to the smaller screen size, longer page loading times, and more distraction, we expect that motivated misreporting is more pronounced on smartphones. Furthermore, we expect that misreporting occurs not only in the filter questions themselves but also extends to data quality in the follow-up questions. We randomly assigned 3,517 respondents of a German online access panel to either the {PC} or the smartphone. Our results show that while both {PC} and smartphone respondents trigger fewer filter questions in the interleafed format than the grouped format, we did not find differences between {PC} and smartphone respondents regarding the number of triggered filter questions. However, smartphone respondents provide lower data quality in the follow-up questions, especially in the grouped format. We conclude with recommendations for web survey designers who intend to incorporate smartphone respondents in their surveys.},
	pages = {0894439319900936--0894439319900936},
	journaltitle = {Social Science Computer Review},
	author = {Daikeler, Jessica and Bach, Ruben L. and Silber, Henning and Eckman, Stephanie},
	date = {2020},
	keywords = {measurement error, {DATA} {QUALITY}, {MOBILE} {WEB}, {PC}, {DEVICES}, filter questions, {FILTER} {QUESTIONS}, follow-up questions, misreporting, mobile data quality, motivated underreporting, {TABLETS}},
}

@article{maineri_slider_2021,
	title = {Slider Bars in Multi-Device Web Surveys},
	volume = {39},
	url = {https://doi.org/10.1177/0894439319879132},
	doi = {10.1177/0894439319879132},
	abstract = {This study explores some features of slider bars in the context of a multi-device web survey. Using data collected among the students of the University of Trento in 2015 and 2016 by means of two web surveys (N = 6,343 and 4,124) including two experiments, we investigated the effect of the initial position of the handle and the presence of numeric labels on answers provided using slider bars. It emerged that the initial position of the handle affected answers and that the number of rounded scores increased with numeric feedback. Smartphone respondents appeared more sensitive to the initial position of the handle but also less affected by the presence of numeric labels resulting in a lower tendency to rounding. Yet, outcomes on anchoring were inconclusive. Overall, no relevant differences have been detected between tablet and {PC} respondents. Understanding to what extent interactive and engaging tools such as slider bars can be successfully employed in multi-device surveys without affecting data quality is a key challenge for those who want to exploit the potential of web-based and multi-device data collection without undermining the quality of measurement.},
	pages = {573--591},
	number = {4},
	journaltitle = {Social Science Computer Review},
	author = {Maineri, Angelica M. and Bison, Ivano and Luijkx, Ruud},
	date = {2021},
	keywords = {College students, Data collection, Internet, Education--Computer Applications, Data quality, Polls \& surveys, {SMARTPHONE}, mobile surveys, {DESIGN}, {MOBILE}, web surveys, Bars, Labels, multi-device surveys, Rounding, slider question, sliders, survey experiment, {COMPUTER}, {DATA} {QUALITY}, {FORMATS}, {RADIO} {BUTTON} {SCALES}, {RELIABILITY}, {VISUAL} {ANALOG} {SCALES}},
}

@article{keusch_web_2017,
	title = {Web Versus Mobile Web: An Experimental Study of Device Effects and Self-Selection Effects},
	volume = {35},
	url = {https://doi.org/10.1177/0894439316675566},
	doi = {10.1177/0894439316675566},
	abstract = {Due to a rising mobile device penetration, Web surveys are increasingly accessed and completed on smartphones or tablets instead of desktop computers or laptops. Mobile Web surveys are also gaining popularity as an alternative self-administered data collection mode among survey researchers. We conducted a methodological experiment among {iPhone} owners and compared the participation and response behavior of three groups of respondents: {iPhone} owners who started and completed our survey on a desktop or laptop {PC}, {iPhone} owners who self-selected to complete the survey on an {iPhone}, and {iPhone} owners who started on a {PC} but were requested to switch to {iPhone}. We found that respondents who completed the survey on a {PC} were more likely to be male, to have a lower educational level, and to have more experience with Web surveys than mobile Web respondents, regardless of whether they used the {iPhone} voluntarily or were asked to switch from a {PC} to an {iPhone}. Overall, {iPhone} respondents had more missing data and took longer to complete the survey than respondents who answered the questions on a {PC}, but they also showed less straightlining behavior. There are only minimal device differences on survey answers obtained from {PCs} and {iPhones}.},
	pages = {751--769},
	number = {6},
	journaltitle = {Social Science Computer Review},
	author = {Keusch, Florian and Yan, Ting},
	date = {2017},
	keywords = {Data collection, Internet, {iPhone}, Smartphones, Education--Computer Applications, smartphones, data quality, Polls \& surveys, {QUALITY}, Mobile computing, Participation, Mobile communication systems, {SMARTPHONE}, Tablet computers, Microcomputers, {PROBABILITY}-{BASED} {PANEL}, {COMPUTER}, response times, {PC}, Personal computers, {ONLINE} {SURVEYS}, break-offs, missing data, mobile Web surveys, straightlining, unintentional mobile, Web surveys, {MECHANICAL} {TURK}, Missing data, 83:Social and Behavioral Sciences ({CI}), Alternative approaches, Data acquisition, Educational attainment},
}

@article{keusch_using_2021,
	title = {Using Smartphone Technology for Research on Refugees: Evidence from Germany},
	volume = {50},
	url = {https://doi.org/10.1177/0049124119852377},
	doi = {10.1177/0049124119852377},
	abstract = {Researchers attempting to survey refugees over time face methodological issues because of the transient nature of the target population. In this article, we examine whether applying smartphone technology could alleviate these issues. We interviewed 529 refugees and afterward invited them to four follow-up mobile web surveys and to install a research app for passive mobile data collection. Our main findings are as follows: First, participation in mobile web surveys declines rapidly and is rather selective with significant coverage and nonresponse biases. Second, we do not find any factor predicting types of smartphone ownership, and only low reading proficiency is significantly correlated with app nonparticipation. However, obtaining sufficiently large samples is challenging?only 5 percent of the eligible refugees installed our app. Third, offering a 30 Euro incentive leads to a statistically insignificant increase in participation in passive mobile data collection.},
	pages = {1863--1894},
	number = {4},
	journaltitle = {Sociological Methods \& Research},
	author = {Keusch, Florian and Leonard, Mariel M. and Sajons, Christoph and Steiner, Susan},
	date = {2021},
	keywords = {participation, Technology, Data collection, Sociology, Internet, Smartphones, smartphones, {DISCLOSURE}, Polls \& surveys, Competence, coverage, Germany, Methodological problems, mobile web surveys, Nonresponse, Ownership, Participation, passive mobile data collection, refugees, Refugees, {RISK}, {MOBILE} {WEB}, {HARM}},
}

@article{buskirk_making_2014,
	title = {Making Mobile Browser Surveys Smarter: Results from a Randomized Experiment Comparing Online Surveys Completed via Computer or Smartphone},
	volume = {26},
	url = {https://doi.org/10.1177/1525822X14526146},
	doi = {10.1177/1525822X14526146},
	abstract = {With nearly 50\% of U.S. mobile phone subscribers using smartphones, survey researchers are beginning to explore their use as a data collection tool. The Got Healthy Apps Study ({GHAS}) conducted a randomized experiment to compare mode effects for a survey completed via {iPhone} mobile browser and online via desktop/laptop computer web browser. Mode effects were assessed for three types of outcomes: randomization/recruitment, survey process/completion, and survey items. In short, the distribution of survey completion times and the distribution of the number of apps owned were significantly different across survey mode after accounting for block group. Other key mode effects outcomes (including open-ended items, slider bar questions, and missing item rates) showed no significant differences across survey mode. Some interesting qualitative findings suggest that {iPhone} respondents enter more characters and omit fewer items than originally thought.},
	pages = {322--342},
	number = {4},
	journaltitle = {Field methods},
	author = {Buskirk, Trent D. and Andrus, Charles H.},
	date = {2014},
	keywords = {Methodology, Internet, survey research, Smartphones, smartphones, Computer Applications, Mobile Phones, Surveys, apps, Computers, Online Surveys, Websites, mode effects, Recruitment, article, 0514: culture and social structure, Methodology (Data Collection), smartphones survey research mode effects apps, social anthropology},
}

@article{revilla_improving_2021,
	title = {Improving the Use of Voice Recording in a Smartphone Survey},
	volume = {39},
	url = {https://www.proquest.com/scholarly-journals/improving-use-voice-recording-smartphone-survey/docview/2607554021/se-2?accountid=14570},
	doi = {10.1177/0894439319888708},
	abstract = {More and more respondents are answering web surveys using mobile devices. Mobile respondents tend to provide shorter responses to open questions than {PC} respondents. Using voice recording to answer open-ended questions could increase data quality and help engage groups usually underrepresented in web surveys. Revilla, Couper, Bosch, and Asensio showed that in particular the use of voice recording still presents many challenges, even if it could be a promising tool. This article reports results from a follow-up experiment in which the main goals were to (1) test whether different instructions on how to use the voice recording tool reduce technical and understanding problems, and thereby reduce item nonresponse while preserving data quality and the evaluation of the tool; (2) test whether nonresponse due to context can be reduced by using a filter question, and how this affects data quality and the tool evaluation; and (3) understand which factors affect nonresponse to open-ended questions using voice recording, and if these factors also affect data quality and the evaluation of the tool. The experiment was implemented within a smartphone web survey in Spain focused on Android devices. The results suggest that different instructions did not affect nonresponse to the open questions and had little effect on data quality for those who did answer. Introducing a filter to ensure that people were in a setting that permits voice recording seems useful. Despite efforts to reduce problems, a substantial proportion of respondents are still unwilling or unable to answer open questions using voice recording.},
	pages = {1159--1178},
	number = {6},
	journaltitle = {Social Science Computer Review},
	author = {Revilla, Melanie and Couper, Mick P.},
	date = {2021},
	keywords = {Internet, Smartphones, Education--Computer Applications, smartphones, Evaluation, android, Birthdays, data quality, Data quality, Electronic devices, Emotions, mobile web survey, nonresponse, Polls \& surveys, Questions, Recording, Research responses, tool evaluation, Voice, voice recording, Webs, {PROBABILITY}-{BASED} {PANEL}, {COMPUTER}, {DATA} {QUALITY}, {MOBILE} {DEVICES}, {PC}, {WEB}},
}

@article{de_bruijne_mobile_2014,
	title = {Mobile Response in Web Panels},
	volume = {32},
	url = {https://doi.org/10.1177/0894439314525918},
	doi = {10.1177/0894439314525918},
	abstract = {This article investigates unintended mobile access to surveys in online, probability-based panels. We find that spontaneous tablet usage is drastically increasing in web surveys, while smartphone usage remains low. Further, we analyze the bias of respondent profiles using smartphones and tablets compared to those using computers, on the basis of several sociodemographic characteristics. Our results indicate not only that mobile web respondents differ from {PC} users but also that tablet users differ from smartphone users. While tablets are used for survey completion by working (young) adults, smartphones are used merely by the young. In addition, our results indicate that mobile web respondents are more progressive and describe themselves more often as pioneers or forerunners in adopting new technology, compared to {PC} respondents. We further discover that respondents? preferences for devices to complete surveys are clearly in line with unintended mobile response. Finally, we present a similar analysis on intended mobile response in an experiment where smartphone users were requested to complete a mobile survey. Based on these findings, testing on tablets is strongly recommended in online surveys. If the goal is to reach young respondents, enabling surveys via smartphones should be considered.},
	pages = {728--742},
	number = {6},
	journaltitle = {Social Science Computer Review},
	author = {De Bruijne, Marika and Wijnant, Arnaud},
	date = {2014},
	keywords = {Technology, Mobile Devices, Internet, Smartphones, Education--Computer Applications, Demographic Characteristics, mobile web survey, Surveys, Preferences, Computers, Websites, Bias, Demographics, respondent preference, survey error, unintended mobile response, web panel, computer methods, media, \& applications, 0188: methodology and research technology, article, mobile web survey unintended mobile response survey error web panel respondent preference, Sociodemographic Characteristics, Young Adults},
}

@article{toepoel_what_2014,
	title = {What Happens if You Offer a Mobile Option to Your Web Panel? Evidence From a Probability-Based Panel of Internet Users},
	volume = {32},
	url = {https://doi.org/10.1177/0894439313510482},
	doi = {10.1177/0894439313510482},
	abstract = {This article reports from a pilot study that was conducted in a probability-based online panel in the Netherlands. Two parallel surveys were conducted: one in the traditional questionnaire layout of the panel and the other optimized for mobile completion with new software that uses a responsive design (optimizes the layout for the device chosen). The latter questionnaire was optimized for mobile completion, and respondents could choose whether they wanted to complete the survey on their mobile phone or on a regular desktop. Results show that a substantive number of respondents (57\%) used their mobile phone for survey completion. No differences were found between mobile and desktop users with regard to break offs, item nonresponse, time to complete the survey, or response effects such as length of answers to an open-ended question and the number of responses in a check-all-that-apply question. A considerable number of respondents gave permission to record their {GPS} coordinates, which are helpful in defining where the survey was taken. Income, household size, and household composition were found to predict mobile completion. In addition, younger respondents, who typically form a hard-to-reach group, show higher mobile completion rates.},
	pages = {544--560},
	number = {4},
	journaltitle = {Social Science Computer Review},
	author = {Toepoel, Vera and Lugtig, Peter},
	date = {2014},
	keywords = {Internet, Education--Computer Applications, Computer Software, Netherlands, Software, {DESIGN}, Households, {MODES}, panel survey, Probability, nonresponse effects, measurement effects, mobile phone survey, computer methods, media, \& applications, Global positioning systems--{GPS}, Income, 0188: methodology and research technology, article, mobile phone survey panel survey measurement effects nonresponse effects},
}

@article{de_bruijne_comparing_2013,
	title = {Comparing Survey Results Obtained via Mobile Devices and Computers: An Experiment With a Mobile Web Survey on a Heterogeneous Group of Mobile Devices Versus a Computer-Assisted Web Survey},
	volume = {31},
	url = {https://doi.org/10.1177/0894439313483976},
	doi = {10.1177/0894439313483976},
	abstract = {With the growing popularity of smartphones and tablet {PCs} (tablets) equipped with mobile browsers, the possibilities to administer surveys via mobile devices have expanded. To investigate the possible mode effect on answer behavior, results are compared between a mobile device?assisted web survey and a computer-assisted web survey. First, a premeasurement in the {CentERpanel} is conducted to analyze the user group of mobile devices. Second, the users are randomly allocated one of the three conditions: (1) conventional computer-assisted web survey, (2) hybrid version: a computer-assisted web survey with a layout similar to mobile web survey, and (3) mobile web survey. Special attention is given to the design of the mobile web questionnaire, taking small screen size, and typical functionalities for touchscreens into account. The findings suggest that survey completion on mobile devices need not lead to different results than on computers, but one should be prepared for a lower response rate and longer survey completion time. Further, the study offers considerations for researchers on survey satisfaction, location during survey completion, and preferred device to access Internet. With adaptations, surveys can be conducted on the newest mobile devices, although new challenges are emerging and further research is called for.},
	pages = {482--504},
	number = {4},
	journaltitle = {Social Science Computer Review},
	author = {De Bruijne, Marika and Wijnant, Arnaud},
	date = {2013},
	keywords = {Heterogeneity, Portable computers, Internet, Smartphones, Education--Computer Applications, smartphones, mobile web survey, Polls \& surveys, Satisfaction, mobile devices, Mobile Phones, Surveys, Computers, Attention, computer web survey, mobile device, Popularity, research surveys, response rate, special attention, {INTERPRETIVE} {HEURISTICS}, {SEARCH}, Access, {SURVEY} {DESIGN}, Response rates, Comparative studies, computer methods, media, \& applications, {OPTIMAL} {NUMBER}, {RATING}-{SCALES}, survey design, tablet {PCs}, touch user interfaces, 0188: methodology and research technology, article, mobile web survey smartphones tablet {PCs} mobile devices survey design touch user interfaces},
}